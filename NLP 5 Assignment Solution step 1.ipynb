{"cells":[{"cell_type":"markdown","id":"8155e530","metadata":{"id":"8155e530"},"source":["# List the files in the directory"]},{"cell_type":"code","execution_count":null,"id":"8e56d4bc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":217,"status":"ok","timestamp":1741778933133,"user":{"displayName":"Navdeep Singh","userId":"10186325231935212999"},"user_tz":-330},"id":"8e56d4bc","outputId":"70b8d630-8581-4f91-f2a4-d390b702ede4"},"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot access '/Dataset-IK/': No such file or directory\n"]}],"source":["!ls /Dataset-IK/"]},{"cell_type":"code","execution_count":null,"id":"UnBBbficmmWH","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":896,"status":"ok","timestamp":1714087266428,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"},"user_tz":240},"id":"UnBBbficmmWH","outputId":"5a2c857b-450d-4366-d8b3-7724d294bb50"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"9bqqBfJRmqgs","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59,"status":"ok","timestamp":1741778933183,"user":{"displayName":"Navdeep Singh","userId":"10186325231935212999"},"user_tz":-330},"id":"9bqqBfJRmqgs","outputId":"336b236b-f15d-408b-ff3f-fe1e4a33c947"},"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot access '/content/drive/MyDrive/IK/Dataset-IK/': No such file or directory\n"]}],"source":["ls /content/drive/MyDrive/IK/Dataset-IK/"]},{"cell_type":"code","execution_count":null,"id":"f7062cb2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":130,"status":"ok","timestamp":1714087269129,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"},"user_tz":240},"id":"f7062cb2","outputId":"08ee1c5f-18ed-4123-ff2f-cd98aef036b9"},"outputs":[{"data":{"text/plain":["20"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# The PDF files for each author are stored in their respective folder\n","import os\n","len(os.listdir('/content/drive/MyDrive/IK/Dataset-IK/')) #No. of authors"]},{"cell_type":"code","execution_count":null,"id":"fc4a38ef","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":142,"status":"ok","timestamp":1714087270609,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"},"user_tz":240},"id":"fc4a38ef","outputId":"7e4145e2-4a3b-44cc-b100-2dccf3ca94d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total No. of PDF files (research papers): 112\n"]}],"source":["c = 0 # No. of files\n","for root, dirs, files in os.walk('/content/drive/MyDrive/IK/Dataset-IK/'):\n","    for file in files:\n","        c = c + 1\n","        #print(os.path.join(root, file))\n","\n","print('Total No. of PDF files (research papers):',c)"]},{"cell_type":"markdown","id":"c2120f7a","metadata":{"id":"c2120f7a"},"source":["# Extracting text from PDFs"]},{"cell_type":"code","execution_count":null,"id":"99fa0eb1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8682,"status":"ok","timestamp":1714086455761,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"},"user_tz":240},"id":"99fa0eb1","outputId":"cf8a92b6-d8f5-489a-80a6-3c2f694ae605"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tika\n","  Downloading tika-2.6.0.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tika) (67.7.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tika) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (2024.2.2)\n","Building wheels for collected packages: tika\n","  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32621 sha256=a59c5cb3df317437daa639b7210e3a4b7365a4c2f5c00e7673c811807817eb8a\n","  Stored in directory: /root/.cache/pip/wheels/5f/71/c7/b757709531121b1700cffda5b6b0d4aad095fb507ec84316d0\n","Successfully built tika\n","Installing collected packages: tika\n","Successfully installed tika-2.6.0\n"]}],"source":["!pip install tika"]},{"cell_type":"code","execution_count":null,"id":"4468825d","metadata":{"id":"4468825d"},"outputs":[],"source":["import tika #PDF Text Extractor\n","from tika import parser\n","\n","# Set up tika\n","tika.initVM()\n","\n","# Function to extract text from PDF\n","def extract_text(filename):\n","    # Parse PDF file\n","    parsed = parser.from_file(filename)\n","\n","    # Extract text from parsed data\n","    text = parsed[\"content\"]\n","\n","    return text"]},{"cell_type":"code","execution_count":null,"id":"e454eee2","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"executionInfo":{"elapsed":1470,"status":"ok","timestamp":1714087281434,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"},"user_tz":240},"id":"e454eee2","outputId":"c7979905-70a2-4bcd-a89a-6b8243ac9c38"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTitle:\\n\\n\\nA Review of Clustering Techniques and Developments \\n\\nAmit Saxena1, Mukesh Prasad2, Akshansh Gupta3, Neha Bharill4, Om Prakash Patel4, Aruna Tiwari4, \\n\\nMeng Joo Er5, Weiping Ding6, Chin-Teng Lin2  \\n\\n1Department of Computer Science & IT, Guru Ghasidas Vishwavidyalaya, Bilaspur, India \\n\\n2Centre for Artificial Intelligence, University of Technology Sydney, Sydney, Australia \\n\\n3School of Computational and Integrative Sciences, Jawaharlal Nehru University, New Delhi, India \\n\\n4Department of Computer Science and Engineering, Indian Institute of Technology Indore, India \\n\\n5School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore \\n\\n6School of Computer and Technology, Nantong University, Nantong, China \\n\\n \\n\\n \\n\\nAbstract \\n\\nThis paper presents a comprehensive study on clustering: exiting methods and developments made at various \\n\\ntimes. Clustering is defined as an unsupervised learning where the objects are grouped on the basis of some \\n\\nsimilarity inherent among them. There are different methods for clustering the objects such as hierarchical, \\n\\npartitional, grid, density based and model based. The approaches used in these methods are discussed with their \\n\\nrespective states of art and applicability. The measures of similarity as well as the evaluation criteria, which are \\n\\nthe central components of clustering are also presented in the paper. The applications of clustering in some \\n\\nfields like image segmentation, object and character recognition and data mining are highlighted. \\n\\nKeywords: Unsupervised learning, Clustering, Data mining, Pattern recognition, Similarity measures \\n\\n1 Introduction \\n\\nGrouping of objects is required for various purposes in different areas of engineering, science and \\n\\ntechnology, humanities, medical science and our daily life. Take for an instance, people suffering from \\n\\na particular disease have some symptoms in common and are placed in a group tagged with some label \\n\\nusually the name of the disease. Evidently, the people not possessing those symptoms (and hence the \\n\\ndisease) will not be placed in that group. The patients grouped for that disease will be treated \\n\\naccordingly while patients not belonging to that group should be handled differently. It is therefore so \\n\\nessential for a medical expert to diagnose the symptoms of a patient correctly such that he/she is not \\n\\nplaced in a wrong group. Whenever we find a labeled object, we will place it into the group with same \\n\\nlabel. It is rather a trivial task as the labels are given in advance. However, on many occasions, no \\n\\nsuch labeling information is provided in advance and we group objects on the basis of some similarity.  \\n\\nBoth of these instances represent a wide range of problems occurring in analysis of data. In generic \\n\\nterms, these cases are dealt under the scope of classification [1]. Precisely, the first case when the class \\n\\n(label) of an object is given in advance is termed as supervised classification whereas the other case \\n\\nwhen the class label is not tagged to an object in advance is termed as unsupervised classification. \\n\\nThere has been a tremendous amount of work in supervised classification and evidently has been \\n\\nreported in the literature widely [2-9]. The main purpose behind the study of classification is to \\n\\ndevelop a tool or an algorithm, which can be used to predict the class of an unknown object, which is \\n\\nnot labeled. This tool or algorithm is called a classifier. The objects in the classification process are \\n\\nmore commonly represented by instances or patterns. A pattern consists of a number of features (also \\n\\ncalled attributes). The classification accuracy of a classifier is judged by the fact as how many testing \\n\\npatterns it has classified correctly. There has been a rich amount of work in supervised classification, \\n\\nsome of the pioneer supervised classification algorithms  can be found in neural networks [10, 11], \\n\\nfuzzy sets [12, 13], PSO [14, 15], rough sets [16-18] , decision tree [19], Bayes classifiers [20] etc. \\n\\nContrary to supervised classification, where we are given labeled patterns; the unsupervised \\n\\nclassification differs in the manner that there is no label assigned to any pattern. The unsupervised \\n\\nclassification is commonly known as clustering. As learning operation is central to the process of \\n\\n\\n\\nclassification (supervised or unsupervised), it is used in this paper interchangeably with the same \\n\\nspirit. Clustering is a very essential component of various data analysis or machine learning based \\n\\napplications like, regression,  prediction, data mining [21] etc. According to Rokach [22] clustering \\n\\ndivides data patterns into subsets in such a way that similar patterns are clustered together. The \\n\\npatterns are thereby managed into a well-formed evaluation that designates the population being \\n\\nsampled. Formally and conventionally, the clustering structure can be represented as a set S of subsets \\n\\nS1, S2, …, Sk , such that: \\n\\n1 2 3...... kS S S S \\uf066\\uf03d\\n                                                              (1) \\n\\nThis means obviously that any instance in S (S1... Sk) belongs to exactly one subset and does not \\n\\nbelong to any other subset. Clustering of objects is also applicable for charactering the key features of \\n\\npeople in recognizing them on the basis of some similarity. In general, we may divide people in \\n\\ndifferent clusters on the basis of gender, height, weight, color, vocal and some other physical \\n\\nappearances. Hence, clustering embraces several interdisciplinary areas such as: from mathematics \\n\\nand statistics to biology and genetics, where all of these use various terminology to explain the \\n\\ntopologies formed using this clustering analysis technique. For example, from biological \\n\\n“taxonomies”, to medical “syndromes” and genetic “genotypes” to manufacturing” group technology”, \\n\\neach of these topics has same identical problem: create groups of instances and assign each instance to \\n\\nthe appropriate groups. \\n\\nClustering is considered to be more difficult than supervised classification as there is no label \\n\\nattached to the patterns in clustering. The given label in the case of supervised classification becomes \\n\\na clue to grouping data objects as a whole. Whereas in the case of clustering, it becomes difficult to \\n\\ndecide, to which group a pattern will belong to, in the absence of a label. There can be several \\n\\nparameters or features which could be considered fit for clustering. The curse of dimensionality can \\n\\nadd to the crisis. High dimensionality not only leads to high computational cost but also affects the \\n\\nconsistency of algorithms. There are although feature selection methods reported as a solution [23]. \\n\\nThe sizes of the databases (e.g. small, large or very large) can also guide the clustering criteria.  \\n\\nJain [24] illustrated that the main aim of data clustering is to search the real grouping(s) of a set of \\n\\ninstances, points, or objects. Webster (Merriam-Webster Online Dictionary) [25] explains clustering as \\n\\n‘‘a statistical classification method for finding whether each of patterns comes into various groups by \\n\\nmaking quantitative comparisons of different features\". It is evident from the above discussion that \\n\\nsimilarity is the central factor to a cluster and hence clustering process. The natural grouping of data \\n\\nbased on some inherent similarity is to be discovered in clustering. In most of the cases, the number of \\n\\nclusters to be formed is specified by the user. As there is only numeric type data available to represent \\n\\nfeatures of the patterns in a group, the only way to extract any information pertaining to the \\n\\nrelationship among patterns is to make use of numeric arithmetic. The features of the objects are \\n\\nrepresented by numeric values. The most common approach to define similarity is taken as a measure \\n\\nof distance among the patterns, lower the distance (e.g. Euclidean distance) between the two objects, \\n\\nhigher the similarity and vice versa.  \\n\\nThe overall paper is organized as follows. Various clustering techniques will be discussed in \\n\\nSection 2. Section 3 presents measures of similarity for differentiating the patterns. In Section 4, the \\n\\nvariants of clustering methods have been presented. The evaluation criteria of the clustering \\n\\ntechniques applied for different problems are provided in Section 5. Section 6 highlights some \\n\\nemerging applications of clustering. Section 7 describes which clustering method to select under \\n\\ndifferent applications followed by conclusions in Section 8. Due to a wide range of topics in the \\n\\nsubject, the omission or the unbalancing of certain topics presented in the paper cannot be denied. The \\n\\nobjective of the paper is however to present a comprehensive timeline study of clustering with its \\n\\nconcepts, comparisons, existing techniques and few important applications. \\n\\n\\n\\n2 Clustering Techniques \\n\\nIn this section, we will discuss various clustering approaches with inherent techniques. The reason \\n\\nfor having different clustering approaches towards various techniques is due to the fact that there is no \\n\\nsuch precise definition to the notion of “cluster” [22, 26]. That is why, different clustering approaches \\n\\nhave been proposed, each of which uses a different inclusion principle. Fraley and Raftery [27] \\n\\nsuggested dividing the clustering approaches into two different groups: hierarchical and partitioning \\n\\ntechniques. Han and Kamber [21] suggested the following three additional categories for applying \\n\\nclustering techniques: density-based methods, model-based methods and grid-based methods. An \\n\\nalternative categorization based on the induction principle of different clustering approaches is \\n\\npresented in Castro et al [26]. However, the number of clusters into which available dataset to be \\n\\ndivided, is decided by the users judiciously by using some of the approaches including heuristic, trial \\n\\nand error or evolutionary. If the user decides suitable number, the accuracy judged by intra-cluster \\n\\ndistance will be high otherwise the accuracy can become low. Fig. 1 shows the taxonomy of clustering \\n\\napproaches [27].  \\n\\n \\n\\nFig. 1 Taxonomy of clustering approaches [27] \\n\\n2.1 Hierarchical Clustering (HC) Methods \\n\\nIn hierarchical clustering methods, clusters are formed by iteratively dividing the patterns using \\n\\ntop-down or bottom up approach. There are two forms of hierarchical method namely agglomerative \\n\\nand divisive hierarchical clustering [32]. The agglomerative follows the bottom-up approach, which \\n\\nbuilds up clusters starting with single object and then merging these atomic clusters into larger and \\n\\nlarger clusters, until all of the objects are finally lying in a single cluster or otherwise until certain \\n\\ntermination conditions are satisfied. The divisive hierarchical clustering follows the top-down \\n\\napproach, which breaks up cluster containing all objects into smaller clusters, until each object forms a \\n\\ncluster on its own or until it satisfies certain termination conditions. The hierarchical methods usually \\n\\nlead to formation of dendrograms as shown in Fig. 2 below.  \\n\\n\\n\\n \\n\\nFig. 2 Hierarchical clustering dendrogram \\n\\nThe hierarchical clustering methods could be further grouped in three categories based on similarity \\n\\nmeasures or linkages [28] as summarized in following sections. \\n\\n2.1.1 Single-linkage Clustering \\n\\nThis type of clustering is often called as the connectedness, the minimum method or the nearest \\n\\nneighbour method. In single-linkage clustering, the link between two clusters is made by a single \\n\\nelement pair, namely those two elements (one in each cluster) that are closest to each other. In this \\n\\nclustering, the distance between two clusters is determined by nearest distance from any member of \\n\\none cluster to any member of the other cluster, this also defines similarity. If the data is equipped with \\n\\nsimilarities, the similarity between a pair of clusters is considered to be equal to the greatest similarity \\n\\nfrom any member of one cluster to any member of the other cluster [29]. Fig. 3 shows the mapping of \\n\\nsingle linkage clustering. The criteria between two sets of clusters A and B is as follow: \\n\\n\\uf07b \\uf07dmin ( , ) : ,d a b a A b B\\uf0ce \\uf0ce\\n                                                      (2) \\n\\n \\n\\nFig. 3 Mapping of single linkage clustering \\n\\n2.1.2 Complete-linkage Clustering   \\n\\nIn complete-linkage clustering also called the diameter, the maximum method or the furthest \\n\\nneighbour method; the distance between two clusters is determined by longest distance from any \\n\\nmember of one cluster to any member of the other cluster [30]. Fig. 4 shows the mapping of complete \\n\\nlinkage clustering. The criteria between two sets of clusters A and B is as follow: \\n\\n\\uf07b \\uf07dmax ( , ) : ,d a b a A b B\\uf0ce \\uf0ce\\n                                                (3) \\n\\n\\n\\n \\n\\nFig. 4 Mapping of complete linkage clustering \\n\\n2.1.3 Average-linkage Clustering  \\n\\nIn average linkage clustering also known as minimum variance method; the distance between two \\n\\nclusters is determined by the average distance from any member of one cluster to any member of the \\n\\nother cluster [31]. Fig. 5 shows the mapping of average linkage clustering. The criteria between two \\n\\nsets of clusters A and B is as follow: \\n\\n1\\n( , )\\n\\n| || |a A b B\\n\\nd a b\\nA B \\uf0ce \\uf0ce\\n\\n\\uf0e5\\uf0e5\\n                                                             (4) \\n\\n \\n\\nFig. 5 Mapping of average linkage clustering \\n\\n2.1.4 Steps of Agglomerative and Divisive Clustering \\n\\n(i) Steps of agglomerative clustering \\n\\n \\n\\n \\n\\n  \\n\\n \\n\\n(ii) Steps of divisive clustering \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n1. Make each point a separate cluster \\n\\n2. Until the clustering is satisfactory \\n\\n3. Merge the two clusters with the smallest inter-cluster distance \\n\\n4. End \\n\\n \\n\\n1. Construct a single cluster containing all points \\n\\n2. Until the clustering is satisfactory \\n\\n3. Split the cluster that yields the two components with the largest inter-cluster distance \\n\\n4. End \\n\\n \\n\\n\\n\\nThe common criticism for classical HC algorithms is that they lack robustness and are, hence, \\n\\nsensitive to noise and outliers. Once an object is assigned to a cluster, it will not be considered again, \\n\\nwhich means that HC algorithms are not capable of correcting possible previous misclassification. The \\n\\ncomputational complexity for most of HC algorithms is at least O(N2) and this high cost limits their \\n\\napplication in large-scale data sets. Other disadvantages of HC include the tendency to form spherical \\n\\nshapes and reversal phenomenon, in which the normal hierarchical structure is, distorted [50]. With \\n\\nthe requirement of large-scale datasets in recent years, the HC algorithms are also enriched with some \\n\\nnew techniques as modifications to classical HC methods presented in following section.  \\n\\n2.1.5 Enhanced Hierarchical Clustering \\n\\nThe main deficiency of hierarchical clustering [33] is that after the two points of the clusters are \\n\\nlinked to each other, they cannot move in other clusters in a hierarchy. Few algorithms, which use \\n\\nhierarchical clustering with some enhancements, are given below:  \\n\\n(i) Balanced Iterative Reducing and Clustering Using Hierarchies (BIRCH) \\n\\nBIRCH [131] contains the idea of cluster features (CF). CF is the triple (n, LS, SS) where n is \\n\\nthe number of data objects in the cluster, LS is the linear sum of the attribute values of the \\n\\nobjects in the cluster and SS is the sum of squares of the attribute values of the objects in the \\n\\ncluster. These are stored in a CF-tree form, so no need to keep all tuples or all clusters in main \\n\\nmemory, but only, their tuples [34]. The main motivations of BIRCH lie in two aspects, the \\n\\nability to deal with large data sets and the robustness to outliers [131]. Also the BIRCH can \\n\\nachieve a comutational complexity of O(N). \\n\\n(ii) Clustering Using Representatives (CURE) \\n\\nCURE [35] is a clustering technique for dealing with large-scale databases, which is robust \\n\\ntowards outliers and accepts clusters of various shapes and sizes. Its performance is good with \\n\\n2-D data sets. BIRCH and CURE both handle outliers well but CURE clustering quality is \\n\\nbetter than that of BIRCH [35]. On the reverse, in terms of time complexity, BIRCH is better \\n\\nthan CURE as it attains computational complexity of O(N) compared to CURE O(N2logN).  \\n\\n(iii) ROCK \\n\\nROCK [130] is applied for categorical data sets which follows the agglomerative hierarchical \\n\\nclustering algorithm. It is based on the number of links between two records; links capture the \\n\\nnumber of other records, which are very similar to each other. This algorithm does not use \\n\\nany distance function. CURE [35] also proposed ROCK, which uses a random sample \\n\\nstrategy to handle large datasets. \\n\\n(iv) CHAMELEON \\n\\nCHAMELEON [36] is a hierarchical clustering algorithm, where clusters are merged only if \\n\\nthe interconnectivity and closeness (proximity) between two clusters are high relative to the \\n\\ninternal interconnectivity of the clusters and closeness of items within the clusters. One \\n\\nlimitation of CHAMELEON is that it is known for low dimensional spaces, and was not \\n\\napplied to high dimensions.  \\n\\nTable1 Features of hierarchical clustering-based enhanced methods \\n\\nName Type of data Complexity Ability to handle high \\n\\ndimensional data \\n\\nBIRCH Numerical O(N) No \\n\\nCURE Numerical O(N2logN) Yes \\n\\nROCK Categorical O(N2+Nmmma+N2logN)* No \\n\\nCHEMELEON Numerical/ Categorical O(Nm + NlogN + m2logN)** No \\n\\n*mm is the maximum number of neighbours for a point ma is the average number of \\n\\nneighbours for a point. \\n\\n**m is the number of initial sub-clusters produced by the graph partitioning algorithm. \\n\\n\\n\\n2.2 Partition Clustering Methods \\n\\nPartitional clustering is opposite to hierarchical clustering; here data are assigned into K clusters \\n\\nwithout any hierarchical structure by optimizing some criterion function [37]. The most commonly \\n\\nused criterion is the Euclidean distance, which finds the minimum distance between points with each \\n\\nof the available clusters and assigning the point to the cluster. The algorithms [33] studied in this \\n\\ncategory include: k-means [38], PAM [173], CLARA [173], CLARANS [174], Fuzzy C-means, \\n\\nDBSCAN etc. Fig. 6 shows the partitional clustering approach.  \\n\\n \\n\\nData points Partitional clusters \\n\\nFig. 6 Partitional clustering approaches \\n\\n2.2.1 K-means Clustering \\n\\nK-means algorithm is one of the best-known, bench marked and simplest clustering algorithms \\n\\n[37, 38], which is mostly applied to solve the clustering problems. In this procedure the given data set \\n\\nis classified through a user defined number of clusters, k. The main idea is to define k centroids, one \\n\\nfor each cluster. The objective function J is given as follows: \\n\\nMinimize \\n\\n2\\n( )\\n\\n1 1\\n\\nk n\\nj\\n\\ni j\\n\\nj i\\n\\nJ x c\\n\\uf03d \\uf03d\\n\\n\\uf03d \\uf02d\\uf0e5\\uf0e5\\n                                                          (5) \\n\\nwhere \\n\\n2\\n( )j\\n\\ni jx c\\uf02d\\n is a chosen distance measure between a data point \\n\\n( )j\\n\\nix\\nand the cluster centre jc\\n\\n, Fig. \\n\\n7 shows the flow diagram of K-means algorithm. \\n\\nAn algorithm similar to k-means, known as the Linde-Buzo-Gray (LBG) algorithm, was \\n\\nsuggested for vector quantization (VQ) [39] for signal compression. In this context, prototype vectors \\n\\nare called code words, which constitute a code book. VQ aims to represent the data with a reduced \\n\\nnumber of elements while minimizing information loss. Although K- Means clustering is still one of \\n\\nthe most popular clustering algorithms yet few limitation are associated with K Means clustering \\n\\ninclude: (a) There is no efficient and universal method for identifying the initial partitions and the \\n\\nnumber of clusters K and (b) K-means is sensitive to outliers and noise. Even if an object is quite far \\n\\naway from the cluster centroid, it is still forced into a cluster and, thus, distorts the cluster shapes [50]. \\n\\n \\n\\nFig. 7 Flow diagram of K -means algorithm \\n\\n\\n\\nThe procedure of K-means algorithm is composed of the following steps: \\n\\n1. Initialization: Suppose we decide to form K clusters of the given dataset. Now take K \\n\\ndistinct points (patterns) randomly. These points represent initial group centroids. As \\n\\nthese centroids will be changing after each iteration before clusters are fixed, there is \\n\\nno need to spend time in decision of choosing the centroids.  \\n\\n2. Assign each object to the group that has the closest centroid. \\n\\n3. When all objects have been assigned, recalculate the positions of the K centroids. \\n\\n4. Repeat Steps 2 and 3 until the centroids no longer move. This produces a separation \\n\\nof the objects into groups from which the metric to be minimized can be calculated.  \\n\\n2.2.2 Fuzzy C-means Clustering \\n\\nFuzzy c-means (FCM) is a clustering method which allows one point to belong to two or more \\n\\nclusters unlike K-means where only one cluster is assigned to each point. This method was developed \\n\\nby Dunn in 1973 [40] and improved by Bezdek in 1981 [41]. The procedure of fuzzy c-means [50] is \\n\\nsimilar to that of K-means. It is based on minimization of the following objective function: \\n2\\n\\n1 1\\n\\n|| || ;1\\nN c\\n\\nm\\n\\nm ij i j\\n\\ni j\\n\\nJ u x v m\\n\\uf03d \\uf03d\\n\\n\\uf03d \\uf02d \\uf03c \\uf03c \\uf0a5\\uf0e5 \\uf0e5\\n                                                     (6)  \\n\\nwhere m is fuzzy partition matrix exponent for controlling the degree of fuzzy overlap, with m > 1. \\n\\nFuzzy overlap refers to how fuzzy the boundaries between clusters are, that is the number of data \\n\\npoints that have significant membership in more than one cluster, uij is the degree of membership of xi \\n\\nin the cluster j, xi is the i-th pattern of d-dimension data, vj is j-th cluster center of the d-dimension and \\n*\\n\\n  is any norm expressing the similarity between any measured data and the center. \\n\\nProcedure for FCM \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nFCM suffers from initial partition dependence, as well as noise and outliers like k-means. Yager \\n\\nand Filev [42] proposed the mountain method to estimate the cluster centers as an initial partition. \\n\\nGath and Geva [43] addressed the initialization problem by dynamically adding cluster prototypes, \\n\\nwhich are located in the space that is not represented well by the previously generated centers. \\n\\n1. Set up a value of c (number of cluster); \\n\\n2. Select initial cluster prototype 1 2, , , cV V V  from iX  , 1,2, ,i N\\uf03d  ; \\n\\n3. Compute the distance i jX V\\uf02d\\n between objects and prototypes; \\n\\n4. Computer the elements of the fuzzy partition matrix \\n( 1,2, ,i N\\uf03d ; 1,2, ,j c\\uf03d \\uf0bc\\uf0bc ) 1\\n\\n1\\n\\nc i j\\n\\nij l\\ni l\\n\\nx v\\nu\\n\\nx v\\n\\n\\uf02d\\n\\n\\uf03d\\n\\n\\uf0e9 \\uf0f9\\uf0e6 \\uf0f6\\uf02d\\n\\uf0ea \\uf0fa\\uf0e7 \\uf0f7\\uf03d\\n\\n\\uf0e7 \\uf0f7\\uf02d\\uf0ea \\uf0fa\\n\\uf0e8 \\uf0f8\\uf0eb \\uf0fb\\n\\n\\uf0e5\\n \\n\\n5. Compute the cluster prototypes ( 1,2, ,j c\\uf03d \\uf0bc\\uf0bc ) \\n2\\n\\n1\\n\\n2\\n\\n1\\n\\nN\\n\\nij ii\\n\\nj N\\n\\niji\\n\\nu x\\nV\\n\\nu\\n\\n\\uf03d\\n\\n\\uf03d\\n\\n\\uf03d\\n\\uf0e5\\n\\n\\uf0e5  \\n\\n6. Stop if the convergence is attained or the number of iterations exceeds a \\ngiven limit. Otherwise, go to step 3. \\n\\n \\n\\nhttp://home.dei.polimi.it/matteucc/Clustering/tutorial_html/cmeans.html#dunn#dunn\\nhttp://home.dei.polimi.it/matteucc/Clustering/tutorial_html/cmeans.html#bezdek#bezdek\\n\\n\\nChanging the proximity distance can improve the performance of FCM in relation to outliers [44]. In \\n\\nanother approach for reducing the effect of noise and outliers, Keller [45] interpreted memberships as \\n\\n“the compatibility of the points with the class prototype” rather than as the degree of membership. \\n\\nThis relaxes uij = 1 to uij > 0 and results in a possibilistic K-means clustering algorithm. \\n\\nThe conditions for a possibilistic fuzzy partition matrix are: \\n\\n\\uf05b \\uf05d0,1 ,1 , 1iju i N j C\\uf0ce \\uf0a3 \\uf0a3 \\uf0a3 \\uf0a3\\n                                                  (7) \\n\\n, 0,j iju i\\uf024 \\uf03e \\uf022\\n                                                                    (8) \\n\\n1\\n\\n0 , 1\\nN\\n\\nij\\n\\ni\\n\\nu N j C\\n\\uf03d\\n\\n\\uf03c \\uf03c \\uf0a3 \\uf0a3\\uf0e5\\n                                                          (9) \\n\\nTable 2 Features of partition clustering based techniques \\n\\nName Type of data complexity Ability to handle high \\n\\ndimensional data \\n\\nK-Mean Numerical O(N) No \\n\\nPAM Numerical O(K(N-K)2)* No \\n\\nCLARA Numerical O(K(40+K)2+K(N-K)) No \\n\\nCLARANS Numerical O(KN2) No \\n\\nFuzzy C-Means Numerical O(N) No \\n\\n*N is the number of points in the dataset and K is the number of clusters defined.  \\n\\nThe k-means algorithms have problems like defining the number of clusters initially, susceptibility to \\n\\nlocal optima, and sensitivity to outliers, memory space and unknown number of iteration steps that are \\n\\nrequired to cluster.  The fuzzy C means clustering are really suitable for handling the issues related to \\n\\nunderstand ability of patterns, incomplete/noisy data, mixed media information, human interaction and \\n\\nit can provide approximate solutions faster. They have been mainly used for discovering association \\n\\nrules and functional dependencies as well as image retrieval. However the time complexity of K \\n\\nmeans is much less than that of FCM thus K means works faster than FCM [191]. \\n\\nSome of the advantages of partition based algorithms includes that they are (i) relatively scalable and \\n\\nsimple and (ii) suitable for datasets with compact spherical clusters that are well-separated. However, \\n\\ndisadvantages with these algorithms include poor (i) cluster descriptors (ii) reliance on the user to \\n\\nspecify the number of clusters in advance (iii) high sensitivity to initialization phase, noise and outliers \\n\\nand (iv) inability to deal with non-convex clusters of varying size and density [175]. \\n\\n3 Measures of Similarities \\n\\nSimilarity of objects within a cluster plays the most important role in clustering process. A good \\n\\ncluster finds maximum similarity among its objects. The measure of similarity in cluster is mainly \\n\\ndecided by the distance among its members. In a conventional cluster (non-fuzzy), a member either \\n\\nbelongs to a cluster wholly or not at all. Many clustering methods use distance measures to determine \\n\\nthe similarity or dissimilarity between any pair of objects [22]. It is useful to denote the distance \\n\\nbetween two instances xi and xj as: d(xi, xj). A valid distance measure should be symmetric i.e d(xi, xj) \\n\\n= d(xj, xi) and obtain its minimum value (ideally zero) in case of identical vectors. The distance \\n\\nmeasure is called a metric distance measure if it also satisfies the following properties: \\n\\nTriangle inequality   \\n( , ) ( , ) ( , ) , ,i k i j j k i j kd x x d x x d x x x x x S\\uf0a3 \\uf02b \\uf022 \\uf0ce\\n\\n                              (10) \\n\\n( , ) 0 ,i j i j i jd x x x x x x S\\uf03d \\uf0de \\uf03d \\uf022 \\uf0ce\\n                                                (11) \\n\\n\\n\\n3.1 Minkowski: Distance Measures for Numeric Attributes \\n\\nA measurement of distance is a fundamental operation in the unsupervised learning process [91]. \\n\\nSmaller is the distance between any two objects; closer these objects are assumed on the basis of \\n\\nsimilarity. A family of distance measures is the Minkowski metrics [29], where the distance is \\n\\nmeasured by following equation  r\\nd\\n\\nk\\n\\nr\\n\\njkikr\\nxxij\\n\\n/1\\n\\n1 \\uf0fe\\n\\uf0fd\\n\\uf0fc\\n\\n\\uf0ee\\n\\uf0ed\\n\\uf0ec\\n\\n\\uf02d\\uf03d \\uf0e5\\n\\uf03d                                                  (12) \\n\\nwhere xik is the value of the k-th variable for entity i, xjk is the value of the k-th variable for entity j. The \\n\\nmost popular and common distance measure is the Euclidean or L2 norm (r =2). More details on \\n\\nunsupervised classification for various non-Euclidean distances can be seen in Saxena et al. [160].   \\n\\n3.2 Cosine Measure \\n\\nCosine Measure [153] is a popular similarity score in text mining and information retrieval [152]. \\n\\nThe normalized inner product for Cosine measure is defined as:  \\n\\n( , )\\n\\nT\\n\\ni j\\n\\ni j\\n\\ni j\\n\\nx x\\nd x x\\n\\nx x\\n\\n\\uf0d7\\n\\uf03d\\n\\n\\uf0d7\\n                                                                 (13) \\n\\n3.3 Pearson Correlation Measure  \\n\\nCorrelation coefficient is first discovered by Bravais [154] and later shown by Person [155]. The \\n\\nnormalized Pearson correlation for two vectors xi and xj is defined as: \\uf028 \\uf029 \\uf028 \\uf029\\n( , )\\n\\nT\\n\\ni i j j\\n\\ni j\\n\\ni i j j\\n\\nx x x x\\nd x x\\n\\nx x x x\\n\\n\\uf02d \\uf0d7 \\uf02d\\n\\uf03d\\n\\n\\uf02d \\uf0d7 \\uf02d\\n                                                      (14) \\n\\nwhere ix\\n denotes the average feature value of x over all dimensions. \\n\\n3.4 Extended Jaccard Measure \\n\\nStrehl and Ghosh [107] represented the extended Jaccard measure as follows: \\n22\\n\\n( , )\\n\\nT\\n\\ni j\\n\\ni j\\nT\\n\\ni j i j\\n\\nx x\\nd x x\\n\\nx x x x\\n\\n\\uf0d7\\n\\uf03d\\n\\n\\uf02b \\uf02d \\uf0d7\\n                                                          (15) \\n\\n3.5 Dice Coefficient Measure \\n\\nIt was independently developed by the Thorvald Sørensen[156] and Raymond Dice [157] The \\n\\ndice coefficient measure is similar to the extended Jaccard measure and it is defined as: \\n22\\n\\n2\\n( , )\\n\\nT\\n\\ni j\\n\\ni j\\n\\ni j\\n\\nx x\\nd x x\\n\\nx x\\n\\n\\uf0d7\\n\\uf03d\\n\\n\\uf02b\\n                                                                  (16) \\n\\n3.6  Choice of Suitable Similarity Measure \\n\\nThe measures of similarities have been applied on millions of applications in clustering. In fact every \\n\\nclustering problem applies one of the similarity measures. The Euclidean distance is mostly applied to \\n\\nfind similarity between two objects, which are expressed numerically. Euclidean distance is highly \\n\\nsensitive to noise and usually not applied to data with hundreds of attributes also features with high \\n\\nvalues tend to dominate others [50] so it may be applied when translations of non-numeric objects to \\n\\nnumeric values are almost nil or minimum. Jaccard similarity coefficient is suitable sufficiently to be \\n\\nemployed in the documents or word similarity measurement. In efficiency measurement, the program \\n\\nperformance can deal appropriately with high stability when failure and mistake spelling occurred. \\n\\nNevertheless, this method is not able to detect the over-type words in the data sets [192]. Pearson \\n\\ncorrelation is usually unable to detect the difference between two variables [50]. Cosine similarity is \\n\\nhttp://en.wikipedia.org/wiki/Thorvald_S%C3%B8rensen\\nhttp://en.wikipedia.org/w/index.php?title=Lee_Raymond_Dice&action=edit&redlink=1\\n\\n\\nalso a good choice for document clustering, it is invariant to rotation but not to linear transformations \\n\\n[50].  \\n\\n4 Variants of Clustering Methods \\n\\n4.1 Graph (Theoretic) Clustering  \\n\\nThe graph theoretic clustering is a method that represents clusters via graphs. The edges of the \\n\\ngraph connect the instances represented as nodes. A well-known graph-theoretic algorithm is based on \\n\\nthe minimal spanning tree (MST) [46]. Inconsistent edges are edges whose weight (in the case of \\n\\nclustering length) is significantly larger than the average of nearby edge lengths. Another graph \\n\\ntheoretic approach constructs graphs based on limited neighbourhood sets [47]. The graph theoretic \\n\\nclustering is convenient to represent clusters via graphs but is weak in handling outliers especially in \\n\\nMST as well as detecting overlapping of clusters [176]. \\n\\n      The graph clustering [177] involves the task of dividing nodes into clusters, so that the edge \\n\\ndensity is higher within clusters as opposed to across clusters. A natural, classic and popular statistical \\n\\nsetting for evaluating solutions to this problem is the stochastic block model, also referred to as the \\n\\nplanted partition model. The general graph l-partition problem is to partition the nodes of an \\n\\nundirected graph into l equal-sized groups so as to minimize the total number of edges that cross \\n\\nbetween groups. Condon [178] presented a simple, linear-time algorithm for the graph l-partition \\n\\nproblem and analyzed it on a random “planted l-partition” model. In this model, the n nodes of a graph \\n\\nare partitioned into l groups, each of size n/l; two nodes in the same group are connected by an edge \\n\\nwith some probability p, and two nodes in different groups are connected by an edge with some \\n\\nprobability r<p. They showed that if p−r≥n−1/2+ϵ for some constant ϵ, then the algorithm finds the \\n\\noptimal partition with probability 1− exp(−nΘ(ε)). Graph clustering decomposes a network into sub \\n\\nnetworks based on some topological properties. In general we look for dense sub networks as shown \\n\\nin Fig. 8. \\n\\n \\n\\nFig. 8 Sub-network clustering of graph \\n\\n      Spectral Clustering, proposed by Donath and Hoffman [179], is an emerging technique under \\n\\ngraph clustering which consists of algorithms cluster points using eigenvectors of matrices derived \\n\\nfrom the data. In the machine learning community, spectral clustering has been made popular by the \\n\\nworks of Shi and Malik [180]. A useful tutorial is available on spectral clustering by Luxburg [181]. \\n\\nThe success of spectral clustering is mainly based on the fact that it does not make strong assumptions \\n\\non the form of the clusters. As opposed to k-means, where the resulting clusters form convex sets (or, \\n\\nto be precise, lie in disjoint convex sets of the underlying space), spectral clustering can solve very \\n\\n\\n\\ngeneral problems like intertwined spirals. Moreover, spectral clustering can be implemented \\n\\nefficiently even for large data sets, as long as we make sure that the similarity graph is sparse. Once \\n\\nthe similarity graph is chosen, we just have to solve a linear problem, and there are no issues of getting \\n\\nstuck in local minima or restarting the algorithm for several times with different initializations. \\n\\nHowever, we have already mentioned that choosing a good similarity graph is not trivial, and spectral \\n\\nclustering can be quite unstable under different choices of the parameters for the neighborhood graphs. \\n\\nSo spectral clustering cannot serve as a “black box algorithm” which automatically detects the correct \\n\\nclusters in any given data set. But it can be considered as a powerful tool which can produce good \\n\\nresults if applied with care [181]. More literature (partially) on graph and spectral clustering can be \\n\\nseen in [182-190]. \\n\\n4.2 Spectral Clustering Algorithms [181] \\n\\nNow we would like to state the most common spectral clustering algorithms. We assume that our \\n\\ndata consists of n “points” x1, . . . , xn, which can be arbitrary objects. We measure their pair wise \\n\\nsimilarities sij = s(xi , xj ) by some similarity function which is symmetric and non-negative, and we \\n\\ndenote the corresponding similarity matrix by S = (sij )I, j=1, ..., n. \\n\\n4.2.1 Un-normalized Spectral Clustering \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n4.2.2 Normalized Spectral Clustering According to Shi and Malik (2000)[180] \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n1. Input: Similarity matrix S ∈ R n×n, number k of clusters to construct. \\n\\n2. Construct a similarity graph by one of the ways described in Section 2 [181]. Let W be its \\n\\nweighted adjacency matrix. \\n\\n3. Compute the un-normalized Laplacian L. \\n\\n4. Compute the first k eigenvectors u1, . . . , uk of L. \\n\\n5. Let U ∈ R n×k be the matrix containing the vectors u1, . . . , uk as columns. \\n\\n6. For i = 1, . . . , n, let yi ∈ R k be the vector corresponding to the i-th row of U. \\n\\n7. Cluster the points (yi)i=1,...,n  in R k with the k-means algorithm into clusters C1, . . . , Ck. \\n\\n8. Output: Clusters A1, . . . , Ak with Ai = {j| yj ∈ Ci}. \\n\\n \\n\\n1. Input: Similarity matrix S ∈ R n×n, number k of clusters to construct.  \\n\\n2. Construct a similarity graph by one of the ways described in Section 2 [181]. Let W be its \\n\\nweighted adjacency matrix.  \\n\\n3. Compute the unnormalized Laplacian L. \\n\\n4. Compute the first k generalized eigenvectors u1, . . . , uk of the generalized eigen problem \\n\\nLu = λDu. \\n\\n5. Let U ∈ R n×k be the matrix containing the vectors u1, . . . , uk as columns.  \\n\\n6. For i = 1, . . . , n, let yi ∈ R k be the vector corresponding to the i-th row of U.  \\n\\n7. Cluster the points (yi)i=1,...,n in R k with the k-means algorithm into clusters C1, . . . , Ck.  \\n\\n8. Output: Clusters A1, . . . , Ak with Ai = {j| yj ∈ Ci}. \\n\\n\\n\\n4.3 Model Based Clustering Methods \\n\\nModel based clustering methods optimize as well as find the suitability of given data with some \\n\\nmathematical models. Similar to conventional clustering; model-based clustering methods also detect \\n\\nfeature details for each cluster, where each cluster represents a concept or class. Decision trees and \\n\\nneural networks are two most frequently used induction methods. \\n\\n(i) Decision Trees \\n\\nThe representation of data in decision tree [19] is modelled by a hierarchical tree, in which \\n\\neach leaf denotes a concept and implies a probabilistic description of that concept. There are \\n\\nmany algorithms, which produce classification trees for defining the unlabelled data. Number \\n\\nof algorithms that have been proposed for conceptual clustering are follows: CLUSTER/2 by \\n\\nMichalski and Stepp [93], COBWEB by Fisher [48], CYRUS by Kolodner [95], GALOIS by \\n\\nCarpineto and Romano [96], GCF by Talavera and Béjar [97], INC by Hadzikadic and Yun \\n\\n[98], ITERATE by Biswas, Weinberg and Fisher [99], LABYRINTH by Thompson and \\n\\nLangley [100], SUBDUE by Jonyer, Cook and Holder [101], UNIMEM by Lebowitz [102] \\n\\nand WITT by Hanson and Bauer [103]. COBWEB is one of the best known algorithms, where \\n\\neach concept defines a set of objects and each object defined as a binary values property list. \\n\\nIts aim is to achieve high predictability of nominal variable values, given a cluster. This \\n\\nalgorithm is not suitable for clustering large database data [48].  \\n\\n(ii) Neural Networks \\n\\n Neural networks [49] represent each cluster by a neuron, whereas input data is also \\n\\nrepresented by neurons, which are connected to the prototype neurons. Each connection is \\n\\nattributed by some weight, which is initialized randomly before learning of these weights \\n\\nadaptively. A very popular neural algorithm for clustering is the self-organizing map (SOM) \\n\\n[104, 105]. SOM is commonly used for vector quantization, feature extraction and data \\n\\nvisualization along with clustering analysis. This algorithm constructs a single-layered \\n\\nnetwork as shown in Fig. 9. The learning process takes place in a “winner-takes-all” fashion: \\n\\nThe prototype neurons compete for the current instance. The winner is the neuron whose \\n\\nweight vector is closest to the instance currently presented. The winner and its neighbours \\n\\nlearn by having their weights adjusted. While SOFMs has the merits of input space density \\n\\napproximation and independence of the order of input patterns, a number of user dependent \\n\\nparameters cause problems when applied in real practice. Like the K-means algorithm,SOFM \\n\\nneed to predefine the size of the lattice, i.e., the number of clusters, which is unknown for \\n\\nmost circumstances. Additionally, trained SOFM may be suffering from input space density \\n\\nmis representation [49], where areas of low pattern density may be over represented and areas \\n\\nof high density under represented [50]. \\n\\n \\n\\nFig. 9 Model of a single layered network \\n\\n4.4 Mixture Density-Based Clustering  \\n\\nXu and Wunsch [50, 51] described clustering in the perspective of probability that data objects are \\n\\ndrawn from a specific probability distribution and the overall distribution of the data is assumed to be \\n\\na mixture of several distributions [53]. Data points [117] can be derived from different types of density \\n\\nhttp://en.wikipedia.org/wiki/Cobweb_(clustering)\\n\\n\\nfunctions (e.g., multivariate Gaussian or t-distribution), or from the same families but with different \\n\\nparameters. The aim of these methods is to identify the clusters and their distribution. Cheeseman and \\n\\nStutz introduced an algorithm named AUTOCLASS [55], which is widely used and covers a broad \\n\\nvariety of distributions, including Gaussian, Bernoulli, Poisson, and log-normal distributions. Ester et \\n\\nal. [54] demonstrated an algorithm called DBSCAN (density-based spatial clustering of applications \\n\\nwith noise), which discovers clusters of arbitrary shapes and is efficient for large spatial databases.  \\n\\nOther well-known density-based techniques are: SNOB proposed by Wallace and Dowe in 1994 \\n\\n[56] and MCLUST introduced by Fraley and Raftery in 1998 [27]. Among these methods, the \\n\\nexpectation-maximization (EM) algorithm is the most popular [52, 56]. For EM algorithm, the log \\n\\nlikelihood function to maximize is as follows: \\nln ( | ) ln ( , | )\\n\\nY\\n\\np X p X Y\\uf051 \\uf03d \\uf051\\uf0e5\\n                                                            (17) \\n\\nwhere X denotes the set of all observed data \\n\\uf07b \\uf07d\\uf028 \\uf0291,..., NX x x\\uf03d\\n\\n, and Y denotes the set of all latent \\n\\nvariables \\n\\uf07b \\uf07d\\uf028 \\uf0291,..., NY y y\\uf03d\\n\\n. The complete data set is formed as \\n\\uf028 \\uf029 \\uf028 \\uf029\\uf07b \\uf07d, ,i iX Y x y\\uf03d\\n\\n and the joint \\n\\ndistribution \\n\\uf028 \\uf029, |p x y \\uf051\\n\\n is ruled by a set of parameters.  The major disadvantages for EM algorithm are \\n\\nthe sensitivity to the selection of initial parameters, the effect of a singular co-variance matrix, the \\n\\npossibility of convergence to a local optimum, and the slow convergence rate [50] [52]. \\n\\nProcedure of EM algorithm \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n4.5 Grid-Based Clustering Methods  \\n\\nThese methods partition the space into a finite number of cells that form a grid structure on which \\n\\nall of the operations for clustering are performed. The main advantage of the approach is its fast \\n\\nprocessing time [122], no need of distance computations and easy to determine which clusters are \\n\\nneighbouring.  \\n\\nThe basic steps of Grid based algorithm \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nThere are many others interesting grid based techniques including: STING (statistical information \\n\\ngrid approach) by Wang, Yang and Muntz [57] in 1997, one of the highly scalable algorithm and has \\n\\nthe ability to decompose the data set into various levels of detail. STING retrieves spatial data and \\n\\ndivides into rectangular cells corresponding to different levels of resolution as shown in Fig. 10. \\n\\n \\n\\n1. Initialize the parameters \\nold\\uf051   \\n\\n2. E step: evaluate ( | , )oldp Y X \\uf051  \\n\\n3. M step: re-estimate the parameters \\narg max ( )new L\\uf051\\uf051 \\uf03d \\uf051\\n\\n \\n\\n4. Check for convergence. If the convergence criterion is not \\n\\nsatisfied, let \\nold new\\uf051 \\uf0ac\\uf051 and return to step 2.  \\n\\n \\n\\n1. Define a set of grid cells \\n\\n2. Assign objects to the appropriate grid cell and compute \\n\\nthe density of each cell \\n\\n3. Eliminate cells, whose density is below a certain \\n\\nthreshold \\n\\n4. Form clusters from contiguous groups of dense cells \\n\\n \\n\\n\\n\\n \\n\\nFig. 10 Rectangular cells corresponding to different levels of resolution \\n\\nEach cell at a higher level is partitioned into a number of smaller cells in the next lower level. Then \\n\\nmean, variance, minimum, maximum of each cell is computed by using the normal and uniform \\n\\ndistribution. Statistical information of each cell is calculated and stored in advance and it uses a top \\n\\ndown approach to answer spatial data queries. Wave Cluster [58] introduced by Sheikholeslami et al. \\n\\n[58] uses multi-resolution approach like STING and allows natural clustering to become more \\n\\ndistinguishable. It uses a signal processing technique that decomposes a signal into different frequency \\n\\nsub-band and data are transformed to preserve relative distance between objects at different levels of \\n\\nresolution. It is highly scalable and can handle outliers well. It is not suitable for high dimensional data \\n\\nset. It can be considered as both grid-based and density-based. CLIQUE is developed by Agrawal et \\n\\nal. [59] in 1998, which can be considered as both density-based and grid based clustering methods. It \\n\\nautomatically finds subspaces of high dimensional data space that allow better clustering than original \\n\\nspace. The accuracy of the clustering result may be degraded at the expense of simplicity of the \\n\\nmethod CLIQUE. \\n\\n4.6 Evolutionary Approaches Based Clustering Methods \\n\\nThe famous evolutionary approaches [60] include evolution strategies (ES) [61], evolutionary \\n\\nprogramming (EP) [62], genetic algorithm (GA) [63, 64], particle swarm optimization (PSO) [65-66], \\n\\nant colony optimization (ACO) [67] etc. \\n\\nThe common approach of evolutionary techniques to data clustering is as follows: \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nOut of these approaches, GA has been most frequently used in clustering, where solutions are in \\n\\nthe form of binary strings. In GAs, a selection operator propagates solutions from the current \\n\\ngeneration to the next generation based on their fitness Selection employs a probabilistic scheme so \\n\\n1. Choose a random population of solutions. Each solution here corresponds \\n\\nto valid k partitions of the data.  \\n\\n2. Associate a fitness value with each solution. Typically fitness is inversely \\n\\nproportional to the squared error value. Higher the error, smaller the \\n\\nfitness and vice versa. \\n\\n3. A solution with a small squared error will have a larger fitness value.  \\n\\n4. Use the evolutionary operators’ viz.  selection, recombination and \\n\\nmutation to generate the next population of solutions. \\n\\n5. Evaluate the fitness values of these solutions. \\n\\n6. Repeat step until some termination condition is satisfied.  \\n\\n \\n\\n\\n\\nthat solutions with higher fitness have a higher probability of getting reproduced. A major problem \\n\\nwith GAs is their sensitivity to the selection of various parameters such as population size crossover \\n\\nand mutation probabilities etc. Grefenstette [123] has studied this problem and suggested guidelines \\n\\nfor selecting these control parameters.  \\n\\nThe general steps of GA for clustering are: \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n4.7 Search Based Clustering Approaches  \\n\\nSearch techniques are basically used to obtain the optimum value (minimum or maximum) of the \\n\\ncriterion function (e.g. distance) called objective function also. The search based approaches are \\n\\ncategorized into stochastic and deterministic search techniques. The stochastic search techniques can \\n\\nevolve an approximate optimal solution (based on fitness value). Most of the stochastic techniques are \\n\\nevolutionary approaches based. The rest of the search techniques come under deterministic search \\n\\ntechniques which guarantee an optimal solution by performing exhaustive enumeration. The \\n\\ndeterministic approaches are typically greedy descent approaches. The stochastic search techniques are \\n\\neither sequential or parallel such as simulated annealing (SA) [172] while evolutionary approaches are \\n\\ninherently parallel. Simulated annealing procedures are designed to avoid or recover from solutions \\n\\nwhich correspond to local optima of the objective functions. This is accomplished by accepting with \\n\\nsome probability a new solution for the next iteration of lower quality as measured by the criterion \\n\\nfunction. The probability of acceptance is governed by a critical parameter called the temperature by \\n\\nanalogy with annealing in metals which is typically specified in terms of a starting first iteration and \\n\\nfinal temperature value. Al Sultan et al [92] studied the effects of control parameters on the \\n\\nperformance of the algorithm and used SA to obtain near optimal partition of the data SA is \\n\\nstatistically guaranteed to find the global optimal solution. \\n\\nThe SA algorithm can be slow in reaching the optimal solution because optimal results require the \\n\\ntemperature to be decreased very slowly from iteration to iteration. Tabu search [68, 69] like SA is a \\n\\nmethod designed to cross boundaries of feasibility or local optimality and to systematically impose \\n\\nand release constraints to permit exploration of otherwise forbidden regions. Tabu search was used to \\n\\nsolve the clustering problem in [3]. \\n\\n4.8 Collaborative Fuzzy Clustering \\n\\nThis is relatively a recent type of clustering which has various applications. The database is \\n\\ndistributed on several sites. The collaborative clustering proposed by Pedrycz [70-73] concerns a \\n\\nprocess of revealing a structure being common or similar to a number of subsets. There are mainly two \\n\\nforms of collaborative clustering; horizontal and vertical collaborative clustering [74]. In horizontal \\n\\ncollaborative clustering, same database is split into different subsets of features, each subset having all \\n\\npatterns in the database. The horizontal collaborative clustering has been applied for Mamdani type \\n\\nfuzzy inference system [124] in order to decide some association between datasets. In vertical \\n\\ncollaborative clustering, database is divided into subsets of patterns such that each pattern of any \\n\\nsubset has all features.  \\n\\nInput: S (instance set), K (number of clusters), n (population size) \\n\\nOutput: clusters \\n\\n1. Randomly create a population of n structures; each \\n\\ncorresponds to valid K-clusters of the data. \\n\\n2. repeat \\n\\na. Associate a fitness value ∀ structure ∈ population. \\n\\nb. Regenerate a new generation of structures. \\n\\n3. until some termination condition is satisfied \\n\\n \\n\\n\\n\\nThe objective function for horizontal collaboration technique is explained in Eq. (13). For vertical \\n\\ncollaboration technique, please refer [73]: \\n2 2 2 2\\n\\n1 1 1 1 1\\n\\n[ ] [ ] [ ] [ , ] { [ ] [ ]} [ ]\\npN c N n\\n\\nij ij ij ij ij\\n\\ni j m i j\\nm l\\n\\nQ l u l d l l m u l u m d l\\uf062\\n\\uf03d \\uf03d \\uf03d \\uf03d \\uf03d\\n\\n\\uf0b9\\n\\n\\uf03d \\uf02b \\uf02d\\uf0e5\\uf0e5 \\uf0e5 \\uf0e5\\uf0e5\\n                          (18) \\n\\nwhere \\uf062  is a user defined parameter based on datasets ( \\uf062 >0), [ , ]l m\\uf062  denotes the collaborative \\n\\ncoefficient with collaborative effect on dataset l through m, c is a number of cluster. 1,2, ,l P\\uf03d . P  \\n\\nis a number of datasets, N  is the number of patterns in the dataset, u represents the partition matrix, n  \\nis a number of features, and d  is an Euclidean distance between patterns and prototypes. \\n\\n    The general scheme of collaborative clustering is shown in Fig. 11, which demonstrates the \\nconnections of matrices in order to accomplish the collaboration between the subsets of the dataset. \\nFirst, we solve the problem for each dataset separately and allow the results to interact globally by \\nforming a collaborative process between the datasets. Collaborative fuzzy partitioning is carried out \\nthrough an iterative optimization of the objective function as shown in Eq. (13). The optimization of \\nQ[l] involves the determination of the partition matrix U and the prototypes V of different data sets as \\nshown in Fig. 11(a) and (b). \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n(a) Collaborative clustering scheme for two datasets (b) Collaborative clustering scheme for three datasets \\n\\nFig. 11 Collaborative clustering scheme \\n\\n4.9 Multi Objective Clustering \\n\\nIn case of multi-objective clustering, many clustering approaches are optimized simultaneously. \\n\\nIn multi-objective clustering with automatic k-determination (MOCK) [78, 79], compactness of \\n\\nclusters is maximized as the first objective while the connectivity of the clusters is maximized as the \\n\\nsecond objective. The Pareto [80] approach is used to optimize the aforesaid two objectives \\n\\nsimultaneously. The multi objective clustering ensemble (MOCE) proposed by Faceili et.al [81] uses \\n\\nMOCK along with a special crossover operator which utilizes ensemble clustering. In Law et. al [82], \\n\\ndifferent clustering methods with different objectives are used. Some more surveys can be seen in \\n\\n[50]. \\n\\n4.10 Overlapping Clustering or Overlapping Community Detection \\n\\nThe partition clustering usually indicates exclusive and overlapping clustering algorithms (like k-\\n\\nmeans discussed above) such that each member or the object belongs to just one cluster. When an \\n\\nobject belongs to more than one cluster, it becomes overlapping clustering method or algorithm, e.g. \\n\\nfuzzy c-means clustering. Nowadays, community detection, as an effective way to reveal the \\n\\nrelationship between structure and function of networks, has drawn lots of attention and been well \\n\\ndeveloped [195]. Networks are modeled as graphs, where nodes represent objects and edges represent \\n\\ninteractions among them. Community detection divides a network into groups of nodes, where nodes \\n\\nare densely connected inside but sparsely connected outside. However, in real world, objects often \\n\\nhave diverse roles and belong to multiple communities. For example, a professor collaborates with \\n\\nresearchers in different fields and a person has his family group as well as friend group at the same \\n\\n\\n\\ntime.  In community detection, these objects should be divided into multiple groups, which are known \\n\\nas overlapping nodes [196]. The aim of overlapping community detection is to discover such \\n\\noverlapping nodes and communities. Until now, lots of overlapping community detection approaches \\n\\nhave been proposed, which can be roughly divided into two categories: node-based and link-based \\n\\nalgorithms. The node-based overlapping community detection algorithms [75, 76] directly divide \\n\\nnodes of the network into different communities. Based on an intuition that a link in networks usually \\n\\nrepresents the unique relation, the link-based algorithms firstly cluster on edges of network, and then \\n\\nmap the link communities to node communities by gathering nodes incident to all edges within each \\n\\nlink community [77]. The newly proposed link-based algorithms have shown its superiority on \\n\\ndetecting complex multi-scale communities. However, they have the high computational complexities \\n\\nand bias on the discovered communities. Shi et. al. [196] proposed a genetic algorithm, GaoCD, for \\n\\noverlapping community detection based on the link clustering framework. Different from those node-\\n\\nbased overlapping community detection algorithms, GaoCD utilized the property of the unique role of \\n\\nlinks and applies a novel genetic algorithm to cluster on edges. Experiments on artificial and real \\n\\nnetworks showed that GaoCD can effectively reveal overlapping structure. \\n\\n5 Evaluation Criteria \\n\\nThe formation of clusters is an important process. However, it is also meaningful to test the \\n\\nvalidity and accuracy of the clusters so formed by any method. It should be tested whether the clusters \\n\\nformed by a certain method show maximum similarity among the objects in the same cluster and \\n\\nminimum similarity among those in other clusters. Recently, many evaluation criteria have been \\n\\ndeveloped. These criteria are divided mainly into two categories: Internal and External. \\n\\n5.1 Internal Quality Criteria Measures \\n\\nInternal Criteria generally measure the compactness of the clusters by applying similarity measure \\n\\ntechniques. In general, it measures the inter-cluster separability and intra-cluster homogeneity, or a \\n\\ncombination of these two.  \\n\\n5.1.1 Sum of Squared Error \\n\\nSum of Square Error (SSE) [158, 159] is the most frequently used criterion measure for clustering. \\n\\nIt is defined as: \\n2\\n\\n1 i k\\n\\nK\\n\\ni k\\n\\nk x C\\n\\nSSE x \\uf06d\\n\\uf03d \\uf022 \\uf0ce\\n\\n\\uf03d \\uf02d\\uf0e5 \\uf0e5\\n                                                            (19) \\n\\nwhere Ck is the set of instances in cluster k; μk is the vector mean of cluster k.  \\n\\n5.1.2 Scatter Criteria \\n\\nThe scatter criteria matrix [1, 22] is defined as follows for the k-th cluster: \\n( )( )\\n\\nk\\n\\nT\\n\\nk k k\\n\\nx C\\n\\nS x x\\uf06d \\uf06d\\n\\uf0ce\\n\\n\\uf03d \\uf02d \\uf02d\\uf0e5\\n                                                          (20) \\n\\n5.1.3 Condorcet’s Criterion.  \\n\\nThe Condorcet’s criterion [110] is another approach to apply for the ranking problem [111]. The \\n\\ncriterion is defined as follows: \\n, ;\\n\\n( , ) ( , )\\ni j k i i j i k i\\n\\nj k\\n\\nj k j k\\n\\nC C x x C C C x C x C\\n\\nx x\\n\\ns x x d x x\\n\\uf0ce \\uf0ce \\uf0ce \\uf0ce \\uf0cf\\n\\n\\uf0b9\\n\\n\\uf02b\\uf0e5 \\uf0e5 \\uf0e5 \\uf0e5\\n\\n                                    (21) \\n\\nwhere s(xj , xk) and d(xj , xk) measure the similarity and distance of the vectors xj and xk. \\n\\n\\n\\n5.1.4 The C-criterion  \\n\\nFortier and Solomon [108] defined the C-criterion, which is an extension of Condorcet’s criterion \\n\\nand it is defined as: \\n, ;\\n\\n( ( , ) ) ( ( , ))\\ni j k i i j i k i\\n\\nj k\\n\\nj k j k\\n\\nC C x x C C C x C x C\\n\\nx x\\n\\ns x x s x x\\uf067 \\uf067\\n\\uf0ce \\uf0ce \\uf0ce \\uf0ce \\uf0cf\\n\\n\\uf0b9\\n\\n\\uf02d \\uf02b \\uf02d\\uf0e5 \\uf0e5 \\uf0e5 \\uf0e5\\n\\n                           (22) \\n\\nwhere γ is a threshold value. \\n\\n5.1.5 Category Utility Metric  \\n\\nThe category utility defined in [109, 112] which measures the goodness of category. A set of \\n\\nentities with size n binary feature set F= {fi}, i=1, …, n and a binary category { , }C c c\\uf03d is calculated \\n\\nas follows: \\n\\n1 1 1\\n\\n( , ) ( ) ( | ) log ( | ) ( ) ( | ) log ( | ) ( ) log ( )\\nn n n\\n\\ni i i i i i\\n\\ni i i\\n\\nCU C F p c p f c p f c p c p f c p f c p f p f\\n\\uf03d \\uf03d \\uf03d\\n\\n\\uf0e9 \\uf0f9\\n\\uf03d \\uf02b \\uf02d\\uf0ea \\uf0fa\\n\\uf0eb \\uf0fb\\n\\n\\uf0e5 \\uf0e5 \\uf0e5\\n   (23) \\n\\nwhere p(c) is the prior probability of an entity belonging to the positive category c, \\n( | )ip f c\\n\\nis the \\n\\nconditional probability of an entity having feature fi given that the entity belongs to category c, \\n( | )ip f c is likewise the conditional probability of an entity having feature fi given that the entity \\n\\nbelongs to category c , and p(fi) is the prior probability of an entity processing feature fi.  \\n\\n5.1.6 Edge Cut Metrics \\n\\nAn edge cut minimization problem [125, 126] is very useful in some cases for dealing with \\n\\nclustering problems. In this case, the cluster quality is measured as the ratio of the remaining edge \\n\\nweights to the total precut edge weights. Finding the optimal value is easy with edge cut minimization \\n\\nproblem, where there is no restriction on the size of the clusters. \\n\\n5.2 External Quality Criteria Measures \\n\\nIn order to match the structure of cluster to a predefined classification of the instances, the \\n\\nexternal quality criteria measure can be useful. \\n\\n5.2.1 Mutual Information Based Measure \\n\\nStrehl et al [113] proposed mutual information based measure, which can be used as an external \\n\\nmeasure for clustering. The criteria measure for m instances clustered using C = {C1,….,Cg} and \\n\\nreferring to the target attribute z whose domain is dom(z) = {c1,….,ck} is defined as follows: \\n,\\n\\n, .\\n\\n1 1 ., ,.\\n\\n2\\nlog\\n\\ng k\\nl h\\n\\nl h g k\\n\\nl h l l\\n\\nm m\\nC m\\n\\nm m m\\uf03d \\uf03d\\n\\n\\uf0e6 \\uf0f6\\uf0d7\\n\\uf03d \\uf0e7 \\uf0f7\\uf0e7 \\uf0f7\\uf0d7\\uf0e8 \\uf0f8\\n\\n\\uf0e5\\uf0e5\\n                                             (24) \\n\\nwhere ml,h indicates the number of instances that are in cluster Cl  and also in class ch. m.,h denotes the \\n\\ntotal number of instances in the class ch. Similarly, ml,.  Indicates the number of instances in cluster Cl. \\n\\n5.2.2 Rand Index \\n\\nThe Rand index [115] is a simple criterion used to compute how similar the clusters are to the \\n\\nbenchmark classifications. The Rand index is defined as: \\nTP TN\\n\\nRAND\\nTP FP FN TN\\n\\n\\uf02b\\n\\uf03d\\n\\n\\uf02b \\uf02b \\uf02b                                                      (25) \\n\\nwhere TP is the number of true positives, TN is the number of true negatives, FP is the number of \\n\\nfalse positives and FN is the number of false negatives. The Rand index lies between 0 and 1. When \\n\\nthe two partitions agree perfectly, the Rand index is 1.  \\n\\n\\n\\n5.2.3 F-measure \\n\\nIn Rand index, the false positives and false negatives are equally weighted and this may cause for \\n\\nan undesirable features for some clustering applications. The F-measure [116] addresses this concern \\n\\nand used to balance of false negatives by weighting recall parameter 0\\uf068 \\uf0b3 . The F-measure is defined \\n\\nas follows: 2\\n\\n2\\n\\n( 1) P R\\nF\\n\\nP R\\n\\n\\uf068\\n\\n\\uf068\\n\\n\\uf02b \\uf0d7 \\uf0d7\\n\\uf03d\\n\\n\\uf0d7 \\uf02b                                                                 (26) \\n\\nwhere P is the precision rate and R is the recall rate. Recall has no impact when 0\\uf068 \\uf03d and increasing η \\n\\nallocates an increasing amount of weight to recall in the final F-measure. Precision and Recall [119, \\n\\n120] is defined as follows: TP\\nP\\n\\nTP FP\\n\\uf03d\\n\\n\\uf02b                                                                       (27) \\nTP\\n\\nR\\nTP FN\\n\\n\\uf03d\\n\\uf02b                                                                         (28) \\n\\n5.2.4 Jaccard Index \\n\\nThe Jaccard index [121] is considered to identify the equivalency between two datasets. The \\n\\nJaccard index is defined as follows: \\n| |\\n\\n( , )\\n| |\\n\\nA B TP\\nJ A B\\n\\nA B TP FP FN\\n\\n\\uf0c7\\n\\uf03d \\uf03d\\n\\n\\uf0c8 \\uf02b \\uf02b                                              (29) \\n\\nIf A and B are both empty, then ( , ) 1J A B \\uf03d , i.e 0 ( , ) 1J A B\\uf0a3 \\uf0a3 . This is simply the number of unique \\n\\nelements common to both sets divided by the total number of unique elements in both sets. \\n\\n5.2.5 Fowlkes–Mallows Index \\n\\nThe Fowlkes-Mallows index [118] determines the similarity between the clusters obtained after \\n\\nthe clustering algorithm. The higher value of the Fowlkes-Mallows index indicates a more similarity \\n\\nbetween the clusters. It can be determined as follows: \\nTP TP\\n\\nFM\\nTP FP TP FN\\n\\n\\uf03d \\uf0d7\\n\\uf02b \\uf02b                                                    (30) \\n\\n5.2.6 Confusion Matrix \\n\\nA confusion matrix is also known as a contingency table or an error matrix [114]. It can be used \\n\\nto quickly visualize the results of a clustering. If a classification system has been trained to distinguish \\n\\nbetween apples, oranges and tomatoes, a confusion matrix will summarize the results of testing the \\n\\nalgorithm for further inspection. Assuming a sample of 27 fruits; 8 apples, 6 oranges, and 13 tomatoes, \\n\\nthe result of confusion matrix look like the table below: \\n\\nTable 3 Confusion Matrix \\n \\n\\nActual class \\n\\nPredicted class \\n\\nApple Orange Tomato \\n\\nApple 5 3 0 \\n\\nOrange 2 3 1 \\n\\nTomato 0 2 11 \\n\\nhttp://en.wikipedia.org/wiki/Fowlkes%E2%80%93Mallows_Index\\nhttp://en.wikipedia.org/wiki/Confusion_matrix\\n\\n\\nExternal indices are based on some pre-specified structure, which is the reflection of prior information \\n\\non the data, and used as a standard to validate the clustering solutions [50]. Internal tests are not \\n\\ndependent on external information (prior knowledge). On the contrary, they examine the clustering \\n\\nstructure directly from the original data. For more on evaluation, refer to [193,194]. \\n\\n6 Applications \\n\\nClustering is useful in several applications. Out of endless useful applications, a few applications \\n\\nare given below in diverse fields. \\n\\n6.1 Image Segmentation \\n\\nImage segmentation is an essential component of image processing. Image segmentation can be \\n\\nachieved using hierarchical clustering [37, 83]. K-means can also be applied for segmentation. \\n\\nMagnetic resonance imaging (MRI) provides a visualization of the internal structures of objects and \\n\\nliving organisms. MRI images have better contrast than computerized tomography; therefore, most \\n\\nmedical image segmentation research uses MRI images. Segmenting an MRI image is a key task in \\n\\nmany medical applications, such as surgical planning and abnormality detection. MRI segmentation \\n\\naims to partition an input image into significant anatomical areas, each of which is uniform according \\n\\nto certain image properties. MRI segmentation can be formulated as a clustering problem in which a \\n\\nset of feature vectors obtained through transformation image measurements and pixel positions is \\n\\ngrouped into a number of structures [28]. \\n\\n6.2 Bioinformatics—Gene Expression Data \\n\\nRecently, advances in genome sequencing projects and DNA microarray technologies have been \\n\\nachieved [50]. The first draft of the human genome sequence project was completed in 2001, several \\n\\nyears earlier than expected [84, 94]. The applications of clustering algorithms in bioinformatics can be \\n\\nseen from two aspects. The first aspect is based on the analysis of gene expression data generated from \\n\\nDNA microarray technologies. The second aspect describes clustering processes that directly work on \\n\\nlinear DNA or protein sequences. The assumption is that functionally similar genes or proteins usually \\n\\nshare similar patterns or primary sequence structures [50]. \\n\\n6.3 Object Recognition  \\n\\nThe use of clustering to group views of 3D objects for the purposes of object recognition in range \\n\\ndata was described in [85]. The system under consideration employed a view point dependent (or view \\n\\ncantered) approach to the object recognition problem; each object to be recognized was represented in \\n\\nterms of a library of range images of that object. \\n\\n6.4 Character Recognition \\n\\nClustering was employed in Jain [86] to identify lexemes in handwritten text for the purposes of \\n\\nwriter independent hand writing recognition. The success of a handwriting recognition system is \\n\\nvitally dependent on its acceptance by potential users. Writer dependent systems can give a higher \\n\\nlevel of recognition accuracy than that given by writer independent systems but the former require a \\n\\nlarge amount of training data. A writer independent system on the other hand must be able to \\n\\nrecognize a wide variety of writing styles in order to satisfy an individual user. \\n\\n6.5 Information Retrieval   \\n\\nInformation retrieval (IR) is concerned with automatic storage and retrieval of documents [87]. \\n\\nMany university libraries use IR systems to provide access to books, journals and other documents. \\n\\nLibraries use the library of congress classification (LCC) scheme for efficient storage and retrieval of \\n\\nbooks. The LCC scheme consists of classes labelled A to Z [88] which are used to characterize books \\n\\nbelonging to different subjects. For example, label Q corresponds to books in the area of science and \\n\\nthe subclass QA is assigned to mathematics. Labels QA76 to QA76.8 are used for classifying books \\n\\nrelated to computers and other areas of computer science. \\n\\n\\n\\n6.6 Data Mining  \\n\\nData mining [21] is the extraction of knowledge from large databases. It can be applied to \\n\\nrelational, transaction and spatial databases as well as large stores of unstructured data such as the \\n\\nWorld Wide Web. There are many data mining systems in use today and applications include the U.S. \\n\\nTreasury detecting money laundering. National basketball association coaches detecting trends and \\n\\npatterns of play for individual players and teams and categorizing patterns of children in the foster care \\n\\nsystem [89]. Several articles have had recent published in special issues on data mining [90].  \\n\\n6.7 Spatial Data Analysis \\n\\nClustering is useful to extract interesting features and identify the patterns, which exist in huge \\n\\namounts of spatial databases [106, 127-129]. It is expensive and very hard for user to deal with large \\n\\nspatial datasets like satellite images, medical equipment, geographical information systems (GIS), \\n\\nimage database exploration etc. Clustering process helps to understand spatial data by analyzing \\n\\nprocess automatically. \\n\\n6.8 Business \\n\\nThe role of clustering is quite interesting in business areas [135-139]. It helps marketer \\n\\nresearchers to do some analysis and prediction about customers in order to provide services based on \\n\\ntheir requirements and it also helps for market segmentation, new product development and product \\n\\npositioning. Clustering may be used to set all available shopping items on web into a group of unique \\n\\nproducts.  \\n\\n6.9 Data Reduction  \\n\\nData reduction or compression is one of the necessary tasks for handling very large data [132-134] \\n\\nand its processing becomes very demanding. Clustering can be applied to help in compressing data \\n\\ninformation by clustering them in different set of interesting clusters. After different set of clusters we \\n\\ncan choose the information or set of data which is useful for us. This process will save data processing \\n\\ntime along with doing data reduction.  \\n\\n6.10 Big Data Mining \\n\\nBig data [161-168] is also an emerging issue. The volume of data which is beyond the capacity of \\n\\nconventional data base management tools is processed under big data mining. Due to use of various \\n\\nsocial sites, travel, e-governance etc practices, mammoth amount of data is being heaped every \\n\\nmoment. Clustering of information (data) can help in aggregating similar information collected in \\n\\nunformatted databases (mainly text). Hadoop is one such big data processing tool [169-171]. It is \\n\\nexpected that big data processing will play an important role in detection of cyber crime, clustering \\n\\ngroups of people with similar behaviour on social network such as face book, WhatsApp etc. or \\n\\npredicting market behaviour based on various polls over these social sites. \\n\\n6.11 Other Applications \\n\\nSequence analysis [140], human genetic clustering [141], social network analysis [142], search \\n\\nresult grouping [143], software evolution [144, 145], recommender systems [146], educational data \\n\\nmining [147-149], Climatology [150], Field Robotics [151] etc. \\n\\n7 Choice of Appropriate Clustering Methods \\n\\nAs depicted in Fig.1, and from the wide amount of literature available with some referred in the \\n\\npaper, it becomes an obvious question: which method is uniformly good? It is to remember that \\n\\naccording to No Free Lunch concept given by Wolpert [197], no algorithm can be uniformly good \\n\\nunder all circumstances. In fact, each algorithm has its merit (strength) under some specific nature of \\n\\ndata but fails on other type of data. The selection of an appropriate clustering method may sometimes \\n\\nalso involve decision on certain parameters. Whether one wants only a proper alignment (or \\n\\nunsupervised grouping) of objects into a number of clusters (say user define k), then only choosing the \\n\\nvalue of k matters. This choice can be made on the ‘how fine tuning among the intra-cluster objects (or \\n\\npatterns) by virtue of distance is expected’. Selecting k can be heuristic or stochastic and evolutionary \\n\\n\\n\\ncomputing like genetic algorithms (GA) can be applied to find k. On the other hand, in case of data \\n\\nmining or data processing applications with dimensionality reduction, mostly it is required to reduce \\n\\nthe number of attributes or features in the existing dataset in order to extract rules with better \\n\\nprediction capability. In many of these occasions, it is expected that while reducing the dimensionality \\n\\nof the dataset, whether the structure or the internal topology of the dataset is not disturbed in the \\n\\nreduced data space. Saxena et. al [23] proposed four unsupervised methods for feature selection using \\n\\ngenetic algorithms. \\n\\nIn [27], Fraley presents a comprehensive discussion on how to decide a clustering method and \\n\\ndescribed a clustering methodology based on multivariate normal mixture models and shown that it \\n\\ncan give much better performance than existing methods. This approach has some limitations, \\n\\nhowever. The first limitation is that computational methods for hierarchical clustering have storage \\n\\nand time requirements that grow at a faster than linear rate relative to the size of the initial partition, so \\n\\nthat they cannot be directly applied to large data sets.  Secondly, although experience to date suggests \\n\\nthat models based on multivariate normal distribution are sufficiently flexible to accommodate many \\n\\npractical situations, the underlying assumption is that groups are concentrated locally about linear \\n\\nsubspaces, so that other models or methods may be more suitable in some instances. Bensmail et al. \\n\\n[198] showed that exact Bayesian inference via Gibbs sampling, with calculations of Bayes factors \\n\\nusing the Laplace–Metropolis estimator, works well in several real and simulated examples [27].   \\n\\nFurther, for large data sets, CURE method is advisable whereas BIRCH being also good but with \\n\\nless time complexity although quality of clustering is inferior to that obtained by CURE, refer to Table \\n\\n1. Under partitioned clustering method, k-means clustering dominates and is still the most popular \\n\\nclustering method, refer to Table 2. How many clusters i.e. k depends on how close or fine tuning we \\n\\nwant among clusters. We should also keep in mind, for what purpose we are applying k-means. In \\n\\nvarious clustering methods presented in the paper already, the strengths and weaknesses of each are \\n\\nmostly given therein. Apart from the discussion above on selection of appropriate method for \\n\\nclustering, it is worth noting looking to a huge amount of literature available with wide variety of \\n\\napplication of clustering; it is not possible to settle to an agreeable recommendation. Specific task \\n\\n(objectives) calls for specific strategy and should be tested experimentally. Finally, a part of \\n\\ncomprehensive and comparative table for various clustering algorithms presented before is given in \\n\\nTable 4, for details and meaning of symbols refer to [199]. \\n\\nTable 4. Comparative study of some clustering algorithms [199] \\n\\nCategory of \\n\\nClustering \\n\\nAlgorithm \\n\\nName \\n\\nTime complexity Scalability Suitable for \\n\\nlarge scale data \\n\\nSuitable for \\n\\nhigh \\n\\ndimensional \\n\\ndata \\n\\nSensitive \\n\\nof noise/ \\n\\noutlier \\n\\nPartition k-means Low O(knt) Middle Yes No High \\n\\nPAM High O(k(n-k)ˆ2)) Low No No little \\n\\nCLARA Middle O(ksˆ 2+k(n-k)) High Yes No Little \\n\\nCLARANS High O(nˆ2) Middle Yes No Little \\n\\nHierarchy BIRCH Low O(n) High Yes No Little \\n\\nCURE Low O(s ˆ2*logs) High Yes Yes Little \\n\\nROCK High O(nˆ2*logn) Middle No Yes Little \\n\\nChameleon High O(nˆ2) High No No Little \\n\\nFuzzy based FCM Low O(n) Middle No No High \\n\\nDensity based DBSCAN Middle O(n*logn) Middle Yes No Little \\n\\nGraph theory CLICK Low O(k*f(v,e)) High Yes No High \\n\\nGrid based CLIQUE Low O(n+kˆ2) High No Yes Moderate \\n\\n \\n\\n\\n\\n8 Conclusions \\n\\nThe classification of objects finds prime importance in several data processing applications \\n\\nincluding data mining, medical diagnostics, pattern recognition and social paradigms. The objects \\n\\nalready labeled are placed in supervised classified groups while those not labeled are grouped in \\n\\nunsupervised classified groups. This paper presented various methods used for clusters with their \\n\\nstates of arts and limitations. In the hierarchical type of clustering methods, clusters are formed by \\n\\niteratively dividing the patterns (instances) into top-down or bottom up manner accordingly \\n\\nagglomerative and divisive or splitting hierarchical clustering methods are discussed. As opposed to \\n\\nhierarchical clustering, partitional clustering assigns data into K clusters without any hierarchical \\n\\nstructure by optimizing some criterion function. The most common criterion is finding Euclidean \\n\\ndistance between the points with each of the available clusters and assigning the point to the cluster \\n\\nwith minimum distance. The benchmark k-means clustering methods with its variations like Fuzzy K-\\n\\nmeans are discussed. The graph theoretic methods produce clusters via graphs. In the mixture density \\n\\nbased methods, data objects are assumed to be generated according to several probability distributions \\n\\nand can be derived from different types of density functions (e.g., multivariate Gaussian or t-\\n\\ndistribution), or from the same families but with different parameters. The grid based clustering \\n\\ntechniques include: STING (statistical information grid approach) a highly scalable algorithm and has \\n\\nthe ability to decompose the data set into various levels of details. The evolutionary approaches for \\n\\nclustering start with a random population of candidate solutions with some fitness function, which \\n\\nwould be optimized. Clustering based on simulated annealing, collaborative clustering, multi objective \\n\\nclustering with their states of art are also included. Various types of the similarity criteria for \\n\\nclustering have been given in the paper. After the clusters have been formed, the evaluation criteria are \\n\\nalso summarised to see the performance and accuracy of clusters. The applications of clustering in \\n\\nimage segmentation, object and character recognition, information retrieval and data mining are \\n\\nhighlighted in the paper. Of course there is an abundant amount of literature available in clustering and \\n\\nits applications; it is not possible to cover that entirely, only basic and few important methods are \\n\\nincluded in this paper with their merits and demerits. \\n\\nAcknowledgement \\n\\nThe authors would like to thank the anonymous reviewers for their valuable suggestions and \\n\\ncomments to improve the quality of the paper. This work is partially supported by the Australian \\n\\nResearch Council (ARC) under discovery grant DP150101645. \\n\\nReferences \\n\\n1. R. O. Duda, P. E. Hart, and D. G. Stork, “Pattern Classification,” Wiley Publications, 2001. \\n\\n2. Y. Zhang, Y. Yin, D. Guo, X. Yu, and L. Xiao, “Cross-validation based weights and structure determination of \\n\\nChebyshev-polynomial neural networks for pattern classification,” Pattern Recognition, vol. 47, no. 10, pp. 3414-\\n\\n3428, 2014. \\n\\n3. H. Nakayama, N. Kagaku, “Pattern classification by linear goal programming and its extensions,” Journal of \\n\\nGlobal Optimization, vol. 12, no. 2, pp. 111–126, 1998. \\n\\n4. C. M. Bishop, “Pattern recognition and machine learning,” Berlin: Springer, ISBN 978-0-387-31073-2. \\n\\n5. G.P. Zhang, “Neural networks for classification: a survey,” IEEE Transaction on Systems, Man, and Cybernetics, \\n\\nPart C: Applications and Reviews, vol. 30, no. 4, pp. 451–462, 2002. \\n\\n6. H. Zhang, J. Liu, D. Ma, and Z. Wang, “Data-core-based fuzzy min–max neural network for pattern classification,” \\n\\nIEEE Transaction on Neural Networks, vol. 22, no. 12, pp. 2339–2352, 2011. \\n\\n7. X. Jiang and A. H. K. S. Wah, “Constructing and training feed-forward neural net- works for pattern \\n\\nclassification,” Pattern Recognition, vol. 36, no. 4, pp. 853–867, 2003. \\n\\n8. G. Ou and Y. L. Murphey, “Multi-class pattern classification using neural networks,” Pattern Recognition, vol. 40, \\n\\nno. 1, pp. 4–18. 2007. \\n\\n9. J. D. Paola and R. A. Schowengerdt, “A detailed comparison of back propagation neural network and maximum-\\n\\nlikelihood classifiers for urban land use classification,” IEEE Transaction on Geoscience and Remote Sensing, vol. \\n\\n33, no. 4, pp. 981–996, 1995. \\n\\n10. D. E. Rumelhart and J. L. McClelland, “Parallel Distributed Processing,” MIT Press, Cambridge, 1986. \\n\\n11. W. Zhou, “Verification of the nonparametric characteristics of back-propagation neural networks for image \\n\\nclassification,” IEEE Transaction on Geoscience and Remote Sensing, vol. 37, no. 2, pp. 771–779, 1999. \\n\\n12. G. Jaeger, U. C. Benz, “Supervised fuzzy classification of SAR data using multiple sources,” IEEE International \\n\\nGeoscience and Remote Sensing Symposium, 1999. \\n\\nhttp://en.wikipedia.org/wiki/International_Standard_Book_Number\\nhttp://en.wikipedia.org/wiki/Special:BookSources/0-387-31073-8\\nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=772033&queryText%3Dsupervised+fuzzy+classification\\nhttp://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6246\\n\\n\\n13. F. S. Marzano, D. Scaranari, and G. Vulpiani, “Supervised Fuzzy-Logic Classification of Hydrometeors Using C-\\n\\nBand Weather Radars,” IEEE Transaction on Geoscience and Remote Sensing, vol. 45 , no. 11, pp. 3784-3799, \\n\\n2007. \\n14. B. Xue, M. Zhang, and W. N. Browne, “Particle Swarm Optimization for Feature Selection in Classification: A \\n\\nMulti-Objective Approach,” IEEE Transaction on Cybernetics, vol. 43, no. 6, pp. 1656-1671, 2013. \\n\\n15. A. Saxena and M. Vora, “Novel Approach for the use of Small World Theory in Particle Swarm Optimization,” \\n\\n16th International Conference on Advanced Computing and Communications, 2008. \\n\\n16. Z. Pawlak, “Rough sets”, International Journal of Computer and Information Science, vol. 11, no. 5, pp. 341-356. \\n\\n1982. \\n\\n17. Z. Pawlak, “Rough sets In Theoretical Aspects of Reasoning about Data,” Kluwer, Netherlands, 1991. \\n\\n18. S. Dalai, B. Chatterjee, D. Dey, S. Chakravorti, and K. Bhattacharya, “Rough-Set-Based Feature Selection and \\n\\nClassification for Power Quality Sensing Device Employing Correlation Techniques,” IEEE Sensors Journal, vol. \\n\\n13, no. 2, pp. 563–573, 2013 \\n\\n19. J. R. Quinlan, “Induction of decision trees,” Machine Learning, vol. 1, no. 1, pp. 81-106, 1986. \\n\\n20. D. M. Farida, L Zhang, C. M. Rahman, M. A. Hossain, and R. Strachan, “Hybrid decision tree and naïve Bayes \\n\\nclassifiers for multi-class classification tasks,” Expert Systems with Applications, vol. 41, no. 2, pp. 1937–1946, \\n\\n2014. \\n\\n21. J. Han, M. Kamber, and J. Pei, “Data Mining: Concepts and Techniques,” Morgan Kaufmann Publishers, 2011. \\n\\n22. L. Rokach, “Clustering Methods,” Data Mining and Knowledge Discovery Handbook, pp 331-352, Springer 2005. \\n\\n23. A. Saxena, N. R. Pal, and M. Vora, “Evolutionary methods for unsupervised feature selection using Sammon’s \\n\\nstress function, Fuzzy Information and Engineering,” vol. 2, no. 3, pp. 229-247, 2010. \\n\\n24. A. K. Jain, “Data Clustering: 50 years beyond k-means,” Pattern Recognition Letters, vol. 31, no. 8, pp. 651–666, \\n\\n2010. \\n\\n25. Merriam-Webster Online Dictionary, 2008 \\n\\n26. V. E. Castro and J. Yang, “A Fast and robust general purpose clustering algorithm,” International Conference on \\n\\nArtificial Intelligence, 2000. \\n\\n27. C. Fraley and A. E. Raftery, “How Many Clusters? Which Clustering Method? Answers Via Model-Based Cluster \\n\\nAnalysis”, Technical Report No. 329, Department of Statistics University of Washington, 1998. \\n\\n28. A. K. Jain, M. N. Murty, and P. J. Flynn, “Data Clustering: A review. ACM Computing Surveys, vol. 31, no. 3, pp. \\n\\n264-323, 1999. \\n\\n29. P. Sneath and R. Sokal, “Numerical Taxonomy,” W.H. Freeman Co, San Francisco, CA, 1973. \\n\\n30. B. King, “Step-wise Clustering Procedures,” Journal of American Statistical Association , vol. 69, no. 317, pp. 86-\\n\\n101, 1967. \\n\\n31. J. H. Ward, “Hierarchical grouping to optimize an objective function,” Journal of the American Statistical \\n\\nAssociation, vol. 58, no. 301, pp. 236-244, 1963. \\n\\n32. F. Murtagh, “A survey of recent advances in hierarchical clustering algorithms which use cluster centers,” \\n\\nComputer Journal, vol. 26, no. 4, pp. 354-359, 1984. \\n\\n33. A. Nagpal, A. Jatain, and D. Gaur, “Review based on Data Clustering Algorithms,” IEEE Conference on \\n\\nInformation and Communication Technologies, 2013. \\n\\n34. A. Periklis, “Data Clustering Techniques,” University of Toronto, 2002. \\n\\n35. S. Guha, R. Rastogi, and S. Kyuseok, “CURE: An efficient clustering algorithm for large databases,” ACM, 1998. \\n\\n36. K. George, E. H. Han, and V. Kumar, “CHAMELEON: A Hierarchical Clustering Algorithm Using Dynamic \\n\\nModeling,” IEEE Computer, vol. 32, no. 8, pp. 68-75, 1999. \\n\\n37. D. Lam and D. C. Wunsch, “Clustering,” Academic Press Library in Signal Processing,” Signal Processing Theory \\n\\nand Machine Learning, vol. 1, 2014 \\n\\n38. J. B. MacQueen, “Some Methods for classification and Analysis of Multivariate Observations,” 5th Symposium on \\n\\nMathematical Statistics and Probability, Berkeley, University of California Press, vol. 1, pp. 281-297, 1967. \\n\\n39. A. Gersho and R. Gray, “Vector Quantization and Signal Compression,” Kluwer Academic Publishers, 1992. \\n\\n40. J. C. Dunn, “A Fuzzy Relative of the ISODATA Process and Its Use in Detecting Compact Well-Separated \\n\\nClusters,” Journal of Cybernetics, vol. 3, no. 3, pp. 32-57, 1973. \\n\\n41. J. C. Bezdek, “Pattern Recognition with Fuzzy Objective Function Algorithms,” Plenum Press, New York, 1981. \\n\\n42. R. Yager and D. Filev, “Approximate clustering via the mountain method,” IEEE Transaction on Systems, Man \\n\\nand Cybernetics, Part B: Cybernetics, vol. 24, no. 8, pp. 1279–1284. 1994 \\n\\n43. I. Gath and A. Geva, “Unsupervised optimal fuzzy clustering,” IEEE Transaction on Pattern Analysis  and \\n\\nMachine Intelligence, vol. 11, no. 7, pp. 773–781. 1989. \\n\\n44.  R. Hathaway, J. Bezdek, and Y. Hu, “Generalized fuzzy c-Means clustering strategies using Lp norm distances,” \\n\\nIEEE Transaction on Fuzzy Systems, vol. 8, no. 5, pp. 576–582. 2000. \\n\\n45.  R. Krishnapuram and J. Keller, “A possibilistic approach to clustering,” IEEE Transaction on Fuzzy Systems, vol. \\n\\n1, no. 2, pp. 98–110, 1993. \\n\\n46. C. T. Zahn, “Graph-theoretical methods for detecting and describing gestalt clusters,” IEEE Transaction on \\n\\nComputer, vol. C-20, no. 1, pp. 68-86, 1971. \\n\\n47. R. Urquhart, “Graph-theoretical clustering based on limited neighborhood sets,” Pattern Recognition, vol. 15, no. \\n\\n3, pp. 173-187, 1982. \\n\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Marzano,%20F.S..QT.&searchWithin=p_Author_Ids:37269625100&newsearch=true\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Scaranari,%20D..QT.&searchWithin=p_Author_Ids:37660501800&newsearch=true\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Vulpiani,%20G..QT.&searchWithin=p_Author_Ids:37550431200&newsearch=true\\nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4373376&queryText%3Dsupervised+fuzzy+classification\\nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4373376&queryText%3Dsupervised+fuzzy+classification\\nhttp://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Bing%20Xue.QT.&newsearch=true\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Mengjie%20Zhang.QT.&newsearch=true\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Browne,%20W.N..QT.&newsearch=true\\nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6381531&searchWithin%3Dpso+based+classification%26searchWithin%3Dpso+based+classification%26refinements%3D4291944246%26queryText%3Dpso+based+classifications\\nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6381531&searchWithin%3Dpso+based+classification%26searchWithin%3Dpso+based+classification%26refinements%3D4291944246%26queryText%3Dpso+based+classifications\\nhttp://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221036\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Dalai,%20S..QT.&newsearch=true\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Chatterjee,%20B..QT.&newsearch=true\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Dey,%20D..QT.&newsearch=true\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Chakravorti,%20S..QT.&newsearch=true\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Bhattacharya,%20K..QT.&newsearch=true\\nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6304906&refinements%3D4291944246%26queryText%3Drough+set+based+classifications\\nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6304906&refinements%3D4291944246%26queryText%3Drough+set+based+classifications\\nhttp://link.springer.com/journal/12543\\n\\n\\n48. D. H. Fisher, “Knowledge acquisition via incremental conceptual clustering,” Machine Learning 2, pp. 139-172. \\n\\n1987. \\n\\n49. S. Haykin, “Neural Networks: A Comprehensive Foundation,” 2nd Edition, Prentice Hall, 1999. \\n\\n50. R. Xu and D. Wunsch, “Survey of clustering algorithms,” IEEE Transaction on Neural Networks, vol. 16, no. 3, \\n\\n645–678, 2005. \\n\\n51. R. Xu. D.C. Wunsch, “Clustering algorithms in biomedical research: a review,” IEEE Reviews in Biomedical \\n\\nEngineering, vol. 3, pp. 120–154. 2010. \\n\\n52. G. McLachlan, T. Krishnan, “The EM Algorithm and Extensions,” Wiley, New York, 1997. \\n\\n53. J. D. Banfield and A. E. Raftery, “Model-based Gaussian and non-Gaussian clustering Biometrics,” vol. 49, no. 3, \\n\\npp. 803-821, 1993. \\n\\n54. M. Ester, H. P. Kriegel, S. Sander S, and X. Xu, “A density-based algorithm for discovering clusters in large \\n\\nspatial databases with noise,” 2nd International Conference on Knowledge Discovery and Data Mining, 1996. \\n\\n55. P. Cheeseman, J. Stutz, “Bayesian Classification (AutoClass): Theory and Results,” Advances in Knowledge \\n\\nDiscovery and Data Mining, pp. 153-180, 1996. \\n\\n56. C. S. Wallace and D. L. Dowe, “Intrinsic classification by mml–the snob program,” 7th Australian Joint Conference \\n\\non Artificial Intelligence, pp. 37-44, 1994. \\n\\n57. W. Wang, J. Yang, and R. R. Muntz, “STING: A Statistical Information Grid Approach to Spatial Data Mining,” \\n\\n23rd VLDB Conference, pp. 86-195, 1997. \\n\\n58. G. Sheikholeslami, S. Chatterjee and A. Zhang, “WaveCluster: a wavelet-based clustering approach for spatial data \\n\\nin very large databases,” The International Journal on Very Large Data Bases, vol. 8, no. 3-4, pp. 289-304, 2000. \\n\\n59. R. Agrawal, G. Johannes, G. Dimitrios, and P. Raghavan, “Automatic Subspace Clustering of High Dimensional \\n\\nData for Data Mining Applications,” SIGMOD Conference, pp. 94-105, 1998. \\n\\n60. A. K. Jain and M. Flynn, “Data clustering: a review,” ACM Computing Surveys (CSUR), vol. 31, no. 3, pp. 264-\\n\\n323, 1999. \\n\\n61. H. P. Schwefel, “Numerical Optimization of Computer Models,” John Wiley, New York, 1981. \\n\\n62. L. J. Fogel, A. J. Owens, and M J Walsh, “Artificial Intelligence Through Simulated Evolution,” John Wiley , New \\n\\nYork, 1965. \\n\\n63. J. H. Holland, “Adaption in Natural and Artificial Systems,” University of Michigan Press, 1975. \\n\\n64. D. Goldberg, “Genetic Algorithms in Search Optimization and Machine Learning,” Addison Wesley Reading, \\n\\n1989. \\n\\n65. J. Kennedy and R. C. Eberhart, “Swarm Intelligence,” Morgan Kaufmann, 2001. \\n\\n66. J. Kennedy and R. Eberhart, “Particle Swarm Optimization,” 4th IEEE International Conference on Neural \\n\\nNetworks. pp. 1942–1948, 1995. \\n\\n67. M. Dorigoand T. Stützle, “Ant Colony Optimization,” MIT Press, 2004. \\n\\n68. F. Glover, “Future Paths for Integer Programming and Links to Artificial Intelligence,” Computers and Operations \\n\\nResearch, vol. 5, no. 5, pp. 533-549, 1986. \\n\\n69. K. S. Al. Sultan, “A Tabu Search Approach to Clustering Problem,” Pattern Recognition, vol. 28, no. 9, pp. 1443-\\n\\n1451, 1995. \\n\\n70. W. Pedrycz, “Collaborative fuzzy clustering,” Pattern Recognition Letters, vol. 23, no. 14, pp. 1675–1686, 2002. \\n\\n71. L. F. S. Coletta, L. Vendramin, E. R. Hruschka, R. J. G. B. Campello, and W. Pedrycz, “Collaborative Fuzzy \\n\\nClustering Algorithms: Some Refinements and Design Guidelines,” IEEE Transactions on Fuzzy Systems, vol. 20, \\n\\nno. 3, pp. 444-462, 2012. \\n\\n72. W. Pedrycz and P. Rai, “Collaborative clustering with the use of Fuzzy C-Means and its quantification,” Fuzzy \\n\\nSets and Systems, vol. 159, no. 18, pp. 2399–2427, 2008. \\n\\n73. W. Pedrycz, “Knowledge Based Clustering: From data to information granules,” Wiley Publications, 2005. \\n\\n74. M. Prasad, C. T. Lin, C. T. Yang, and A. Saxena, “Vertical Collaborative Fuzzy C-Means for Multiple EEG Data \\n\\nSets,” Springer Intelligent Robotics and Applications Lecture Notes in Computer Science, vol. 8102, pp 246-257, \\n\\n2013. \\n\\n75. C. Pizzuti, “Overlapping Community Detection in Complex Networks,” GECCO, pp. 859–866, 2009. \\n\\n76. S. Gregory, “A Fast Algorithm to Find Overlapping Communities in Networks,” PKDD, pp. 408–423, 2008. \\n\\n77. Y. Y. Ahn, J. P. Bagrow, and S. Lehmann, “Link communities reveal multi-scale complexity in networks,” Nature, \\n\\nvol. 466, pp. 761–764, 2010. \\n\\n78. G Forestier, P Gancarski, and C Wemmert, “Collaborative Clustering with back ground knowledge,” Data and \\n\\nKnowledge Engineering, vol. 69, no. 2, pp. 211–228, 2010. \\n\\n79. J. Handl and J. Knowles, “An evolutionary approach to Multiobjective clustering,” IEEE Transaction on \\n\\nEvolutionary Computation, vol.11, no. 1, pp. 56-76, 2007. \\n\\n80. A. Konak, D. Coit, and A. Smith, “Multiobjective optimization using genetic algorithms: A tutorial,” Reliability \\n\\nEngineering and System Safety, vol. 91, no. 9, pp. 992-1007, 2006. \\n\\n81. K. Faceili, A. D. Carvalho, and D. Souto, “Multiobjective Clustering ensemble,” International Conference, on \\n\\nHybrid Intelligent Systems, 2006. \\n\\n82. M. K. Law, A. Topchy, and A. K. Jain, “Multiobjective Data Clustering,” IEEE Conference on Computer Vision \\n\\nand Pattern Recognition, vol. 2, pp. 424-430, 2004. \\n\\n83. D. Forsyth and J. Ponce, “Computer vision: a modern approach,” Prentice Hall, 2002. \\n\\nhttp://www.informatik.uni-trier.de/~ley/pers/hd/g/Gehrke:Johannes.html\\nhttp://www.informatik.uni-trier.de/~ley/pers/hd/g/Gunopulos:Dimitrios.html\\nhttp://www.informatik.uni-trier.de/~ley/pers/hd/r/Raghavan:Prabhakar.html\\nhttp://www.informatik.uni-trier.de/~ley/db/conf/sigmod/sigmod98.html#AgrawalGGR98\\nhttp://www.google.co.in/search?tbo=p&tbm=bks&q=inauthor:%22Marco+Dorigo%22\\nhttp://www.google.co.in/search?tbo=p&tbm=bks&q=inauthor:%22Thomas+St%C3%BCtzle%22\\nhttp://link.springer.com/search?facet-author=%22Mukesh+Prasad%22\\nhttp://link.springer.com/search?facet-author=%22Chin-Teng+Lin%22\\nhttp://link.springer.com/search?facet-author=%22Chien-Ting+Yang%22\\nhttp://link.springer.com/search?facet-author=%22Amit+Saxena%22\\nhttp://link.springer.com/book/10.1007/978-3-642-40852-6\\nhttp://link.springer.com/bookseries/558\\n\\n\\n84. I. H. G. S. Consortium, “Initial sequencing and analysis of the human genome,” Nature, vol. 409, pp. 860–921, \\n\\n2001.  \\n\\n85. C. Dorai and A. K. Jain, “Shape Spectra Based View Grouping for Free Form Object,” International Conference on \\n\\nImage Processing, vol. 3, pp. 240-243, 1995. \\n\\n86. S. Connell and A. K. Jain, “Learning Prototypes for On-Line Handwritten Digits,” 14th International Conference on \\n\\nPattern Recognition, vol. 1, pp. 182-184, 1998. \\n\\n87. E. Rasmussen, “Clustering Algorithms,” Information Retrieval: Data Structures and Algorithms, Prentice Hall \\n\\nEnglewood Cliffs, pp 419-442, 1992. \\n\\n88. G. McKiernan, “LC Classification Outline,” Library of Congress Washington, D. C, 1990. \\n\\n89. S. R. Hedberg, “Searching for the mother lode: Tales of the first data miners,” IEEE Expert: Intelligent Systems an \\n\\nTheir Applications, vol. 11, no. 5, pp. 4-7, 1996. \\n\\n90. J. Cohen, “Communications of the ACM: Data Mining Association for Computing Machinery,” Nov. 1996.  \\n\\n91. A. Saxena, J. Wang, “Dimensionality Reduction with Unsupervised Feature Selection and Applying Non-\\n\\nEuclidean Norms for Classification Accuracy,” International Journal of Data Warehousing and Mining, vol. 6, no. \\n\\n2, pp 22-40, 2010. \\n\\n92. K. S. Al. Sultan and M. M. Khan, “Computational experience on four algorithms for the hard clustering problem,” \\n\\nPattern Recognition Letters, vol. 17, no. 3, pp. 295-308, 1996. \\n\\n93. R. Michalski, R. E. Stepp, and E. Diday, “Automated construction of classifications: conceptual clustering versus \\n\\nnumerical taxonomy,” IEEE Transaction on Pattern Analysis and Machine Intelligence, vol. 5, no. 4, pp. 396–409, \\n\\n1983. \\n\\n94. J. C. Venter et. al.,“The sequence of the human genome,”Science,vol. 291, pp. 1304–1351, 2001. \\n\\n95. J. L. Kolodner, “Reconstructive memory: A computer model,” Cognitive Science, vol. 7, no. 4, pp. 281-328, 1983. \\n\\n96. C. Carpineto and G. Romano, “An order-theoretic approach to conceptual clustering,” 10th International \\n\\nConference on Machine Learning, pp. 33–40, 1993. \\n\\n97. L. Talavera and J. Bejar. “Generality-Based Conceptual Clustering with Probabilistic Concepts,” IEEE \\n\\nTransactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 2, pp. 196-206, 2001. \\n\\n98. M. Hadzikadic and D. Yun, “Concept formation by incremental conceptual clustering,” 11th International Joint \\n\\nConference Artificial Intelligence, pp. 831-836, 1989. \\n\\n99. G. Biswas, J. B. Weinberg, and D. H. Fisher, “Iterate: A conceptual clustering algorithm for data mining,” IEEE \\n\\nTransactions on Systems, Man, and Cybernetics, Part C, vol. 28, no. 2, pp. 219–230, 1998. \\n\\n100. K. Thompson and P. Langley, “Concept formation in structured domains,” Concept Formation: Knowledge and   \\n\\nExperience in Unsupervised Learning, Morgan Kaufmann, 1991. \\n\\n101. I. Jonyer, D. Cook, and L. Holder, “Graph-based hierarchical conceptual clustering,” Journal of Machine Learning \\n\\nResearch, vol. 2, pp. 19-43, 2001. \\n\\n102. M. Lebowitz, “Experiments with Incremental Concept Formation: UNIMEM,” Machine Learning, vol. 2, no. 2, \\n\\npp. 103-138, 1987. \\n\\n103. S. Hanson and M. Bauer, “Conceptual clustering, categorization and polymorphy,” Machine Learning Journal, \\n\\nvol. 3, no. 4, pp. 343-372, 1989. \\n\\n104. T. Kohonen, “The self-organizing map,” Neurocomputing, vol. 21, no. 1–3, Pages 1–6, 1998. \\n\\n105. J. Vesanto and E. Alhoniemi, “Clustering of the Self-Organizing Map,” IEEE Transactions on Neural Networks, \\n\\nvol. 11, no. 3, 2000. \\n\\n106. J. G. Upton and B. Fingelton, “Spatial Data Analysis by Example,” Point Pattern and Quantitative Data, John \\n\\nWiley & Sons, New York, vol. 1, 1985. \\n\\n107. A. Strehl, J. Ghosh, and R. Mooney, “Impact of similarity measures on web-page clustering,” Workshop on \\n\\nArtificial Intelligence for Web Search, pp 58–64, 2000. \\n\\n108. J. J. Fortier, and H. Solomon, “Clustering procedures,” The Multivariate Analysis, pp. 493-506, 1996. \\n\\n109. M. A. Gluck and  J. E. Corter,(1985), “Information, uncertainty, and the utility of categories,” Program of the 7th \\n\\nAnnual Conference of the Cognitive Science Society, pp. 283–287, 1985. \\n\\n110.  M. J. A. N. Condorcet, “Essai sur l’Application de l’Analyse `a la Probabilite´ des decisions rendues a la \\n\\nPluralite´ des Voix,” paris: L’Imprimerie Royale, 1785. \\n\\n111. J. F. Marcotorchino and P. Michaud, “Optimisation en Analyse Ordinale des Donnees Masson, Paris, 1979. \\n\\n112. J. E. Corter and M. A. Gluck, “Explaining basic categories: Feature predictability and information,” Psychological \\n\\nBulletin, vol. 111, no. 2, pp. 291–303, 1992. \\n\\n113. A. Strehl and J. Ghosh, “Clustering Guidance and Quality Evaluation Using Relationship-based Visualization,” \\n\\nIntelligent Engineering Systems through Artificial Neural Networks, St. Louis, Missouri, USA, pp 483-488, 2000. \\n\\n114. S. V. Stehman, “Selecting and interpreting measures of thematic classification accuracy” Remote Sensing of \\n\\nEnvironment, vol. 62, no. 1, pp. 77–89, 1997. \\n\\n115. W. M. Rand, “Objective criteria for the evaluation of clustering methods,” Journal of the American Statistical \\n\\nAssociation, vol. 66, no. 336, pp. 846– 850, 1971. \\n\\n116. V. Rijsbergen, “Information retrieval,” Butterworths, London, 1979. \\n\\n117. J. F. Brendan and D. Dueck, “Clustering by passing messages between data points,”.Science, vol. 315, pp. 972–\\n\\n976, 2007. \\n\\n118. E. B. Fowlkes and C. L. Mallows (1983), “A Method for Comparing Two Hierarchical Clusterings,” Journal of \\n\\nthe American Statistical Association, vol. 78, no. 383, pp. 553–569, 2010. \\n\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Kohonen,%20T..QT.&searchWithin=p_Author_Ids:37318173400&newsearch=true\\nhttp://www.sciencedirect.com/science/journal/09252312\\nhttp://www.sciencedirect.com/science/journal/09252312/21/1\\nhttp://en.wikipedia.org/wiki/Journal_of_the_American_Statistical_Association\\nhttp://en.wikipedia.org/wiki/Journal_of_the_American_Statistical_Association\\nhttp://www.sciencedirect.com/science/article/pii/S0306457301000486#BIB33\\nhttp://en.wikipedia.org/wiki/Science_(journal)\\n\\n\\n119. D. L. Olson and D. Delen, “Advanced Data Mining Techniques,” Springer, 1st edition, 2008. \\n\\n120. D. M. W. Powers, “Evaluation: From Precision, Recall and F-Factor to ROC, Informedness, Markedness & \\n\\nCorrelation,” Journal of Machine Learning Technologies, vol. 2, no. 1, pp. 37–63, 2007. \\n\\n121. P. Jaccard, “Distribution de la flore alpine dans le bassin des Dranses et dans quelques régions voisines,” Bulletin \\n\\nde la Société Vaudoise des Sciences Naturelles, vol. 37, pp. 241-272, 1901. \\n\\n122. J. Han, M. Kamber, and J. Pei, “Data mining: Concepts and techniques,” Morgan Kaufman, San Francisco, USA, \\n\\n2011. \\n\\n123. J. J. Grefenstette, “Optimization of Control Parameters for Genetic Algorithms,” IEEE Transaction on Systems, \\n\\nMan and Cybernetics, vol. 16, no. 1, pp. 122–128, 1986. \\n\\n124. C. T. Lin, M. Prasad, and J. Y. Chang, “Designing mamdani type fuzzy rule using a collaborative FCM scheme,” \\n\\nInternational Conference on Fuzzy Theory and Its Applications, 2013. \\n\\n125. L. Eugene, “Chapter 4.5. Combinatorial Implications of Max-Flow Min-Cut Theorem, Chapter 4.6. Linear \\n\\nProgramming Interpretation of Max-Flow Min-Cut Theorem,” Combinatorial Optimization: Networks and \\n\\nMatroids, Dover. pp. 117–120, 2001. \\n\\n126. C. H. Papadimitriou and K. Steiglitz, “Chapter 6.1 The Max-Flow, Min-Cut Theorem,” Combinatorial \\n\\nOptimization: Algorithms and Complexity. Dover. pp. 120– 128, 1998. \\n\\n127. A. S. Fotheringham, M. E. Charlton, and C. Brunsdon, “Geographically weighted regression: a natural evolution \\n\\nof the expansion method for spatial data analysis,” Environment and Planning, vol. 30, no. 11, pp. 1905-1927, \\n\\n1998. \\n\\n128. M. Honarkhah, and J. Caers, “Stochastic Simulation of Patterns Using Distance-Based Pattern Modeling,” \\n\\nMathematical Geosciences, vol. 42, no. 5, pp. 487–517, 2010. \\n\\n129. P. Tahmasebi, A. Hezarkhani, and M Sahimi, “Multiple-point geostatistical modeling based on the cross-\\n\\ncorrelation functions,” Computational Geosciences, vol.16, no. 3, pp. 779-797, 2012. \\n\\n130. S. Guha, R. Rastogi, and K. Shim, “ROCK: A Robust Clustering Algorithm for Categorical Attributes,” IEEE \\n\\nConference on Data Engineering, 1999. \\n\\n131. T. Zhang, R. Ramakrishnan, and M. Linvy, “BIRCH: An Efficient Method for Very Large Databases,” ACM \\n\\nSIGMOD, 1996. \\n\\n132.  D. Jiang, G. Chen, B. C. Ooi, K. L. Tan, and S. W, “epiC: an Extensible and Scalable System for Processing Big \\n\\nData,” 40th VLDB Conference, pp. 541 - 552, 2014. \\n\\n133. Z. Huang, “A Fast Clustering Algorithm to Cluster very Large Categorical Data Sets in Data Mining,” DMKD, \\n\\n1997. \\n\\n134. A. Hinneburg and D. Keim, “An Efficient Approach to Clustering in Large Multimedia Databases with Noise,” \\n\\nKDD Conference, 1998. \\n\\n135. M. J. A. Berry and G. Linoff, “Data Mining Techniques For Marketing, Sales and Customer Support,” John Wiley \\n\\n& Sons, Inc., USA, 1996. \\n\\n136. G. Fennell, G. M. Allenby, S. Yang and Y. Edwards, “The Effectiveness of Demographics and Phychographic \\n\\nVariables for Explaining Brand and Product Category Use,” Quantitative Marketing and Economics, vol. 1, no. 2, \\n\\npp. 223-224, 2003. \\n\\n137. M. Y. Kiang, D. M. Fisher, M. Y. Hu, “The effect of sample size on the extended self-organizing map network- A \\n\\nmarket segmentation application,” Computational Statistics and Data Analysis, vol. 51, no. 12, pp. 5940-5948, \\n\\n2007. \\n\\n138. S. Dolnicar, “Using Cluster Analysis for Market Segmentation–Typical Misconceptions, Established \\n\\nMethodological Weaknesses and Some Recommendations for Improvement,” Journal of Marketing Research, vol. \\n\\n11, no. 2, pp. 5-12, 2003. \\n\\n139. R. Wagner, S. W. Scholz, and R. Decker, “The number of clusters in market segmentation,” Data Analysis and \\n\\nDecision Support, Heidelberg: Springer, pp. 157-176, 2005. \\n\\n140. R. M. Durbin, S. R. Eddy, A. Krogh, and G. Mitchison, “Biological Sequence Analysis: Probabilistic Models of \\n\\nProteins and Nucleic Acids,” Cambridge: Cambridge University Press, 1998. \\n\\n141. J. M. Kaplan,el, R. G. Winther, “Prisoners of Abstraction? The Theory and Measure of Genetic Variation, and the \\n\\nVery Concept of “Race”,” Biological Theory, vol. 7. 2012. \\n\\n142. P. J. Carrington, and J. Scott, “Social Network Analysis: An Introduction,” The Sage Handbook of Social \\n\\nNetwork Analysis, London, vol. 1, 2011. \\n\\n143. “Yippy growing by leaps, bounds,” The News-Press. 23 May 2010, Retrieved 24 May 2010. \\n\\n144. D. Dirk, “A concept-oriented approach to support software maintenance and reuse activities” 5th Joint Conference \\n\\non Knowledge Based Software Engineering, 2002. \\n\\n145. M. G. B. Dias, N. Anquetil, and K. M. D. Oliveira, “Organizing the knowledge used in software maintenance,” \\n\\nJournal of Universal Computer Science, vol. 9, no. 7, pp. 641–658, 2003. \\n\\n146. R. Francesco, L Rokach and B. Shapira, “Introduction to Recommender Systems Handbook,” Recommender \\n\\nSystems Handbook, Springer, 2011, pp. 1-35. \\n\\n147.  “www.educationaldatamining.org,” 2013. \\n\\n148. R. Baker, “Data Mining for Education,” International Encyclopedia of Education (3rd edition), Oxford, UK, \\n\\nElsevier, vol. 7, pp. 112-118, 2010. \\n\\n149. G. Siemens, R. S. J. D. Baker, “Learning analytics and educational data mining: towards communication and \\n\\ncollaboration,” 2nd International Conference on Learning Analytics and Knowledge, pp. 252–254, 2012. \\n\\nhttp://www.bioinfo.in/uploadfiles/13031311552_1_1_JMLT.pdf\\nhttp://www.bioinfo.in/uploadfiles/13031311552_1_1_JMLT.pdf\\nhttp://www.sciencedirect.com/science/article/pii/S095741740700663X#bib18\\nhttp://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=p_Authors:.QT.Grefenstette,%20J.J..QT.&searchWithin=p_Author_Ids:37388906500&newsearch=true\\nhttp://en.wikipedia.org/wiki/Eugene_Lawler\\nhttp://en.wikipedia.org/wiki/Christos_H._Papadimitriou\\nhttp://en.wikipedia.org/wiki/Kenneth_Steiglitz\\nhttp://www.vldb.org/pvldb/vol7/p541-jiang.pdf\\nhttp://www.vldb.org/pvldb/vol7/p541-jiang.pdf\\nhttp://en.wikipedia.org/wiki/Richard_M._Durbin\\nhttp://en.wikipedia.org/wiki/Sean_Eddy\\nhttp://en.wikipedia.org/wiki/Anders_Krogh\\nhttp://books.google.com/books?id=2chSmLzClXgC&pg=PA1\\nhttp://www.news-press.com/article/20100523/BUSINESS/5230344/1014/BUSINESS/Yippy-growing-by-leaps--bounds\\nhttp://en.wikipedia.org/wiki/The_News-Press\\nhttp://www.educationaldatamining.org/\\n\\n\\n150. R. Huth, C. Beck, A. Philipp, M. Demuzere, Z. Ustrnul, M. Cahynova, J. Kysely, and O. E. Tveito, \\n\\n“Classifications of Atmospheric Circulation Patterns: Recent Advances and Applications” Annals of the New \\n\\nYork Academy Science, vol. 1146, no. 1, pp. 105-152, 2008. \\n\\n151. A. Bewley. R. Shekhar, S. Leonard, B. Upcroft, and P. Lever, “Real-time volume estimation of a dragline \\n\\npayload,” IEEE International Conference on Robotics and Automation\", pp. 1571-1576, 2011. \\n\\n152. C. D. Manning, P. Raghavan, and H. Schu¨tze, “An Introduction to Information Retrieval,” Cambridge University, \\n\\nPress, 2009. \\n\\n153. D. T. Nguyen, L. Chen, and C. K. Chan, “Clustering with Multi-viewpoint-Based Similarity Measure,” IEEE \\n\\nTransactions on Knowledge and Data Engineering, vol. 24, no. 6, pp. 988-1001, 2012. \\n\\n154.  Bravais, “Memoires par divers savants,” T, IX, Paris, pp. 255–332, 1846. \\n\\n155. K. Pearson, “Mathematical Contributions to the Theory of Evolution, III, Regression, Heredity, and Panmixia,” \\n\\nPhilosophical Transactions of the Royal Society of London, Series A, vol. 187, pp. 253–318, 1896. \\n\\n156. T. Sørensen, “A method of establishing groups of equal amplitude in plant sociology based on similarity of \\n\\nspecies and its application to analyses of the vegetation on Danish commons,” Kongelige Danske Videnskabernes \\n\\nSelskab, vol. 5, no. 4, pp. 1–34, 1948. \\n\\n157. L. R. Dice, “Measures of the Amount of Ecologic Association Between Species,” Ecology, vol. 26, no. 3, pp. \\n\\n297–302, 1945. \\n\\n158. J. D. Hamilton, “Time Series Analysis,” Princeton University Press, 1994. \\n\\n159. R. S. Tsay, “Analysis of Financial Time Series,” John Wiley & SONS, 2005. \\n\\n160. A Saxena and J. Wang, “Dimensionality Reduction with Unsupervised Feature Selection and Applying Non-\\n\\nEuclidean Norms for Classification Accuracy,” International Journal of Data Warehousing and Mining (IJDWM), \\n\\nvol. 6, no. 2, pp. 22–40, 2010. \\n\\n161. S. Arora, I. Chana, “A Survey of Clustering Techniques for Big Data Analysis,” 5th International Conference on \\n\\nThe Next Generation Information Technology Summit (Confluence), 2014. \\n\\n162. A. S. Shirkhorshidi, S. Aghabozorgi, T. Y. Wah, and T. Herawan, “Big Data Clustering: A Review,” Lecture \\n\\nNotes in Computer Science, vol. 8583, pp. 707-720, 2014. \\n\\n163. H. Wang, W. Wang, J. Yang, and P. S. Yu, “Clustering by Pattern Similarity in Large Data Sets,” International \\n\\nConference on Management of Data, ACM, 2002. \\n\\n164. Z. Huang, “A Fast Clustering Algorithm to Cluster Very Large Categorical Data Sets in Data Mining,” DMKD. \\n\\n1997. \\n\\n165. X. Wu, X. Zhu, G. Q. Wu, and W. Ding, “Data mining with big data,” IEEE Transaction on Knowledge and Data \\n\\nEngineering, vol. 26, no. 1, pp. 97-107, 2014. \\n\\n166. P. Russom, “Big Data Analytics,” TDWI Best Practices Report, Fourth Quarter, 2011. \\n\\n167. C. Xiao, F. Nie, and H. Huang, “Multi-view k-means clustering on big data,” The Twenty-Third International \\n\\nJoint Conference on Artificial Intelligence, AAAI, 2013. \\n\\n168. W. Fan and B. Albert, “Mining Big Data: Current Status and Forecast to the Future,” ACM SIGKDD Explorations \\n\\nNewsletter, vol. 14, no. 2, pp. 1-5, 2013. \\n\\n169. K. Shvachko, H. Kuang, S. Radia, and R. Chansler, “The hadoop distributed file system,” IEEE 26th Symposium \\n\\non Mass Storage Systems and Technologies (MSST), 2010. \\n\\n170. D. Jeffrey and S. Ghemawat, “MapReduce: a flexible data processing tool,” Communications of the ACM, vol. \\n\\n53, no. 1, pp. 72-77, 2010. \\n\\n171. https://hadoop.apache.org/ \\n\\n172. G. Celeux, and G. Govaert, “A classification EM algorithm for clustering and two stochastic versions,” \\n\\nComputational statistics & Data analysis, vol. 14, no. 3, pp. 315-332, 1992. \\n\\n173. L. Kaufman and P. Rousseeuw, “Finding Groups in Data: An Introduction to Cluster Analysis,” Wiley, 1990. \\n\\n174. R. Ngand and J. Han, “CLARANS: A method for clustering objects for spatial data mining,” IEEE Trans. \\n\\nKnowledge Data Engineering, vol. 14, no. 5, pp. 1003–1016, 2002. \\n\\n175. Sisodia, singh, sisodia, and saxena, “Clustering Techniques: A Brief Survey of Different Clustering Algorithms”, \\n\\nInternational Journal of Latest Trends in Engineering and Technology (IJLTET), vol. 1, no. 3, pp. 82-87, 2012. \\n\\n176. Zhong, Miao, and Wang, “A graph-theoretical clustering method based on two rounds of minimum spanning \\n\\ntrees,” Pattern Recognition, vol. 43, pp. 752 – 766, 2010. \\n\\n177.  Y. Chen, S. Sanghavi, and H. Xu, “Improved graph clustering,” IEEE Transactions on Information Theory, vol. \\n\\n60, no. 10, pp. 6440-6455, 2014. \\n\\n178. A. Condon, and R. Karp, “Algorithms for graph partitioning on the planted partition model,” Random Structures \\n\\nAlgorithms, vol. 18, no. 2, pp. 116-140, 2001. \\n\\n179. W. E. Donath and A. J. Hoffman, “Lower bounds for the partitioning of graphs,” IBM J. Res. Develop., vol. 17, \\n\\npp. 420 – 425, 1973. \\n\\n180. J. Shi, J. and J. Malik, “Normalized cuts and image segmentation,” IEEE Transactions on Pattern Analysis and \\n\\nMachine Intelligence, vol. 22, no. 8, pp. 888 – 905, 2000. \\n\\n181. U. Luxburg, “A tutorial on spectral clustering,” Statistics and Computing, vol. 17, no. 4, pp. 395-416, 2007. \\n\\n182. K. Rohe, S. Chatterjee, and B. Yu, “Spectral clustering and the high-dimensional stochastic block model,” The \\n\\nAnnals of Statistics, vol. 39, no. 4, pp. 1878-1915, 2011. \\n\\n183.  S. Gunnemann, I. Farber, B. Boden, and T. Seidl, “Subspace clustering meets dense sub-graph mining,” A \\n\\nsynthesis of two paradigms, In ICDM, 2010. \\n\\nhttp://en.wikipedia.org/wiki/Plant_sociology\\nhttp://en.wikipedia.org/wiki/Kongelige_Danske_Videnskabernes_Selskab\\nhttp://en.wikipedia.org/wiki/Kongelige_Danske_Videnskabernes_Selskab\\nhttp://amzn.to/1cqB6QD\\nhttp://amzn.to/1blTqWD\\nhttp://link.springer.com/bookseries/558\\nhttp://link.springer.com/bookseries/558\\n\\n\\n184. K. Macropol and A. Singh, “Scalable discovery of best clusters on large graphs,” Proceedings of the VLDB \\n\\nEndowment, vol. 3, no. 1-2, pp. 693-702, 2010. \\n\\n185. J. J. Whang, X. Sui, and I. S. Dhillon, “Scalable and memory-efficient clustering of large-scale social networks,” \\n\\nIn ICDM, 2012. \\n\\n186. G. Karypis and V. Kumar, “A fast and high quality multilevel scheme for partitioning irregular graphs,” SIAM \\n\\nJournal on Scientific Computing, vol. 20, no. 1, pp. 359-392, 1998. \\n\\n187. G. Karypis and V. Kumar, “Multilevel k-way partitioning scheme for irregular graphs,” Journal of Parallel and \\n\\nDistributed Computing, vol. 48, pp. 96-129, 1998. \\n\\n188. D. Yan, L. Huang, and M. I. Jordan, “Fast approximate spectral clustering,” In KDD, pp. 907-916, 2009. \\n\\n189. J. Liu, C. Wang, M. Danilevsky, and J. Han, “Large-scale spectral clustering on graphs,” In IJCAI, 2013. \\n\\n190. W. Yang and H. Xu, “A divide and conquer framework for distributed graph clustering,” In ICML, 2015. \\n\\n191. Ghosh and Dubey, “Comparative Analysis of K-Means and Fuzzy C Means Algorithms,” International Journal of \\n\\nAdvanced Computer Science and Applications, vol. 4, no.4, pp. 35-39, 2013. \\n\\n192. S. Niwattanakul, J. Singthongchai, E. Naenudorn and S. Wanapu, “Using of Jaccard Coefficient for Keywords \\n\\nSimilarity”, Proceedings of the International MultiConference of Engineers and Computer Scientists 2013 Vol I, \\n\\nIMECS 2013, March 13 - 15, 2013, Hong Kong, 1-5.  \\n\\n193. C. Chen, L. Pau, and P. Wang, “Hand book of Pattern Recognition and Computer Vision , Eds., World Scientific, \\n\\nSingapore, pp. 3 –32. R.Dubes, “Cluster analysis and related issue”. \\n\\n194. A. Jain and R. Dubes, “Algorithms for Clustering Data,” Englewood, Cliffs, NJ: Prentice-Hall, 1988. \\n\\n195. C. Shi, Y. Cai, D. Fu, Y. Dong, and B. Wu, “A link clustering based overlapping community detection algorithm,” \\n\\nData & Knowledge Engineering, vol. 87, pp. 394–404, 2013. \\n\\n196. G. Palla, I. Derenyi, I. Farkas, and T. Vicsek, “Uncovering the overlapping community structure of complex \\n\\nnetworks in nature and society,” Nature, vol. 435, pp. 814–818, 2005. \\n\\n197. D. H. Wolpert and W. G. Macready, “No Free Lunch Theorem for Optimization,” IEEE Transactions on \\n\\nEvolutionary Computation, vol. 1, No. 1, pp. 67-82, 1997 \\n\\n198. Bensmail, H., Celeux, G., Raftery, A. E. and Robert, C. P. (1997) Inference in model-based cluster analysis. \\n\\nStat.Comput., 7, 1–10. \\n\\n199. Xu.D., Tian, Y., “A Comprehensive Survey o f Clustering Algorithms”, Ann. Data Sci. 2, 165-193,2015. \\n\\n\\n'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["#Testing on a sample PDF file\n","filename = '/content/drive/MyDrive/IK/Dataset-IK/Amit Saxena/A Review of Clustering Techniques.pdf'\n","extract_text(filename)"]},{"cell_type":"markdown","id":"cb15d444","metadata":{"id":"cb15d444"},"source":["## If you only want to extract the 'Abstract' from a research paper\n","### Please note that this function might not work correctly for some PDFs.  "]},{"cell_type":"code","execution_count":null,"id":"abf8c8e1","metadata":{"id":"abf8c8e1"},"outputs":[],"source":["# Function to extract abstract from PDF\n","def extract_abstract(filename):\n","    # Parse PDF file\n","    parsed = parser.from_file(filename)\n","    print(type(parsed))\n","\n","    # Extract text from parsed data\n","    text = parsed[\"content\"]\n","    # Find introduction section by looking for keywords\n","    abstract_start = text.find(\"Abstract\")\n","    print(f\"abstract_start {abstract_start}\")\n","    abstract_end_options = [text.find(\"\\n1 \", abstract_start), text.find(\"\\ni \", abstract_start), text.find(\"\\nI \", abstract_start)]\n","    abstract_end = min(pos for pos in abstract_end_options if pos != -1)\n","    # Extract introduction section\n","    abstract = text[abstract_start:abstract_end]\n","\n","    return abstract"]},{"cell_type":"code","execution_count":null,"id":"NTeIc2_dmDpW","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"executionInfo":{"elapsed":1320,"status":"ok","timestamp":1714087288479,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"},"user_tz":240},"id":"NTeIc2_dmDpW","outputId":"a8127c09-3615-42f2-9aba-ef9db3f808e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'dict'>\n","abstract_start 794\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Abstract \\n\\nThis paper presents a comprehensive study on clustering: exiting methods and developments made at various \\n\\ntimes. Clustering is defined as an unsupervised learning where the objects are grouped on the basis of some \\n\\nsimilarity inherent among them. There are different methods for clustering the objects such as hierarchical, \\n\\npartitional, grid, density based and model based. The approaches used in these methods are discussed with their \\n\\nrespective states of art and applicability. The measures of similarity as well as the evaluation criteria, which are \\n\\nthe central components of clustering are also presented in the paper. The applications of clustering in some \\n\\nfields like image segmentation, object and character recognition and data mining are highlighted. \\n\\nKeywords: Unsupervised learning, Clustering, Data mining, Pattern recognition, Similarity measures \\n'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["filename = '/content/drive/MyDrive/IK/Dataset-IK/Amit Saxena/A Review of Clustering Techniques.pdf'\n","extract_abstract(filename)"]},{"cell_type":"markdown","id":"dcd681d9","metadata":{"id":"dcd681d9"},"source":["# Extracting Introduction"]},{"cell_type":"code","execution_count":null,"id":"e1f5c8de","metadata":{"id":"e1f5c8de"},"outputs":[],"source":["def extract_intro(filename):\n","    # Parse PDF file\n","    parsed = parser.from_file(filename)\n","\n","    # Extract text from parsed data\n","    text = parsed[\"content\"]\n","\n","    # Find introduction section by looking for the \"1 Introduction\" heading\n","    intro_start = text.find(\"Introduction\")\n","\n","    # Find the next heading that starts with \"2 \", \"ii \", or \"II \"\n","    intro_end_options = [text.find(\"\\n2 \", intro_start), text.find(\"\\nii \", intro_start), text.find(\"\\nII \", intro_start)]\n","    intro_end = min(pos for pos in intro_end_options if pos != -1)\n","\n","    # Extract the introduction section\n","    intro = text[intro_start:intro_end]\n","\n","    return intro"]},{"cell_type":"markdown","id":"9fc9167f","metadata":{"id":"9fc9167f"},"source":["# Using NER to extract author names"]},{"cell_type":"code","execution_count":null,"id":"bf35040d","metadata":{"id":"bf35040d"},"outputs":[],"source":["!pip install -U pip setuptools wheel\n","!pip install -U spacy\n","!python3 -m spacy download en_core_web_sm"]},{"cell_type":"markdown","id":"beb30d3e","metadata":{"id":"beb30d3e"},"source":["### Via 'spacy'\n","spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more."]},{"cell_type":"code","execution_count":null,"id":"b8269eeb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6912,"status":"ok","timestamp":1714087304385,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"},"user_tz":240},"id":"b8269eeb","outputId":"8905a61f-b90a-4615-a777-bf1f5c42920b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n","Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyPDF2\n","Successfully installed PyPDF2-3.0.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install PyPDF2"]},{"cell_type":"code","execution_count":null,"id":"efe3176c","metadata":{"id":"efe3176c"},"outputs":[],"source":["import PyPDF2 # Another text extractor. You can use it when tika doesn't work.\n","import spacy\n","\n","def extract_author_names(filename):\n","\n","    # Load the spaCy NER model\n","    nlp = spacy.load('en_core_web_sm')\n","\n","    # Open the PDF file\n","    pdf_file = open(filename, 'rb')\n","\n","    # Create a PDF reader object\n","    pdf_reader = PyPDF2.PdfReader(pdf_file)\n","\n","    # Extract the text from the first page\n","    first_page = pdf_reader.pages[0]\n","    first_page_text = first_page.extract_text()\n","\n","    # Close the PDF file\n","    pdf_file.close()\n","\n","    # Apply the spaCy NER model to the text\n","    doc = nlp(first_page_text)\n","    author_names = []\n","    for entity in doc.ents:\n","        if entity.label_ == 'PERSON':\n","            author_names.append(entity.text)\n","\n","    # Return the list of author names\n","    return author_names"]},{"cell_type":"code","execution_count":null,"id":"f6fXKmj7nw6y","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":122,"status":"ok","timestamp":1714087357036,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"},"user_tz":240},"id":"f6fXKmj7nw6y","outputId":"af3bc937-2c1d-468c-d9ea-d4f83436d802"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/IK/Dataset-IK/Amit Saxena/A Review of Clustering Techniques.pdf'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["filename"]},{"cell_type":"code","execution_count":null,"id":"pjAQyprozcz2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2020,"status":"ok","timestamp":1714087361511,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"},"user_tz":240},"id":"pjAQyprozcz2","outputId":"bf7f9e44-531d-485a-c210-2fc1fad1c1e5"},"outputs":[{"data":{"text/plain":["['Aruna Tiwari4',\n"," 'Meng Joo',\n"," 'Weiping Ding6',\n"," 'Chin -Teng Lin2',\n"," 'Guru Ghasidas Vishwavidyal']"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["extract_author_names(filename)"]},{"cell_type":"markdown","id":"3a77b384","metadata":{"id":"3a77b384"},"source":["## Extract 'Abstract', 'Introduction', and 'Author_Names' together"]},{"cell_type":"code","execution_count":null,"id":"86a14361","metadata":{"id":"86a14361"},"outputs":[],"source":["import spacy\n","!python3 -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":null,"id":"7dc42b01","metadata":{"id":"7dc42b01"},"outputs":[],"source":["!python3 -m spacy download en_core_web_lg"]},{"cell_type":"code","execution_count":null,"id":"04e1e5d2","metadata":{"id":"04e1e5d2"},"outputs":[],"source":["!python3 -m spacy download en"]},{"cell_type":"code","execution_count":null,"id":"3d69bded","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2730,"status":"ok","timestamp":1714087851420,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"},"user_tz":240},"id":"3d69bded","outputId":"2183add2-1562-4722-8a79-4bdbeeabb35c"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'dict'>\n","abstract_start 794\n","['Aruna Tiwari4', 'Meng Joo', 'Weiping Ding6', 'Chin -Teng Lin2', 'Guru Ghasidas Vishwavidyal']\n"]}],"source":["import os\n","import tika\n","from tika import parser\n","\n","#tqdm --> Can help you with the progress bar\n","\n","# Set up Tika\n","tika.initVM()\n","\n","#Testing on a sample PDF file\n","filename = '/content/drive/MyDrive/IK/Dataset-IK/Amit Saxena/A Review of Clustering Techniques.pdf'\n","\n","# Extract the introduction from the PDF file\n","text = extract_text(filename)\n","intro = extract_intro(filename)\n","abstract = extract_abstract(filename)\n","author_names = extract_author_names(filename)\n","print(author_names)"]},{"cell_type":"markdown","id":"688e2355","metadata":{"id":"688e2355"},"source":["# Complete Extraction"]},{"cell_type":"code","execution_count":null,"id":"22c1a894","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"22c1a894","outputId":"a860cf9d-cdb5-443c-8e60-b075f9b623ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Index for the failed files: 1\n","Index for the failed files: 2\n","Index for the failed files: 3\n","Index for the failed files: 4\n","Index for the failed files: 5\n","<class 'dict'>\n","abstract_start 572\n","<class 'dict'>\n","abstract_start 552\n","Index for the failed files: 8\n","Index for the failed files: 9\n","Index for the failed files: 10\n","<class 'dict'>\n","abstract_start 704\n","Index for the failed files: 12\n","<class 'dict'>\n","abstract_start 468\n","Index for the failed files: 14\n","Index for the failed files: 15\n","Index for the failed files: 16\n","Index for the failed files: 17\n","Index for the failed files: 18\n","Index for the failed files: 19\n","Index for the failed files: 20\n","<class 'dict'>\n","abstract_start 792\n","Index for the failed files: 22\n","Index for the failed files: 23\n","<class 'dict'>\n","abstract_start 2295\n","<class 'dict'>\n","abstract_start 794\n","Index for the failed files: 26\n","Index for the failed files: 27\n","<class 'dict'>\n","abstract_start 828\n","Index for the failed files: 29\n","<class 'dict'>\n","abstract_start 499\n","Index for the failed files: 31\n","Index for the failed files: 32\n","<class 'dict'>\n","abstract_start -1\n","Index for the failed files: 33\n","Index for the failed files: 34\n","Index for the failed files: 35\n","<class 'dict'>\n","abstract_start 949\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:PyPDF2._reader:incorrect startxref pointer(1)\n"]},{"name":"stdout","output_type":"stream","text":["Index for the failed files: 36\n","Index for the failed files: 37\n","Index for the failed files: 38\n","Index for the failed files: 39\n","<class 'dict'>\n","abstract_start 4170\n","<class 'dict'>\n","abstract_start 121\n","<class 'dict'>\n","abstract_start 1133\n","<class 'dict'>\n","abstract_start 1370\n","Index for the failed files: 44\n","<class 'dict'>\n","abstract_start 606\n","<class 'dict'>\n","abstract_start 394\n","Index for the failed files: 47\n","Index for the failed files: 48\n","<class 'dict'>\n","abstract_start 379\n","Index for the failed files: 50\n","<class 'dict'>\n","abstract_start -1\n","Index for the failed files: 51\n","Index for the failed files: 52\n","Index for the failed files: 53\n","Index for the failed files: 54\n","Index for the failed files: 55\n","Index for the failed files: 56\n","Index for the failed files: 57\n","Index for the failed files: 58\n","Index for the failed files: 59\n","Index for the failed files: 60\n","Index for the failed files: 61\n","Index for the failed files: 62\n","Index for the failed files: 63\n","Index for the failed files: 64\n","Index for the failed files: 65\n","Index for the failed files: 66\n","<class 'dict'>\n","abstract_start 1962\n","Index for the failed files: 68\n","Index for the failed files: 69\n","Index for the failed files: 70\n","<class 'dict'>\n","abstract_start 490\n","Index for the failed files: 72\n","Index for the failed files: 73\n","Index for the failed files: 74\n","Index for the failed files: 75\n","Index for the failed files: 76\n","<class 'dict'>\n","abstract_start 3690\n","Index for the failed files: 78\n","<class 'dict'>\n","abstract_start 5051\n","Index for the failed files: 80\n","<class 'dict'>\n","abstract_start 75255\n","Index for the failed files: 81\n","<class 'dict'>\n","abstract_start 3696\n","Index for the failed files: 83\n","Index for the failed files: 84\n","<class 'dict'>\n","abstract_start 575\n","<class 'dict'>\n","abstract_start -1\n","Index for the failed files: 86\n","Index for the failed files: 87\n","<class 'dict'>\n","abstract_start 1488\n","Index for the failed files: 89\n","<class 'dict'>\n","abstract_start 699\n","<class 'dict'>\n","abstract_start 1705\n","<class 'dict'>\n","abstract_start 650\n","Index for the failed files: 93\n","Index for the failed files: 94\n","Index for the failed files: 95\n","<class 'dict'>\n","abstract_start 794\n","<class 'dict'>\n","abstract_start -1\n","Index for the failed files: 97\n","<class 'dict'>\n","abstract_start 5735\n","Index for the failed files: 99\n","Index for the failed files: 100\n","Index for the failed files: 101\n","Index for the failed files: 102\n","Index for the failed files: 103\n","Index for the failed files: 104\n","Index for the failed files: 105\n","Index for the failed files: 106\n","Index for the failed files: 107\n","Index for the failed files: 108\n","Index for the failed files: 109\n","Index for the failed files: 110\n","Index for the failed files: 111\n","<class 'dict'>\n","abstract_start -1\n","Index for the failed files: 112\n"]}],"source":["import os\n","import tika\n","from tika import parser\n","\n","#tqdm --> Can help you with the progress bar\n","\n","# Set up Tika\n","tika.initVM()\n","\n","folder_names = []\n","texts = []\n","filenames = []\n","introduction_vector = []\n","abstract_vector = []\n","author_names_vector = []\n","\n","c = 0 # For printing indexes of the PDF files on which tika couldn't extract the text successfully\n","\n","for root, dirs, files in os.walk('/content/drive/MyDrive/IK/Dataset-IK/'):\n","     for file in files:\n","        #print(os.path.join(root, file))\n","\n","        c = c + 1\n","\n","        try:\n","          # /content/drive/MyDrive/IK/Dataset-IK/AUTHOR_NAME/PAPER_NAME\n","            # Extract the introduction from the PDF file\n","            text = extract_text(os.path.join(root, file))\n","            intro = extract_intro(os.path.join(root, file))\n","            abstract = extract_abstract(os.path.join(root, file))\n","            author_names = extract_author_names(os.path.join(root, file))\n","\n","            folder_names.append(root.split('/')[-1])\n","            texts.append(text)\n","            introduction_vector.append(intro)\n","            abstract_vector.append(abstract)\n","            author_names_vector.append(author_names)\n","            filenames.append(file)\n","\n","        except:\n","            print(\"Index for the failed files:\", c)\n","            #introduction_vector.append([])\n","            #abstract_vector.append([])\n","            #author_names_vector.append([])\n","            # texts.append([])\n","            # filenames.append([])\n","            # folder_names.append([])"]},{"cell_type":"code","execution_count":null,"id":"83de3e35","metadata":{"id":"83de3e35"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","df = pd.DataFrame(filenames,columns = ['FileName'])\n","df['Author'] = folder_names\n","df['Text'] = texts"]},{"cell_type":"code","execution_count":null,"id":"211fedc3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1713587199824,"user":{"displayName":"Shivam","userId":"00171756857051516066"},"user_tz":-330},"id":"211fedc3","outputId":"1cdc2673-8865-4ae6-8b4a-456a6797974a"},"outputs":[{"data":{"text/plain":["(182, 3)"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["df.shape"]},{"cell_type":"code","execution_count":null,"id":"27fddce3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1148,"status":"ok","timestamp":1713587200956,"user":{"displayName":"Shivam","userId":"00171756857051516066"},"user_tz":-330},"id":"27fddce3","outputId":"deb5db18-d3d8-4ff3-d46e-999d76646f79"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"df\",\n  \"rows\": 182,\n  \"fields\": [\n    {\n      \"column\": \"FileName\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 180,\n        \"samples\": [\n          \"Kolla2021_Chapter_HealthAssessmentAndModalAnalys.pdf\",\n          \"Robust Consensus.pdf\",\n          \"Soft metaphor detection using fuzzy c-means.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Author\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 59,\n        \"samples\": [\n          \"Yew-Soon Ong\",\n          \"Yayati Gupta\",\n          \"Vidhi Khanduja\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 178,\n        \"samples\": [\n          \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSee discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/338539015\\n\\nLateral Response Reduction of Tall Buildings Using Portal Frame as TMD\\n\\nChapter \\u00b7 January 2020\\n\\nDOI: 10.1007/978-981-15-1404-3_5\\n\\nCITATIONS\\n\\n0\\nREADS\\n\\n175\\n\\n4 authors, including:\\n\\nSome of the authors of this publication are also working on these related projects:\\n\\nSHM Based Fatigue Life Prediction View project\\n\\nEarthquake Resistant Design of Tall Structures View project\\n\\nPriyanka Palvai\\n\\nInternational Institute of Information Technology, Hyderabad\\n\\n1 PUBLICATION\\u00a0\\u00a0\\u00a00 CITATIONS\\u00a0\\u00a0\\u00a0\\n\\nSEE PROFILE\\n\\nEshwar Kuncham\\n\\nIndian Institute of Technology Mandi\\n\\n5 PUBLICATIONS\\u00a0\\u00a0\\u00a06 CITATIONS\\u00a0\\u00a0\\u00a0\\n\\nSEE PROFILE\\n\\nVenkata Dilip Kumar Pasupuleti\\n\\nMahindra University \\u00c9cole Centrale School of Engineering\\n\\n49 PUBLICATIONS\\u00a0\\u00a0\\u00a055 CITATIONS\\u00a0\\u00a0\\u00a0\\n\\nSEE PROFILE\\n\\nAll content following this page was uploaded by Venkata Dilip Kumar Pasupuleti on 27 August 2020.\\n\\nThe user has requested enhancement of the downloaded file.\\n\\nhttps://www.researchgate.net/publication/338539015_Lateral_Response_Reduction_of_Tall_Buildings_Using_Portal_Frame_as_TMD?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_2&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/publication/338539015_Lateral_Response_Reduction_of_Tall_Buildings_Using_Portal_Frame_as_TMD?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_3&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/project/SHM-Based-Fatigue-Life-Prediction?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_9&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/project/Earthquake-Resistant-Design-of-Tall-Structures?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_9&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_1&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/profile/Priyanka-Palvai?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_4&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/profile/Priyanka-Palvai?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_5&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/institution/International-Institute-of-Information-Technology-Hyderabad?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_6&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/profile/Priyanka-Palvai?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_7&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/profile/Eshwar-Kuncham?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_4&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/profile/Eshwar-Kuncham?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_5&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/institution/Indian-Institute-of-Technology-Mandi?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_6&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/profile/Eshwar-Kuncham?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_7&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/profile/Venkata-Pasupuleti?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_4&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/profile/Venkata-Pasupuleti?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_5&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/profile/Venkata-Pasupuleti?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_7&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/profile/Venkata-Pasupuleti?enrichId=rgreq-54bc2e3470f63110d643093916af9f4a-XXX&enrichSource=Y292ZXJQYWdlOzMzODUzOTAxNTtBUzo5Mjg5NDA3OTc0ODkxNTJAMTU5ODQ4ODE1MzczNQ%3D%3D&el=1_x_10&_esc=publicationCoverPdf\\n\\n\\nLateral Response Reduction of Tall\\nBuildings Using Portal Frame as TMD\\n\\nRaaga Varshita Chilakalapallii, Priyanka Palvai, Eshwar kuncham\\nand Venkata Dilip Kumar Pasupuleti\\n\\nAbstract Majority of construction industries are aiming to go for taller and lighter\\nbuildings which may result in flexible and slender structures. Hence serviceability\\nand safety become a critical issue during the occurrence of heavy winds and high\\nmagnitude earthquakes. Therefore, considerable techniques are adopted to minimize\\nthe vibrations caused by these natural responses of the structures. One of the tech-\\nniques used prominently for tall structures is Tuned Mass Damper (TMD). TMD\\u2019s\\nhave been very effective in controlling structural vibrations. This study proposes a\\ndetailed analysis of a 2D frame structure with a TMD system placed at different\\nlevels of the structure in order to evaluate the behaviour of structure for given earth-\\nquake ground motions. The results obtained indicate installation of simple frames\\ncan decrease the response of the structure during an earthquake and location of TMD\\nis also discussed in detail.\\n\\nKeywords TMD- tuned mass damper \\u00b7 Damping \\u00b7 And vibration control\\n\\n1 Introduction\\n\\nA growing population and advancement in technology have led to the evolution of\\nnew construction techniques which focuses on alternatives to reduce the damage\\ncaused to the structure due to heavy winds and seismic loads. Size and density of the\\ncities influence the height of the building hence taller structures are majorly adopted\\n\\nR. V. Chilakalapallii \\u00b7 P. Palvai \\u00b7 V. D. K. Pasupuleti\\nMahindra Ecole Centrale, College of Engineering, Hyderabad, Telangana, India\\ne-mail: varshita14231@mechyd.ac.in\\n\\nP. Palvai\\ne-mail: priyanka14134@mechyd.ac.in\\n\\nV. D. K. Pasupuleti\\ne-mail: venkata.pasupuleti@mechyd.ac.in\\n\\nE. kuncham (B)\\nIndian Institute of Technology Patna, Patna, India\\ne-mail: eshwar.research@gmail.com\\n\\n\\u00a9 Springer Nature Singapore Pte Ltd. 2020\\nK. Ganesh Babu et al. (eds.), Emerging Trends in Civil Engineering,\\nLecture Notes in Civil Engineering 61,\\nhttps://doi.org/10.1007/978-981-15-1404-3_5\\n\\n43\\n\\n\\n\\n44 R. V. Chilakalapallii et al.\\n\\nby many of the construction industries. However, taller structures are sensitive to\\nwind and seismic excitations. These excitations can cause large displacements in the\\nstructures such that they fail to satisfy the serviceability criteria. Therefore, to reduce\\nthese structural responses in tall structures different types of damping systems are\\nused. Dampers act as shock absorbers. Base isolation is ideal for short structures\\nwhich tend to undergo shear failures during an earthquake. The most commonly\\nused base isolation units used in the construction are laminated rubber bearing.\\nThese bearings are made of alternative layers of vulcanized rubber and steel [1]. But\\nthis phenomenon doesn\\u2019t give effective results for high rise buildings. The concept\\nof tuned mass damper (TMD) was evolved and first suggested by Frahm in 1909 to\\nenervate unenviable vibrations in ships. The device predominantly consists of a mass\\n(m), spring (k) and damping devices (c). The device is mounted on the structure to\\navert the failure of building [2]. The working principle of TMD is when building\\nbegins to oscillate, it sets the TMD into motion by means of spring, when building\\nsways to the right side, and consequently the TMD sways to left side in order to\\nnarrow down the excitation of structure.\\n\\nA TMD is naturally tuned to the first natural frequency of the structure. The\\nenergy dissipation effectiveness of a TMD depends on (a) The accuracy of its tuning,\\n(b) Mass of damper compared to modal mass of its target mode and (c) Extent of\\ninternal damping built-in TMD [3]. Themain advantage of incorporating TMD in tall\\nstructures is they don\\u2019t need any external power sources for their operation, unlike\\nother dampers. TMD\\u2019s are easy to maintain and also respond small excitations.\\nFew other dampers used in structures for the controlling of the wind and seismic\\nexcitations are Tuned Liquid Column Dampers (TLCD), Pendulum modelled TMD,\\nViscous dampers, etc.[4]. TLCD was proposed by sakai [5] to test the vibrations\\ninduced by wind and seismic excitations in the structure. The efficiency of TLCD\\ndepends on tuning the frequency of TLCDwith respect to the frequency of structures.\\n\\nThe viscous damper for structures is similar to the shock absorber on an auto-\\nmobile but operates at a much higher force level [6]. It is constructed of stainless\\nsteel. Pendulum Tuned Mass Dampers (PTMDs) are substituted with a translational\\nspring and damper system with a pendulum, it comprises of a mass sustained by\\na cable which pivots about a point. They are often designed as a simple pendulum\\n[7]. Even slight angular oscillations make the PTMD act similar to a translational.\\nIt is preferred over the translational TMD system because of the absence of any\\ntype of bearings. PTMDs are economical, adaptable and easy to maintain. PTMD is\\nimplemented in Taipei 101 tower in Taipei and Crystal Tower in Osaka [8].\\n\\n\\n\\nLateral Response Reduction of Tall Buildings Using \\u2026 45\\n\\n2 Existing TMD\\u2019s\\n\\n2.1 One Wall Center, Sheraton Vancouver, Canada\\n\\nOneWallCentre is a skyscraper hotel opened inVancouver\\u2019s downtown. The building\\nis of 157.8 m height above the ground level and extends up to 4 floors below the\\nground level [9]. As the building is tall and slender wind tunnel tests signified that\\nstorm winds approaching in the vicinal of the building would sway the structure.\\nTherefore, a conventional technique is followed to minimize the displacements with-\\nout changing the mass and stiffness of the structures; the structure is connected to a\\ndamper which functions as a shock absorber.\\n\\nTwo water tanks each with 16 m long \\u00d7 4.5 m wide \\u00d7 8 m tall and each tank\\ndesigned with a capacity of 50,000-imperial-gallon are installed as a pendulummod-\\nelled Tuned Water Damper in the building. The frequency of the splash of the water\\nin the tanks counteracts the frequency of the swaying of the building. Therefore,\\nwhen the building proceeds to sway under wind loading, the water moves to and fro\\ntransmits its momentum to the structure and restrains the effects of wind vibration.\\n\\n2.2 Taipei 101 Tower, Taipei\\n\\nThe Taipei 101 is an iconic super-tall structure. The structure used high-performance\\nsteel and concrete in its construction. Besides five basement levels, It also comprises\\nof 101 floors above ground. Outrigger trusses were installed at eight-floor intervals,\\nwhich joins the columns in the building\\u2019s core to those on the exterior. These com-\\nponents made Taipei 101 one of the most stable buildings. It is located just above\\n201.16 m from a major fault line, as a result, it is viable to earthquakes, and even\\nstormy winds are common in this area of the Asia-Pacific. Therefore, to attain stabil-\\nity and lessen the impact of violent motion engineers had to design a gigantic TMD\\nthat could withstand gale winds up to 216 km/h and the strongest earthquakes.\\n\\nIt\\u2019s basically an enormous weighted ball of 728 tons steel pendulum that serves\\nas a TMD, which is placed on hydraulic cylinders and counteracts the building\\u2019s\\nmovement. TheTMDsystem is suspended from the 92nd to 87th floorwith the design\\nof the simple pendulum. And this pendulum oscillates to counterbalance movements\\ntriggered in the building by strong gale. The dimensions of the sphere are 5.48 m\\nin diameter, comprising 41 circular steel plates of varying diameters, each 0.125 m\\nthick, welded together to form a 5.5 m diameter sphere. Two additional TMD\\u2019s,\\neach weighing 6 tones, are installed at the tip of the spire which halts the damage to\\nstructure due to strong wind loads [10].\\n\\n\\n\\n46 R. V. Chilakalapallii et al.\\n\\n2.3 Chiba Port Tower, Japan\\n\\nChiba Port Tower (125 m high) is a high-rise steel structure incorporated with steel-\\nframed reinforced concrete construction. A two-mass model TMD is installed at the\\ntop of the tower to reduce the wind vibrations on the structure. The damper system\\nconsists of a mass of 15 tones, two frames overlapped at right angles, these frames\\ncan slide in X and Y direction with the help of roller bearings. These frames consist\\nof coil springs and damping devices. The damping devices have a rotator in high\\nviscosity liquid and produce a damping force by shearing the liquid.\\n\\nThe TMD is designed by setting the damper weight to 100th of the first mode of\\neffective weight [11]. Thereby the TMD reduces the earthquake vibrations by 30%\\nand wind vibrations by 10% effectively.\\n\\n2.4 Chiba Port Tower, Japan\\n\\nThe Shanghai Center Tower (SHC) is a high-rise building with an elevation of\\n631.85 m. The design company stated that a building of this tall could be malleable\\nto more than 1.524 m of sway during the typhoon conditions. In order to counterbal-\\nance its vibrations during wind storms, a new eddy-current TMD was stationed on\\nthe 125th floor.\\n\\nSpecial protective appliances were blended to prevent excessively large amplitude\\nmotion of the TMD under extreme wind or earthquake scenarios. The damper is a\\n1000-ton weight is suspended with the aid of steel cables. TMD is incorporated with\\ntwo systems (hydraulic rams, and a \\u2018tuneable\\u2019 self-generated magnetic field) which\\nprevent the weight from moving too fast. The iron weight hangs above 10 m \\u00d7 10 m\\ncopper plate, which is studded with 125 powerful magnets. As the iron swings over\\nthe magnets, it initiates electrical current in the copper plate, which is enough to limit\\nthe motion of the mass. The merit is in its simplicity, there\\u2019s no necessity of power\\nsource, and is a completely a self-regulating system [12].\\n\\n3 Numerical Formulation\\n\\nThe present paper is centred on an MDOF structural system interconnected with\\nsingle TMD on top, an overture has been evolved to detect the optimum parameters\\nof TMD placed in a multi-storied building for minimum top deflection caused by\\nlateral excitation. Over the last few decades, considerable numbers of buildings were\\nincorporated with TMD\\u2019s all over the world.\\n\\n\\n\\nLateral Response Reduction of Tall Buildings Using \\u2026 47\\n\\n3.1 Dynamic Analysis\\n\\nThe first step for performing a dynamic analysis is to set up the equations of motions\\nandmost basic is Newton\\u2019s second law ofmotionwhich states that \\u201cthe rate of change\\nof momentum of a mass equals the force acting on it\\u201d [13] . Figure 1 shows steps\\ninvolved in derivation of dynamic equation of motion with and without damper.\\n\\nd/dt m(du/dt) = F(t) (1)\\n\\nIf one affixes a spring and a damper to the mass, by postulating that the spring\\ncomplies with Hooke\\u2019s law and the damping is of viscous type (the damping force is\\nproportional to the velocity of the mass), the equilibrium is formulated by summing\\nup the terms of spring and damping-induced forces.\\n\\nMu\\u0308(t) + Cu\\u0307(t) + Ku(t) = P(t) (2)\\n\\nwhere the dot over the symbol represents differentiation with respect to time t. M,\\nC, K represents the mass, damping, and stiffness of the structure respectively.\\n\\nUndamped Free vibration: During the initial excitation, when P(t) and damping\\nare zero the response is termed as free vibration.\\n\\nFrom Eq. (1),\\n\\nmu\\u0308(t) + ku(t) = 0 (3)\\n\\nLinear, homogeneous second-order differential equation\\n\\nu\\u0308 + k\\n\\nm\\nu = 0 (4)\\n\\nFor notational convenience, let\\u2019s assume\\n\\nFig. 1 a Equation of single degree of freedom system, b Undamped free vibration system\\n\\n\\n\\n48 R. V. Chilakalapallii et al.\\n\\nk\\n\\nm\\n= \\u03c92 (Hence\\u03c92 is positive number). (5)\\n\\nd2u\\n\\ndt2\\n= \\u2212\\u03c92u (6)\\n\\nThis is the differential equation for SHM.The sinusoidal solution for displacement\\nof spring can be given by\\n\\nx = A \\u2217 cos(\\u03c9t) (7)\\n\\nwhere A is the amplitude of the motion and \\u03c9 is the angular frequency.\\n\\n\\u03c92 = k\\n\\nm\\n=> \\u03c9 =\\n\\n\\u221a\\nk\\n\\nm\\n(8)\\n\\n3.2 Optimization Theory for TMD\\n\\nMany studies have been carried out in the past to understand and determine the\\noptimal parameters for a TMD under different kinds of excitations, to predict the\\nvibrational behaviour of the main structures and assess the efficiency of the TMD\\ncontributions in terms of attenuating vibrations in the main structure. Outcome of\\nfew studies has clearly mentioned that usage of elastic body can be supplanted by\\nan equivalent SDOF system for reduction of amplitudes during natural disasters.\\nProvided that the frequencies of the elastic body are well distinguished and the\\ndamper response is majorly contributed by the fundamental mode.\\n\\nIn the damping of the main system, it is recommended that constrained damping\\nof the main system has a very small effect on the TMD\\u2019s optimal parameters. In real\\nsystems with light damping, if this frequency condition is satisfied, it is plausible to\\napply the optimal parameters of the TMD for the undamped equivalent system to\\nlessen the dynamic response of the system.\\n\\nSingle Degree of Freedom System.\\nThe equations of motion for an undamped SDOF system with TMD\\n\\nM_mu\\u0308 + K_mu + c_au\\u0307 + k_au \\u2212 c_au\\u0307_a \\u2212 k_au_a = p(t) (9)\\n\\nmau\\u0308a + cau\\u0307a + kaua \\u2212 cau\\u0307 \\u2212 kau = 0 (10)\\n\\nwhere M, K ,U represents mass, stiffness, and displacement, respectively, whereas\\nm and a refers to the parameters of structure and TMD, respectively.\\n\\nIf harmonic forces act on the system, then p(t) = pei\\u03c9t . Therefore, the displace-\\nment of the main system can be reduced if the vibration of system is with its natural\\n\\n\\n\\nLateral Response Reduction of Tall Buildings Using \\u2026 49\\n\\nFig. 2 Schematic diagram\\nof SDOF with TMD\\n\\nfrequency, schematic diagram of SDOF with tuned mass damper is shown in Fig. 2.\\n\\nu =\\n(\\nka \\u2212 ma\\u03c9\\n\\n2 + i\\u03c9ca\\n)\\npei\\u03c9t(\\n\\nKm + ka \\u2212 Mm\\u03c92 + i\\u03c9ca\\n)(\\nka \\u2212 ma\\u03c92 + i\\u03c9ca\\n\\n) \\u2212 (ka + i\\u03c9ca)\\n2 (11)\\n\\nVelocity of the system is\\n\\nu\\u0307 =\\n(\\nka \\u2212 ma\\u03c9\\n\\n2 + i\\u03c9ca\\n)\\ni\\u03c9pei\\u03c9t(\\n\\nKm + ka \\u2212 Mm\\u03c92 + i\\u03c9ca\\n)(\\nka \\u2212 ma\\u03c92 + i\\u03c9ca\\n\\n) \\u2212 (ka + i\\u03c9ca)\\n2 (12)\\n\\nAcceleration of the system is\\n\\nu\\u0308 = \\u2212\\u03c92\\n(\\nka \\u2212 ma\\u03c9\\n\\n2 + i\\u03c9ca\\n)\\npei\\u03c9t(\\n\\nKm + ka \\u2212 Mm\\u03c92 + i\\u03c9ca\\n)(\\nka \\u2212 ma\\u03c92 + i\\u03c9ca\\n\\n) \\u2212 (ka + i\\u03c9ca)\\n2 (13)\\n\\nMass ratio:\\n\\n\\u03bc = ma/Mm (14)\\n\\nTuning ratio:\\n\\nf = \\u03c9a/\\u03c9m (15)\\n\\n\\u03c92\\na = ka\\n\\nma\\n(16)\\n\\n\\u03c92\\nm = Km/Mm (17)\\n\\nForced frequency ratio:\\n\\nr = \\u03c9a/\\u03c9m (18)\\n\\nAbsorber damping ratio:\\n\\n\\u03b3a = ca/2ma\\u03c9a (19)\\n\\n\\n\\n50 R. V. Chilakalapallii et al.\\n\\nP(t) = Force vector\\n\\nmaZ jr u\\u0308a + ca Z jr u\\u0307a + ka Z jr ua \\u2212 ca Z jr U\\u0307 j \\u2212 ka Z jrU j = 0 (20)\\n\\nZ is the matrix of orthogonal characteristics modes of the system and Zr is the\\nvector of the r th\\n\\nMode,\\n\\nZr = (\\nZ1r , Z2r , . . . , Z jr , . . . , Znr\\n\\n)T\\n(21)\\n\\n4 Numerical Modelling\\n\\nInitially, a two-Dimensional 15 storey structure is modelled using ETABS as shown\\nin Fig. 4. Respective dimensions for the structure are assigned based on Table 1.\\nModal analysis has been carried out. The time period of the structure is found to be\\n1.141 cycles/sec. Using trial and error basis, a single portal frame is analysed to get\\nsame frequency of the structure. This portal frame ismounted on the top of 15th storey\\nand time history analysis is carried out. Same procedure is followed with TMD on\\ndifferent floors of the structure. For the next trial, the dimensions of the portal frame\\nare changed to 5 m (beam)\\u00d7 2 m (column) and made sure that its frequency matches\\nwith 15 storey structure. Portal frame is mounted on various floors of the structure\\nand checked for the minimum response. Similarly, same procedure is carried (for\\nboth the portal frames) by placing the portal frame in inverted position.\\n\\nTable 1 Geometry and\\nmaterial property of 2D\\nframed structure\\n\\nProperty Dimension\\n\\nStructure Column dimension 450 mm \\u00d7 300 mm\\n\\nStructure Beam dimensions 300 mm \\u00d7 300 mm\\n\\nColumn dimensions of TMD 161 mm \\u00d7 161 mm\\n\\nBeam dimensions of TMD 1324 mm \\u00d7 1324 mm\\n\\nGrade of Concrete M 25\\n\\nRebar\\u2019s HYSD 415\\n\\nHeight of storey 3 m\\n\\n\\n\\nLateral Response Reduction of Tall Buildings Using \\u2026 51\\n\\n5 Case Study\\n\\n5.1 System Model\\n\\nThe 15 storey structure consisting of rigid floors and beams which are supported\\nby deformable columns is modeled with uniform bay with 3 m. All the numerical\\nmodels presented in this study are developed using commercially available ETABS\\nsoftware. And natural period of 15 storey structure 1.141 s.\\n\\nThe joints at the base of the structure are restrained to rotation and translation in\\nboth x and y directions. The designed TMD is tuned to the same natural frequency\\nof the building and installed on the building.\\n\\nCase 1. Erected TMD system of height 2 m installed on top of slab. In this\\nstudy, a TMD of column and beam length of 2 m specified in Table 1 is placed at\\n15th, 14th and 13th storey of building as shown in the Fig. 3a.\\n\\nCase 2. Inverted TMD system of height 2 m: In this case the TMD is placed in\\nan inverted position and analysis is carried. The same TMD dimensions mentioned\\nin the Table 1 are considered.\\n\\nCase 3. Erected TMD system of height 5 m installed on top of slab: In this\\ncase, the column length of the TMD is increased to 5 m and the dimensions of the\\ncolumn and beam of TMD remain same as mentioned in Table 1.\\n\\nCase 4. Inverted TMD system of height 5 m: In this case, we have placed the\\nTMD inverted at 13th, 14th and 15th storey as shown in fig i, ii and iii, respectively.\\nThe dimensions of TMD are unchanged.\\n\\nFig. 3 a Installation of TMD on 13 th, 14th, 15th floor, b Installation of inverted TMD on 15th,\\n14th, 13th floor\\n\\n\\n\\n52 R. V. Chilakalapallii et al.\\n\\n6 Results\\n\\nA parametric study for installation of TMD at three different heights is carried to\\nunderstand the response of the structure.\\n\\n6.1 Model Analysis\\n\\nStructural displacement response is compared with and without TMD and for the\\nsame plots have been plotted in Fig. 4. It is observed that the response of the structure\\nhaving the TMD decays faster than that of without TMD. It is observed that when\\nthe TMD is placed over the 14th storey, the structural response to the applied ground\\nmotion is less when compared to the responses of the building with TMD installed\\nat 12th or 13th storey of the structure.\\n\\n6.2 Dynamic Analysis\\n\\nTo analyse the 2D frame EL CENTRO (Centro 1940 North\\u2013South Component,\\nPeknold Version) earthquake ground motion with a scale factor of 10 with equal\\ntime interval of 0.02 s. Figure 5 shows the comparisional time history for various\\nscenarios. Maximum seismic response reduction is found when TMD of height 5 m\\nis erected of 14th storey of the structure. Comparably from the analysis and results\\nobtained, erected portal frame TMD is more effective than inverted portal frame\\nTMD model.\\n\\n7 Conclusion\\n\\nThis study indicates using a portal frame as TMD can be very effective in controlling\\nvibrations of structure due to seismic or high wind loads and few major observations\\nof this research work are give as follows:\\n\\ni. Portal frame in the structure is more effective TMD than on the top.\\nii. Inverted TMD\\u2019s are observed to be more efficient in reducing the lateral\\n\\nvibration.\\niii. Location of the TMDhas greatly affected the vibration behavior of the structure.\\n\\n\\n\\nLateral Response Reduction of Tall Buildings Using \\u2026 53\\n\\nFig. 4 Mode shapes of the structure with TMD of a column length 2 m placed at 13th, 14th, 15th,\\nb column length 2 m placed inverted at 13th, 14th, 15th, c column length 5 m placed at 12th, 13th,\\n14th and d column length 5 m placed inverted at 15th, 14th, 13th\\n\\nReferences\\n\\n1. Tsai, H.-C. (1994). The effect of tuned-mass dampers on the seismic response of base-isolated\\nstructures. International Journal of Solids and Structures, 32(8\\u20139), 1195\\u20131210.\\n\\n2. Tuan Alex, Y., & Shang, G. Q. (2014). Vibration control in a 101-storey building using a Tuned\\nMass Damper. Applied Science and Engineering, 17(2), 141\\u2013156.\\n\\n3. Tuned Mass Dampers, http://www.deicon.com/air-suspended-tuned-mass-damper/. Accessed\\n9 Sepetember 2018.\\n\\n4. Tuned Mass Dampers, https://www.slideshare.net/subhajitbiswas102/tuned-mass-damper.\\n5. Tuned Liquid Column Dampers, Natural Hazards, University of Notre Dame.\\n6. Samuele, I., Jamieson, R., & Rob, S. (2008). Viscous dampers in high rise buildings. 14th\\n\\nWorld Conference on Earthquake Engineering.\\n\\n\\n\\n54 R. V. Chilakalapallii et al.\\n\\nFig. 5 Displacement of structure with a 2 m TMD at 15th storey, b 2 m TMD at 14th storey, c 2 m\\nTMD at 13th storey, d 2 m inverted TMD at 15th storey, e 2 m inverted TMD at 14th storey, f 2 m\\ninverted TMD at 13th storey, g 5 m TMD at 14th storey, h 5 m TMD at 13th storey, i 5 m TMD\\nat 12th storey, j 5 m inverted TMD at 14th storey, k 5 m inverted TMD at 15th storey and l 5 m\\ninverted TMD at 13th storey\\n\\nFig. 5 (continued)\\n\\n\\n\\nLateral Response Reduction of Tall Buildings Using \\u2026 55\\n\\n7. Richard, L. (2011). Design, construction and testing of an adaptive pendulum tuned mass\\ndamper. University of Waterloo.\\n\\n8. https://en.wikipedia.org/wiki/Tuned_mass_damper.\\n9. Robert, S. (2001). One Wall Centre, Vancouver. Canadian Consulting Engineer magazine.\\n10. Eric, L. (2015). Taipei 101\\u2019s mass damper. Popular mechanics.\\n11. Harruyuki, K., & Takafumi, F. (1988). Design and analysis of a tower structure with a Tuned\\n\\nMass Damper. Ninth World Conference on Earthquake Engineering.\\n12. Chris, M. (2015). How a 121 storey building uses a giant magnet to prevent swaying, Gizmodo.\\n13. Chopra, A. K. (2017). Dynamics of structures: Theory and applications to earthquake\\n\\nengineering (5th ed.). Englewood Cliffs, NJ: Prentice Hall.\\n\\nView publication stats\\n\\nhttps://www.researchgate.net/publication/338539015\\n\\n\",\n          \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSee discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/320558117\\n\\nDetecting Stealth DHCP Starvation Attack using Machine Learning Approach\\n\\nArticle\\u00a0\\u00a0in\\u00a0\\u00a0Journal of Computer Virology and Hacking Techniques \\u00b7 August 2018\\n\\nDOI: 10.1007/s11416-017-0310-x\\n\\nCITATIONS\\n\\n22\\nREADS\\n\\n7,327\\n\\n2 authors, including:\\n\\nSome of the authors of this publication are also working on these related projects:\\n\\nNovel Application Layer Denial-of-Service Attacks and their Detection View project\\n\\nNikhil Tripathi\\n\\nIndian Institute of Technology Indore\\n\\n17 PUBLICATIONS\\u00a0\\u00a0\\u00a0284 CITATIONS\\u00a0\\u00a0\\u00a0\\n\\nSEE PROFILE\\n\\nAll content following this page was uploaded by Nikhil Tripathi on 11 January 2018.\\n\\nThe user has requested enhancement of the downloaded file.\\n\\nhttps://www.researchgate.net/publication/320558117_Detecting_Stealth_DHCP_Starvation_Attack_using_Machine_Learning_Approach?enrichId=rgreq-02eda3dbeebee4ee29030b278c3ffc06-XXX&enrichSource=Y292ZXJQYWdlOzMyMDU1ODExNztBUzo1ODE1NDk5Njk1NTU0NTZAMTUxNTY2MzcyNTcxOQ%3D%3D&el=1_x_2&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/publication/320558117_Detecting_Stealth_DHCP_Starvation_Attack_using_Machine_Learning_Approach?enrichId=rgreq-02eda3dbeebee4ee29030b278c3ffc06-XXX&enrichSource=Y292ZXJQYWdlOzMyMDU1ODExNztBUzo1ODE1NDk5Njk1NTU0NTZAMTUxNTY2MzcyNTcxOQ%3D%3D&el=1_x_3&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/project/Novel-Application-Layer-Denial-of-Service-Attacks-and-their-Detection?enrichId=rgreq-02eda3dbeebee4ee29030b278c3ffc06-XXX&enrichSource=Y292ZXJQYWdlOzMyMDU1ODExNztBUzo1ODE1NDk5Njk1NTU0NTZAMTUxNTY2MzcyNTcxOQ%3D%3D&el=1_x_9&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/?enrichId=rgreq-02eda3dbeebee4ee29030b278c3ffc06-XXX&enrichSource=Y292ZXJQYWdlOzMyMDU1ODExNztBUzo1ODE1NDk5Njk1NTU0NTZAMTUxNTY2MzcyNTcxOQ%3D%3D&el=1_x_1&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/profile/Nikhil-Tripathi-2?enrichId=rgreq-02eda3dbeebee4ee29030b278c3ffc06-XXX&enrichSource=Y292ZXJQYWdlOzMyMDU1ODExNztBUzo1ODE1NDk5Njk1NTU0NTZAMTUxNTY2MzcyNTcxOQ%3D%3D&el=1_x_4&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/profile/Nikhil-Tripathi-2?enrichId=rgreq-02eda3dbeebee4ee29030b278c3ffc06-XXX&enrichSource=Y292ZXJQYWdlOzMyMDU1ODExNztBUzo1ODE1NDk5Njk1NTU0NTZAMTUxNTY2MzcyNTcxOQ%3D%3D&el=1_x_5&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/institution/Indian_Institute_of_Technology_Indore?enrichId=rgreq-02eda3dbeebee4ee29030b278c3ffc06-XXX&enrichSource=Y292ZXJQYWdlOzMyMDU1ODExNztBUzo1ODE1NDk5Njk1NTU0NTZAMTUxNTY2MzcyNTcxOQ%3D%3D&el=1_x_6&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/profile/Nikhil-Tripathi-2?enrichId=rgreq-02eda3dbeebee4ee29030b278c3ffc06-XXX&enrichSource=Y292ZXJQYWdlOzMyMDU1ODExNztBUzo1ODE1NDk5Njk1NTU0NTZAMTUxNTY2MzcyNTcxOQ%3D%3D&el=1_x_7&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/profile/Nikhil-Tripathi-2?enrichId=rgreq-02eda3dbeebee4ee29030b278c3ffc06-XXX&enrichSource=Y292ZXJQYWdlOzMyMDU1ODExNztBUzo1ODE1NDk5Njk1NTU0NTZAMTUxNTY2MzcyNTcxOQ%3D%3D&el=1_x_10&_esc=publicationCoverPdf\\n\\n\\nNoname manuscript No.\\n(will be inserted by the editor)\\n\\nDetecting Stealth DHCP Starvation Attack using Machine\\nLearning Approach\\n\\nNikhil Tripathi, Neminath Hubballi\\n\\nDiscipline of Computer Science and Engineering, School of Engineering\\n\\nIndian Institute of Technology Indore, Madhya Pradesh, India\\n\\n{phd1401101002, neminath}@iiti.ac.in\\n\\nReceived: date / Accepted: date\\n\\nAbstract Dynamic Host Configuration Protocol\\n\\n(DHCP) is used to automatically configure clients with\\n\\nIP address and other network configuration parame-\\n\\nters. Due to absence of any in-built authentication, the\\n\\nprotocol is vulnerable to a class of Denial-of-Service\\n\\n(DoS) attacks, popularly known as DHCP starvation\\n\\nattacks. However, known DHCP starvation attacks\\n\\nare either ineffective in wireless networks or not\\n\\nstealthy in some of the network topologies. In this\\n\\npaper, we first propose a stealth DHCP starvation\\n\\nattack which is effective in both wired and wireless\\n\\nnetworks and can not be detected by known detection\\n\\nmechanisms. We test the effectiveness of proposed\\n\\nattack in both IPv4 and IPv6 networks and show that\\n\\nit can successfully prevent other clients from obtaining\\n\\nIP address, thereby, causing DoS scenario. In order to\\n\\ndetect the proposed attack, we also propose a Machine\\n\\nLearning (ML) based anomaly detection framework.\\n\\nIn particular, we use some popular one-class classifiers\\n\\nfor the detection purpose. We capture IPv4 and IPv6\\n\\ntraffic from a real network with thousands of devices\\n\\nand evaluate the detection capability of different\\n\\nmachine learning algorithms. Our experiments show\\n\\nthat the machine learning algorithms can detect the\\n\\nattack with high accuracy in both IPv4 and IPv6\\n\\nnetworks.\\n\\nKeywords Anomaly Detection \\u00b7 One-class Classifiers \\u00b7\\nDHCP \\u00b7 DHCPv6 \\u00b7 DHCP Starvation Attack\\n\\n1 Introduction\\n\\nDynamic Host Configuration Protocol (DHCP) [2] is\\n\\nused to obtain network configuration parameters in-\\n\\nAddress(es) of author(s) should be given\\n\\ncluding IP address from a DHCP server. This proto-\\n\\ncol is vulnerable to a class of Denial-of-Service (DoS)\\n\\nattacks popularly known as classical DHCP starvation\\n\\nattacks. Classical DHCP starvation attacks [4], [5] re-\\n\\nquire a malicious client to inject a large number of IP re-\\n\\nquests using spoofed MAC addresses. For every such re-\\n\\nquest received, a new IP address is released by a DHCP\\n\\nserver. Thus, eventually DHCP server runs out of the\\n\\nIP addresses. However, it is not easy to launch classical\\n\\nDHCP starvation attacks using spoofed MAC addresses\\n\\nin wireless networks as Access Point (AP) drops all the\\n\\npackets having source or destination MAC address pre-\\n\\nviously not associated with it. The only way to create a\\n\\nstarvation attack is to precede and maintain association\\n\\nwith AP for each spoofed MAC address. However, con-\\n\\nsidering the computational complexity involved in asso-\\n\\nciation and key exchange phase in WPA2 wireless net-\\n\\nworks, it is not feasible to perform multiple manual as-\\n\\nsociations [11]. Moreover, various security features like\\n\\nport security [25] implemented on network switches can\\n\\neasily mitigate this attack by disabling the suspicious\\n\\nport on which multiple MAC addresses are seen at a\\n\\ntime. On the other hand, Induced DHCP starvation at-\\n\\ntacks [17], [6], though effective in wireless networks, can\\n\\nbe mitigated by features like Dynamic ARP Inspection\\n\\n(DAI) [1] in wired networks as discussed in Section 3.3.\\n\\nIn this paper, we propose a new stealth DHCP star-\\n\\nvation attack that is effective in both IPv4 and IPv6\\n\\nnetworks. This attack exploits IP address conflict de-\\n\\ntection scheme implemented on all DHCP clients. This\\n\\nattack is highly stealth as various popular security fea-\\n\\ntures in modern network switches can not detect the\\n\\nattack. Moreover, other proposed methods belonging to\\n\\neither of three categories, viz., Encryption [14], Thresh-\\n\\nold based [23] and Fair IP address allocation [27] have\\n\\nvarious drawbacks such as implementation complexity\\n\\n\\n\\n2 Please give a shorter version with: \\\\authorrunning and \\\\titlerunning prior to \\\\maketitle\\n\\nissues in encryption based techniques, high misclassifi-\\n\\ncation rate and cumbersome process of deciding thresh-\\n\\nold in threshold based mechanisms and infeasibility of\\n\\nIP address allocation to each port in wireless networks.\\n\\nMotivated from this fact, we also propose an anomaly\\n\\ndetection framework that uses one-class classification\\n\\ntechniques to detect the proposed attack. To develop ef-\\n\\nficient and robust Intrusion Detection/Prevention Sys-\\n\\ntems (IDS/IPS), machine learning algorithms are al-\\n\\nready used in the literature [8], [9]. Authors in [7] pre-\\n\\nsented recent comprehensive study on use of machine\\n\\nlearning techniques to detect intrusion in the networks.\\n\\nOur proposed detection framework learns the normal\\n\\ntraffic (DHCP and ARP in IPv4 and DHCPv6 and NS\\n\\nin IPv6) behaviour and once trained, can be put into\\n\\nuse to detect anomalies in the network traffic behaviour.\\n\\nIn particular, we make following specific contributions:\\n\\n\\u2013 We propose a stealth DHCP starvation attack that\\n\\nis easier to launch and difficult to detect by known\\n\\nsecurity measures.\\n\\n\\u2013 We test the effectiveness of proposed DHCP star-\\n\\nvation attack in both IPv4 and IPv6 networks and\\n\\nreport the results.\\n\\n\\u2013 We propose an anomaly detection framework in\\n\\nwhich we use various one-class classifiers to detect\\n\\nanomalous traffic in different time intervals.\\n\\nRest of the paper is organized as follows. We pro-\\n\\nvide a brief overview of DHCP working in Section 2. In\\n\\nSection 3, we describe our proposed DHCP starvation\\n\\nattack. Experiments performed to test effectiveness of\\n\\nproposed attack are presented in Section 4. We discuss\\n\\nthe proposed detection framework\\u2019s working and exper-\\nimental results in Section 5 and Section 6 respectively.\\n\\nWe discuss the prior works available in the literature to\\n\\ndetect starvation attacks in Section 7. Finally the paper\\n\\nis concluded in Section 8.\\n\\n2 Background\\n\\nIn this section, we briefly discuss DHCP and DHCPv6\\n\\noperations in IPv4 and IPv6 networks respectively.\\n\\n2.1 DHCP in IPv4 Networks\\n\\nDHCP servers are implemented in local networks to\\n\\nautomate the IP address allocation and thus, to reduce\\n\\nthe IP address conflicts. DHCP provides ability to de-\\n\\nfine TCP/IP configurations from a central location and\\n\\nthus, it can handle any network changes very efficiently.\\n\\nIt is quite cumbersome to allot IP address and other\\n\\nnetwork configuration parameters (e.g., default gateway\\n\\nFig. 1: Exchange of messages in DHCP operation\\n\\nand DNS server IP address) to each individual device\\n\\nmanually within the network. Moreover, it may lead to\\n\\nIP address conflicts also as same IP address can be mis-\\n\\ntakenly allotted to two or more different devices. Due\\n\\nto these reasons, almost every other network today is\\n\\nequipped with one or more DHCP servers.\\n\\nDHCP server, implemented within a network, is\\n\\nconfigured with a pool of IP addresses and other net-\\n\\nwork configuration parameters. As soon as a client joins\\n\\nthe network, it tries to configure its interface with\\n\\nan IP address by exchanging four messages with the\\n\\nDHCP server as shown in Figure 1. These messages are\\n\\nDISCOVER, OFFER, REQUEST and ACK and thus,\\n\\nthe complete process of IP address allocation is known\\n\\nas DORA process. Also, there are various other mes-\\n\\nsage types which are exchanged in case of unsuccessful\\n\\nIP address configuration. DHCPDECLINE is one such\\n\\nmessage that is transmitted by client to DHCP server\\n\\nif client finds that the allotted IP address is already\\n\\nassigned to some other client. A DHCP enabled client\\n\\nfinds out such IP address conflicts using ARP request\\n\\nprobes destined to the IP address in question.\\n\\n2.2 DHCPv6 in IPv6 Networks\\n\\nIn an IPv6 network, Stateful mode of IP addressing\\n\\nuses DHCPv6 [3] server to allocate site-local unicast\\n\\nIPv6 addresses to clients. Similar to DHCP server in\\n\\nIPv4 networks, DHCPv6 server is also configured with\\n\\na pool of IP addresses and other network configuration\\n\\nparameters. As soon as a client joins the network, it\\n\\ntries to configure its interface with a site-local unicast\\n\\nIP address by exchanging four messages with the DHCP\\n\\nserver. These messages are SOLICIT, ADVERTISE,\\n\\nREQUEST and REPLY and the purpose of these mes-\\n\\n\\n\\nDetecting Stealth DHCP Starvation Attack using Machine Learning Approach 3\\n\\nsages is similar to DHCPDISCOVER, DHCPOFFER,\\n\\nDHCPREQUEST and DHCPACK respectively in IPv4\\n\\nnetworks. Various other DHCPv6 message types and\\n\\ntheir purposes are described in RFC 3315 [3]. DE-\\n\\nCLINE is one of the message types that is sent by client\\n\\nto DHCP server if it finds that the allotted IP address\\n\\nis already in use by some other client. The client finds\\n\\nsuch IP address conflicts using NS probe messages des-\\n\\ntined to the allotted IP address.\\n\\n3 Proposed Stealth DHCP Starvation Attack\\n\\nIn this section, we discuss the working of proposed\\n\\nstealth DHCP starvation attack in IPv4 and IPv6 net-\\n\\nworks and subsequently compare it with previously\\n\\nknown starvation attacks.\\n\\n3.1 Stealth DHCP Starvation Attack in IPv4 Networks\\n\\nConsider a network topology similar to the one shown\\n\\nin Figure 2. In this topology, there are four entities\\n\\nnamely DHCP server, malicious client, victim client\\n\\nand a switch. All entities are connected to switch using\\n\\nwired connection.\\n\\nFig. 2: Proposed Stealth DHCP Starvation Attack Event Se-\\nquence\\n\\nThe sequence of events occurred while launching\\n\\nstealth DHCP starvation attack in an IPv4 network is\\n\\nas follows:\\n\\n1. Manual IP Address Configuration by Malicious\\n\\nClient: As soon as malicious client joins the network,\\n\\nit disables the DHCP daemon running in back-\\n\\nground and manually configures its interface with\\n\\nan IP address and other configuration parameters.\\n\\nThe malicious client does so because in such a way,\\n\\nDHCP server does not allot the IP address to mali-\\n\\ncious client and thus, it does not possess any record\\n\\nof IP address binded to malicious client\\u2019s MAC ad-\\n\\ndress in its DHCP database. This step helps mali-\\n\\ncious client to bypass DAI security feature which is\\n\\ndependent on trusted DHCP snooping database to\\n\\ncheck inconsistencies in IP-MAC bindings.\\n\\n2. Address Allocation to Victim Client using DORA\\n\\nProcess: Victim client joins the network and ac-\\n\\nquires IP address from DHCP server using DORA\\n\\nprocess. After successful allocation, DHCP server\\n\\nadds an entry for a binding between offered IP and\\n\\nvictim client\\u2019s MAC address in its database.\\n\\n3. ARP Request Broadcast by Victim Client: The\\n\\nvictim client broadcasts an ARP request to check\\n\\nif the offered IP address is already in use. The\\n\\nsource IP address of this ARP request is \\u201c0.0.0.0\\u201d\\n\\nas victim client does not configure its network\\n\\ninterface with the offered IP prior to performing\\n\\nconflict check.\\n\\n4. ARP Request Broadcast by Malicious Client: As\\n\\nsoon as malicious client receives ARP probe request\\n\\nsent in step 3, it also broadcasts an ARP probe\\n\\nrequest. The source IP address of this probe is\\n\\nset to \\u201c0.0.0.0\\u201d, target IP address is set to IP\\n\\naddress in question, target MAC address is set to\\n\\n\\u201c00:00:00:00:00:00\\u201d, destination MAC in ethernet\\n\\nheader is set to \\u201cff:ff:ff:ff:ff:ff\\u201d while the source\\n\\nMAC address in ethernet header is set to malicious\\n\\nclient\\u2019s interface MAC address.\\n\\n5. DHCPDECLINE Message Broadcast by Victim\\n\\nClient: Once the victim client receives the ARP\\n\\nprobe request sent in step 4, it declines the as-\\n\\nsigned IP address by broadcasting a DHCPDE-\\n\\nCLINE message. DHCP server, on receiving this\\n\\nmessage, marks the IP address in question as un-\\n\\navailable for the length of lease period. Victim client\\n\\nreinitiates the process of acquiring a new IP ad-\\n\\ndress and set of operations are repeated; effectively\\n\\npreventing the victim client from ever acquiring an\\n\\naddress. This ultimately leads to DoS scenario. As\\n\\nDHCP server marks every declined IP addresses un-\\n\\navailable for the length of lease period, it runs out\\n\\nof IP addresses available in pool after few iterations.\\n\\n3.2 Stealth DHCP Starvation Attack in IPv6 Networks\\n\\nThe sequence of events occurred while launching\\n\\nproposed attack in an IPv6 network is as follows:\\n\\n1. Manually Deriving Link-Local and Site-Local Uni-\\n\\ncast IP Address by Malicious Client: As soon as\\n\\nmalicious client joins the network, it disables the\\n\\nDHCPv6 daemon running in background and man-\\n\\n\\n\\n4 Please give a shorter version with: \\\\authorrunning and \\\\titlerunning prior to \\\\maketitle\\n\\nually configures its interface with site-local unicast\\n\\nIPv6 address.\\n\\n2. Deriving Link-Local and Site-Local Unicast IP Ad-\\n\\ndress by Victim Client: As soon as victim client joins\\n\\nthe network, it first automatically derives a link-\\n\\nlocal unicast IP address without the intervention of\\n\\nDHCP server and configures its interface with this\\n\\nIP. After deriving link-local unicast IP address, vic-\\n\\ntim client tries to obtain a routable site-local unicast\\n\\nIP address. To do so, it exchanges SOLICIT, AD-\\n\\nVERTISE, REQUEST and REPLY messages with\\n\\nDHCPv6 server. Site-local unicast IP address are\\n\\nequivalent to private IP addresses in IPv4 networks.\\n\\n3. Duplicate Address Detection (DAD) Checking for\\n\\nSite-local Unicast IP Address by Victim Client: Be-\\n\\nfore using the site-local unicast IP address offered\\n\\nby DHCPv6 server in Step 2, victim client checks if\\n\\nthe address is already in use by some other client. To\\n\\nperform this check, it broadcasts a NS probe with\\n\\ntarget IP address set to offered site-local unicast ad-\\n\\ndress. The source and destination IP address of this\\n\\nprobe is set to unspecified address (::) and solicited\\n\\nnode multicast address respectively.\\n\\n4. Similar NS Message by Malicious Client: As soon\\n\\nas malicious client receives NS probe sent by victim\\n\\nclient in Step 3, it broadcast a similar NS probe with\\n\\nthe target, source and destination IP address of the\\n\\nprobe set to offered site-local unicast IP address,\\n\\nunspecified address (::) and solicited node multicast\\n\\naddress respectively.\\n\\n5. DECLINE Message by Victim Client: When victim\\n\\nclient receives the NS probe sent by malicious client\\n\\nin previous step, it broadcast a DECLINE message\\n\\nto DHCPv6 server informing that the offered ad-\\n\\ndress is already in use by some other client. DHCP\\n\\nserver, on receiving DECLINE message, marks the\\n\\nIP address in question as unavailable for the lease\\n\\ntime. Victim client then reinitiates the process of\\n\\nacquiring a new IP address and set of operations\\n\\nare repeated; effectively preventing the victim client\\n\\nfrom ever acquiring an address. This results into\\n\\nDoS scenario.\\n\\n3.3 Stealth DHCP Starvation Attack v/s Classical and\\n\\nInduced DHCP Starvation Attacks\\n\\nThe proposed attack is easier to launch and stealthier\\n\\nas compared to previously known starvation attacks in\\n\\nthe following way:\\n\\n\\u2013 Attacks\\u2019 Effectiveness in Wired and Wireless Net-\\n\\nworks: Since the proposed attack does not require\\n\\nMAC address spoofing, it can easily be launched\\n\\nin wireless networks. Thus, it is equally effective in\\n\\nboth wired and wireless networks similar to Induced\\n\\nDHCP starvation attack. However, as discussed ear-\\n\\nlier, classical DHCP starvation attack is not feasible\\n\\nto launch in wireless networks due to MAC spoofing\\n\\nand also, it fails to exhaust IP address pool in wire-\\n\\nless networks even if a fake association is preceded\\n\\ndue to restrictions on number of MAC address as-\\n\\nsociations enforced by AP.\\n\\n\\u2013 Attacks\\u2019 Stealthiness: Various security features\\n\\navailable in modern network switches can not de-\\n\\ntect proposed attack as discussed in Section 7. How-\\n\\never, classical DHCP starvation attack can easily\\n\\nbe mitigated using port security [25] while Induced\\n\\nDHCP starvation attack can be mitigated using Dy-\\n\\nnamic ARP Inspection (DAI) [1] assuming a net-\\n\\nwork topology similar to the one shown in Figure\\n\\n3. We can notice that the DAI enabled switch drops\\n\\nthe fake ARP reply sent by malicious client since the\\n\\ntrusted DHCP snooping database does not have any\\n\\nrecord of IP-MAC binding present in the fake ARP\\n\\nreply.\\n\\nFig. 3: Ineffectiveness of Induced DHCP Starvation Attack\\n\\n4 Experimental Setup and Results\\n\\nIn order to test the effectiveness of proposed attack\\n\\nin an IPv4 network, we created a testbed consisting\\n\\nof four computers and one ISC DHCP server having\\n\\n252 available IP addresses in its pool. One computer\\n\\nwas designated as malicious client while other three\\n\\nas victim clients. Malicious and victim clients were us-\\n\\ning Kali Linux 2.0 and Windows 7 operating systems\\n\\nrespectively. All these machines were connected using\\n\\na switch. We deployed a python script on malicious\\n\\nclient to sniff ARP probes and send corresponding ARP\\n\\n\\n\\nDetecting Stealth DHCP Starvation Attack using Machine Learning Approach 5\\n\\nprobes in order to mislead victim clients. Using this\\n\\nsetup, we launched the attack. While attack was going\\n\\non, victim clients were trigerred to get an IP address\\n\\nfrom DHCP server. After getting IP address, victim\\n\\nclients started probing to check for any address con-\\n\\nflict. Each time malicious client received an ARP probe\\n\\nfrom victim clients, it sent similar ARP probes. As a re-\\n\\nsult, victim clients could not acquire IP addresses even\\n\\nafter several attempts. Figure 4 shows the number of\\n\\nIP addresses released (equal to number of ARP probes\\n\\nsent by victim clients) by DHCP server over time. The\\n\\nproposed attack took around 970 seconds to exhaust IP\\n\\npool.\\n\\nFig. 4: Number of IP Addresses released by DHCP Server\\nOver Time\\n\\nTo test proposed attack in an IPv6 network, we cre-\\n\\nated a similar network setup. Instead of ISC DHCP\\n\\nserver, we used Dibbler DHCPv6 server. A python\\n\\nscript on malicious client was used to sniff NS probes\\n\\nand send corresponding NS probes to mislead victim\\n\\nclients. Using this setup, we launched the attack for\\n\\n1000 seconds. During this time period, we observed that\\n\\neach victim client continuously tried to configure its in-\\n\\nterface with a link-local unicast IPv6 address and each\\n\\ntime, malicious client sent NS probe in response to the\\n\\nprobe sent by victim client for conflict detection pur-\\n\\npose. As a result, victim clients could not acquire site-\\n\\nlocal unicast IP addresses even after several attempts.\\n\\nThus, proposed attack is effective in creating starva-\\n\\ntion scenario in IPv6 network also. Figure 5 shows the\\n\\nnumber of IP addresses released (equal to number of\\n\\nNS probes sent by victim clients) by DHCPv6 server\\n\\nover duration of 1000 seconds. Since the pool size of\\n\\nDHCPv6 server was very large, it would take a long\\n\\ntime to consume the whole pool. Nevertheless, attack\\n\\nremained effective as victim clients were not able to ob-\\n\\ntain IP address due to conflict generated by malicious\\n\\nclient.\\n\\nFig. 5: Number of IP Addresses released by DHCPv6 Server\\nin 1000 seconds\\n\\n5 Proposed Anomaly Detection Framework\\n\\nIn this section, we present our proposed anomaly detec-\\n\\ntion framework that uses machine learning approach to\\n\\ndetect the stealth DHCP starvation attack. In partic-\\n\\nular, we use six different one-class classification algo-\\n\\nrithms for detection purpose - Gaussian Density based\\n\\none-class classifier [18], Naive Bayes one-class classi-\\n\\nfier [20], K-Nearest Neighbour (KNN) algorithm [19],\\n\\nK-means clustering algorithm [18], Principal Compo-\\n\\nnent Analysis (PCA) [18] and Support Vector Data\\n\\nDescription (SVDD) [21]. The reason behind choosing\\n\\ndifferent one-class classifiers is to help network adminis-\\n\\ntrators to select a suitable classification technique based\\n\\non the network characteristics. One-class classification\\n\\nalgorithms are found to be highly useful in the scenar-\\n\\nios when only information of one of the classes, the\\n\\ntarget class, is available. The task of classification in-\\n\\nvolves defining such boundary around target class so as\\n\\nto cover as much of the target objects as possible and\\n\\nat the same time to minimize the chance of accepting\\n\\noutlier objects.\\n\\nThe proposed framework, shown in Figure 6, con-\\n\\nsists of two phases: the pre-processing phase and the\\n\\nclassification phase. In subsequent subsections, we dis-\\n\\ncuss these two phases.\\n\\n5.1 Pre-processing Phase\\n\\nIn this phase, DHCP and ARP (with source IP\\n\\n\\u201c0.0.0.0\\u201d) traffic is collected using a packet sniffer to ex-\\n\\ntract packet information like ARP header, UDP header\\n\\nand DHCP header from each packet. We use JNet-\\n\\nPcap [12] java library to extract packet information.\\n\\nAfter that, the packet\\u2019s information is partitioned by\\n\\nconsidering different message types and formed into a\\n\\nrecord by aggregating information after every \\u2206T time\\n\\n\\n\\n6 Please give a shorter version with: \\\\authorrunning and \\\\titlerunning prior to \\\\maketitle\\n\\nFig. 6: Proposed Anomaly Detection Framework\\n\\nTable 1: Candidate Features to Detect Attacks in IPv4 Net-\\nworks\\n\\nFeatures Description Type\\nFeature-1 Number of DHCPDISCOVER\\n\\nmessages in \\u2206T\\nNumeric\\n\\nFeature-2 Number of DHCPRREQUEST\\nmessages in \\u2206T\\n\\nNumeric\\n\\nFeature-3 Number of DHCPDECLINE\\nmessages in \\u2206T\\n\\nNumeric\\n\\nFeature-4 Number of ARP probe requests\\nwith source IP \\u201c0.0.0.0\\u201d in \\u2206T\\n\\nNumeric\\n\\nTable 2: Candidate Features to Detect Attacks in DHCPv6\\nequipped IPv6 Networks\\n\\nFeatures Description Type\\nFeature-1 Number of SOLICIT messages in\\n\\n\\u2206T\\nNumeric\\n\\nFeature-2 Number of REQUEST messages\\nin \\u2206T\\n\\nNumeric\\n\\nFeature-3 Number of DECLINE messages\\nin \\u2206T\\n\\nNumeric\\n\\nFeature-4 Number of NS probe requests\\nhaving link-local unicast source\\nIP and site-local unicast target IP\\nin \\u2206T\\n\\nNumeric\\n\\nduration. Each record consists of key signature features\\n\\nrepresenting the characteristics of network traffic dur-\\n\\ning \\u2206T . After extensive experiments, we find 4 essen-\\n\\ntial features to detect any anomaly in both IPv4 and\\n\\nIPv6 networks. These features along with their types\\n\\nare shown in Tables 1 and 2 for IPv4 and IPv6 net-\\n\\nworks respectively. We consider counts of only those\\n\\nmessages as key features which are broadcasted within\\n\\nthe network. Messages like DHCPOFFER, DHCPACK,\\n\\nARP reply in IPv4 networks and ADVERTISE, RE-\\n\\nPLY, NA in IPv6 networks are unicast and we do not\\n\\nconsider count of these messages as features. Due to\\n\\nthis reason, even if proposed anomaly detection frame-\\n\\nwork can not receive unicast communication, it can still\\n\\ndetect anomaly.\\n\\nFew examples of data records obtained after pre-\\n\\nprocessing phase in IPv4 network are shown in Table\\n\\nTable 3: Example of Data Records from Pre-processing Phase\\nin IPv4 Network\\n\\nInterval Data of 4 features Class\\nT1 69, 250, 1, 1286 Normal\\nT2 49, 260, 0, 1435 Normal\\nT3 58, 127, 0, 1150 Normal\\n\\n3 where network traffic in time intervals T1, T2 and T3\\nbelong to normal class.\\n\\n5.2 Classification Phase\\n\\nIn the classification phase, data records obtained from\\n\\nthe pre-processing phase is classified as normal or at-\\n\\ntack data. The classification phase is further divided\\n\\ninto training and testing period. In training period,\\n\\nwe train the selected one-class classifier as a detection\\n\\nmodel by using pre-processed data records belonging to\\n\\nnormal traffic (answer class) only collected over a pe-\\n\\nriod of n time intervals, each of size \\u2206T . Thus, all data\\n\\nrecords during training period are labeled as normal\\n\\ncategory. Once the detection model is trained, we test\\n\\nthe model with new or unknown dataset during testing\\n\\nperiod where each record was captured in a real-time\\n\\nenvironment as shown in classification phase in Figure\\n\\n6.\\n\\n5.3 Consistency-based Model Selection for Different\\n\\nOne-class Classifiers\\n\\nIn order to select model with optimized parameter, we\\n\\nuse consistency-based model selection [28] with differ-\\n\\nent classifiers. This model selection works based on 2-\\n\\nsigma bound around error of classification model. It de-\\n\\ntermines a threshold error, errthr, as shown in Equation\\n\\n1 and runs different classifiers with their parameters till\\n\\nit yields less error than errthr. We use consistency based\\n\\nmodel selection in order to select optimal parameter so\\n\\nthat classifiers can create a proper boundary.\\n\\nerrthr =(M \\u2217 fracrej + sigma thr \\u2217 sqrt(fracrej\\u2217\\n(1 \\u2212 fracrej) \\u2217M))/M\\n\\n=fracrej + sigma thr \\u2217 sqrt(fracrej\\u2217\\n(1 \\u2212 fracrej)/M)\\n\\n(1)\\n\\nHere, M = (N/f) such that f denotes number of\\n\\nfolds, M denotes number of samples in validation set,\\n\\nsigma thr = Required threshold for estimating deci-\\n\\nsion boundary during model selection, M \\u2217 fracrej\\n\\n\\n\\nDetecting Stealth DHCP Starvation Attack using Machine Learning Approach 7\\n\\ndenotes estimated number of objects to be rejected,\\n\\nsqrt(fracrej \\u2217 (1 \\u2212 fracrej) \\u2217M) is the standard de-\\n\\nviation and M \\u2217 fracrej + sigma thr \\u2217 sqrt(fracrej \\u2217\\n(1 \\u2212 fracrej) \\u2217M is the maximum allowed number of\\n\\nrejected target objects. For our experiments, we took\\n\\nfracrej = 5% of most erroneous data.\\n\\n6 Experiments and Performance Evaluation\\n\\nIn this section, we describe the experiments conducted\\n\\nto evaluate the detection performance of proposed\\n\\nframework. In next few subsections, we discuss testbed\\n\\nsetup, datasets for training and testing purpose and de-\\n\\ntection performance of proposed framework in IPv4 and\\n\\nIPv6 networks.\\n\\n6.1 Testbed Setup for IPv4 Network\\n\\nTo collect normal DHCP and ARP traffic for train-\\n\\ning period, we used our institute network which con-\\n\\nnects approximately 3000 heterogeneous clients like mo-\\n\\nbile phones, laptops and desktops as shown in Figure\\n\\n7. There were five types of entities in the network as\\n\\nDHCP server, malicious client, switch, packet sniffer\\n\\nand genuine clients. The DHCP server used in the in-\\n\\nstitute network was ISC DHCP server. This server was\\n\\nconfigured to offer 65,536 IP addresses1 in the range of\\n\\n10.100.0.0 to 10.100.255.255. We installed tcpdump, a\\n\\npacket sniffer, on one computer to capture only broad-\\n\\ncast DHCP traffic and ARP request messages having\\n\\nsource IP \\u201c0.0.0.0\\u201d. This computer was having 16 GB\\n\\nof physical memory and Core i5 quad core processor and\\n\\nusing Ubuntu 16.04LTS operating system. The genuine\\n\\nclients joined and left the network to access different re-\\n\\nsources as usual without any intervention by us. In this\\n\\nway, we collected normal DHCP and ARP traffic from\\n\\nIPv4 network to evaluate the detection performance of\\n\\nproposed framework.\\n\\nIn order to collect mixed (normal and anomalous)\\n\\ntraffic for testing period, we configured malicious client\\n\\nto launch proposed stealth DHCP starvation attack\\n\\nagainst only one victim client such that remaining gen-\\n\\nuine clients were unaffected by this attack and able to\\n\\ncommunicate within the network as usual. The mali-\\n\\ncious client\\u2019s computer was having 4 GB of physical\\n\\nmemory and Core i5 quad core processor and using\\n\\nKali 2.0 operating system. We wrote a python script\\n\\nusing scapy library [13] and executed it on malicious\\n\\n1 Among 65536, one each was allotted to malicious client\\nand server itself. Other two IP addresses, 10.100.0.0 and\\n10.100.255.255 were Network and Broadcast address respec-\\ntively and were not used.\\n\\nclient in order to sniff ARP requests having source IP\\n\\n\\u201c0.0.0.0\\u201d from genuine clients and send ARP requests\\n\\naccordingly. These two functions were implemented as\\n\\ntwo threads such that first thread executed the sniffing\\n\\nmodule while second thread generated ARP requests\\n\\nwith source IP \\u201c0.0.0.0\\u201d. In this way, we collected mixed\\n\\nDHCP and ARP traffic from IPv4 network to evaluate\\n\\nthe detection performance of proposed framework.\\n\\nFig. 7: Testbed for Data Collection\\n\\n6.2 Testbed Setup for IPv6 Network\\n\\nTo collect normal DHCPv6 and NS traffic for training\\n\\nperiod, we configured a Dibbler DHCPv6 server in the\\n\\nsame institute network. Since clients nowadays come\\n\\nwith IPv4/IPv6 dual stack with IPv6 enabled by de-\\n\\nfault, they start configuring their interfaces with IPv6\\n\\naddress as soon as they receive ADVERTISE message\\n\\nfrom DHCPv6 server in response to SOLICIT message\\n\\nsent by them. So, we did not need to configure each\\n\\nclient to use IPv6 along with IPv4. We set the filter-\\n\\ning parameters of tcpdump accordingly so as to capture\\n\\nonly DHCPv6 traffic and NS messages having link-local\\n\\nunicast source IP and site-local unicast target IP. Using\\n\\nthis setup, we collected normal DHCPv6 and NS traf-\\n\\nfic to evaluate the detection performance of proposed\\n\\nframework in IPv6 network.\\n\\nIn order to collect mixed (normal and anomalous)\\n\\ntraffic for testing period, we configured malicious client\\n\\nto launch the attack against only one victim client such\\n\\nthat remaining genuine clients were unaffected by this\\n\\nattack. To launch the attack in IPv6 network, we mod-\\n\\nified the python script executed on malicious client to\\n\\nsniff DHCPv6 traffic and NS messages having link-local\\n\\n\\n\\n8 Please give a shorter version with: \\\\authorrunning and \\\\titlerunning prior to \\\\maketitle\\n\\nunicast source IP and site-local unicast target IP and\\n\\nto send corresponding NS messages. In this way, we\\n\\ncollected mixed DHCPv6 and NS traffic from IPv6 net-\\n\\nwork.\\n\\n6.3 Training and Testing Dataset in IPv4 Network\\n\\nWe collected 3 days of normal DHCP and ARP traffic\\n\\nfrom the packet sniffer and partitioned the whole col-\\n\\nlected traffic in several smaller time intervals \\u2206T = 10\\n\\nminutes. As a result, we obtained 432 smaller time in-\\n\\ntervals. The traffic in these time intervals was then pre-\\n\\nprocessed in the pre-processing phase which resulted\\n\\ninto 432 normal records with each record having 4 fea-\\n\\ntures similar to the example shown in Table 3. Out of\\n\\nthese 432 normal records, we used 144 normal records\\n\\n(i.e. 1 day normal traffic) for training purpose. The re-\\n\\nmaining 288 normal records (i.e. 2 days normal traf-\\n\\nfic) was then used for testing purpose. We further vali-\\n\\ndated proposed framework by rotating the training and\\n\\ntesting data so as to use different training and testing\\n\\ndata in each iteration. In this way, we performed 3\\u2212fold\\n\\nvalidation. We also collected 3 days of mixed DHCP\\n\\nand ARP traffic generated by targeting only one victim\\n\\nclient. This traffic was also partitioned in time intervals\\n\\nof size \\u2206T = 10 minutes. As a result, we obtained 432\\n\\nsmaller time intervals which was then pre-processed to\\n\\nform 432 attack records. As a result, we obtained a total\\n\\nof 720 records for testing purpose in which there were\\n\\n288 normal records and 432 attack records.\\n\\n6.4 Detection Performance in IPv4 Network\\n\\nTable 4 shows various one-class classifiers and the\\n\\nassociated parameters along with their range. For few\\n\\nof the classifiers, we selected range which was further\\n\\nused by consistency-based model selection to choose\\n\\nan optimal parameter while few classifiers used the\\n\\nrange chosen by DD toolbox [29] itself. Table 5 shows\\n\\nmean of various detection performance metrics along\\n\\nwith deviations for different one-class classifiers. These\\n\\nresults were obtained after 3\\u2212fold cross validation.\\n\\nWe compared the performance of various one-class\\n\\nclassifiers integrated in DD toolbox available for\\n\\nmatlab. We used following performance metrics to\\n\\nevaluate detection performance of proposed framework:\\n\\nAccuracy = TP+TN\\nTP+TN+FP+FN\\n\\nRecall = TP\\nTP+FN\\n\\nPrecision = TP\\nTP+FP\\n\\nTable 4: One-Class Classifiers along with Parameters and\\ntheir Range\\n\\nClassifer Parameter\\nName\\n\\nParameter Range\\n\\nGaussian\\nDensity\\nbased\\nClassifier\\n\\nRegularization\\nparameter\\n\\n[10\\u22126, 106]\\n\\nNaive\\nBayes\\nClassifier\\n\\nk-smoothing\\nparameter\\n\\nDefault range selected by\\nDD toolbox itself\\n\\nKNN Clas-\\nsifier\\n\\nNumber of\\nNearest Neigh-\\nbours\\n\\n[1-20]\\n\\nK-means\\nClustering\\n\\nNumber of\\nClusters\\n\\n[1-20]\\n\\nPCA Fraction of\\nAccepted\\nVariance\\n\\nDefault value selected by\\nDD toolbox itself\\n\\nSVDD Width parame-\\nter in the RBF\\nkernel\\n\\nTwenty values are se-\\nlected between minimum\\nand maximum pair wise\\nsquared Euclidean dis-\\ntance among training\\nrecords\\n\\nHere TP is True Positive, FP is False Positive,\\n\\nTN is True Negative and FN is False Negative. A TP\\n\\nis the case when normal record is correctly detected\\n\\nas normal record by framework while FP is the case\\n\\nwhen attack record is incorrectly detected as normal\\n\\none. Similarly, TN is the case when attack record\\n\\nis correctly detected as attack record by framework\\n\\nwhile FN is the case when normal record is incorrectly\\n\\ndetected as attack one.\\n\\nWe can notice from Table 5 that K-means cluster-\\n\\ning gave best detection results while SVDD performed\\n\\nworst for detection purpose. Observed Precision for all\\n\\nthe classifiers was 100%. We can also notice that the\\n\\ntime taken to build the model was highest in case of\\n\\nNaive Bayes classifier while it was lowest for KNN clas-\\n\\nsifier. Thus, based on these performance metrics, ad-\\n\\nministrators need to keep a trade-off between the time\\n\\ntaken to build the model and detection accuracy and\\n\\nthen decide which classifier is appropriate in a particu-\\n\\nlar network to detect proposed attack.\\n\\n6.5 Training and Testing Dataset in IPv6 network\\n\\nWe collected 3 days of normal DHCPv6 and NS traffic\\n\\nfrom the packet sniffer and partitioned the whole traffic\\n\\nin several smaller time intervals again of size \\u2206T =\\n\\n10 minutes. As a result, we obtained 432 smaller time\\n\\nintervals. On pre-processing the traffic in these time\\n\\nintervals, we obtained 432 normal records. Out of these\\n\\n\\n\\nDetecting Stealth DHCP Starvation Attack using Machine Learning Approach 9\\n\\nTable 5: Detection Performance of Various One-Class Classifiers in case of IPv4 Network\\n\\nClassifer Accuracy Recall Precision Time taken to build model (in seconds)\\nGaussian Density based Classifier 96.02\\u00b12.93 90.05\\u00b17.34 100\\u00b10 0.0548\\u00b10.0476\\nNaive Bayes Classifier 94.68\\u00b12.64 86.69\\u00b16.61 100\\u00b10 1.9342\\u00b13.0651\\nKNN Classifier 96.20\\u00b12.26 90.51\\u00b15.66 100\\u00b10 0.0427\\u00b10.0332\\nK-means Clustering 96.44\\u00b11.90 91.09\\u00b14.74 100\\u00b10 0.6236\\u00b10.5640\\nPCA 96.20\\u00b11.52 90.51\\u00b13.81 100\\u00b10 0.3706\\u00b10.4077\\nSVDD 85.93\\u00b19.83 64.81\\u00b124.59 100\\u00b10 0.1809\\u00b10.1297\\n\\n432 normal records, we used 144 normal records (i.e. 1\\n\\nday normal traffic) for training purpose. The remaining\\n\\n288 normal records (i.e. 2 days normal traffic) was then\\n\\nused for testing purpose. We further validated proposed\\n\\nframework by rotating the training and testing data so\\n\\nas to use different training and testing data in each\\n\\niteration. In this way, we performed 3\\u2212fold validation.\\n\\nWe also collected 3 days of mixed DHCPv6 and NS\\n\\ntraffic generated by targeting one victim client only.\\n\\nThis traffic was also partitioned in time intervals of size\\n\\n\\u2206T = 10 minutes. As a result, we obtained 432 smaller\\n\\ntime intervals which was then pre-processed to form\\n\\n432 attack records. As a result, we obtained a total of\\n\\n720 records for testing purpose in which there were 288\\n\\nnormal and 432 attack records.\\n\\n6.6 Detection Performance in IPv6 Network\\n\\nThe range of parameters of various classifiers to eval-\\n\\nuate detection performance of framework in IPv6 net-\\n\\nwork is shown in Table 4. Tables 6 shows mean of var-\\n\\nious detection performance metrics along with devia-\\n\\ntions for different one-class classifiers in case of IPv6\\n\\nnetwork. These results were obtained after 3\\u2212fold cross\\n\\nvalidation. We can notice from Table 6 that K-means\\n\\nclustering and Gaussian Density based classifier gave\\n\\nbest results while SVDD again performed worst for de-\\n\\ntection purpose. Observed Precision for all the classi-\\n\\nfiers was 100%. We can also notice that the time taken\\n\\nto build the model was highest in case of Naive Bayes\\n\\nclassifier while it was lowest for KNN classifier.\\n\\n7 Prior Work and Comparison\\n\\nDifferent techniques have been proposed in the litera-\\n\\nture for the detection and mitigation of DHCP star-\\n\\nvation attacks. In this section, we first mention few\\n\\nwell-known schemes and then discuss their abilities to\\n\\ndetect/mitigate proposed stealth DHCP starvation at-\\n\\ntack.\\n\\n7.1 Detection and Mitigation Techniques\\n\\nPort Security: Port security [25] feature available in\\n\\nmodern switches puts a restriction on number of MAC\\n\\naddresses attached to a port of the network switch.\\n\\nOnce this limit is crossed, the port is disabled auto-\\n\\nmatically and an SNMP trap is generated. Since the\\n\\nproposed attack does not require MAC address spoof-\\n\\ning and the malicious client uses its own MAC address\\n\\nfor communication, port security can see only one MAC\\n\\naddress at the port. Thus, the proposed attack goes un-\\n\\ndetected by port security.\\n\\nDHCP Snooping: DHCP Snooping [10] notices\\n\\nthe difference in MAC address present in the Ether-\\n\\nnet header and CHADDR field of a DHCPDISCOVER\\n\\nmessage which helps in mitigating classical DHCP star-\\n\\nvation attack. However, proposed stealth DHCP star-\\n\\nvation attack does not manipulate the header of DHCP\\n\\nmessages and thus, it goes undetected by DHCP snoop-\\n\\ning also.\\n\\nDAI with DHCP Snooping: Dynamic ARP In-\\n\\nspection [1] determines the validity of an ARP mes-\\n\\nsage by checking valid IP-MAC bindings stored in\\n\\nDHCP snooping database. Since malicious client allo-\\n\\ncates IP manually to its interface without the inter-\\n\\nvention of DHCP server, snooping database does not\\n\\ncontain an IP-MAC binding corresponding to malicious\\n\\nclient. Moreover, the fake ARP requests sent by mali-\\n\\ncious client are having source IP \\u201c0.0.0.0\\u201d which is seen\\n\\nas a client has just joined the network and it is send-\\n\\ning ARP probes to check address conflict. Thus, the\\n\\nproposed attack is not detected by DAI also.\\n\\nDHCP Traffic Threshold based Mechanism:\\n\\nAuthors in [23] proposed a scheme that counts the\\n\\nnumber of DHCPREQUEST packets within a period\\n\\nof time. If this count crosses a predefined threshold,\\n\\nthe detection system raises the alarm. The proposed\\n\\nstealth DHCP starvation attack also forces victim client\\n\\nfor multiple DORA processes due to which several\\n\\nDHCPREQUEST messages are generated. However,\\n\\nthis number is quite less as compared to classical DHCP\\n\\nstarvation attack. As a result, it is difficult to detect\\n\\nproposed attack using this detection scheme.\\n\\n\\n\\n10 Please give a shorter version with: \\\\authorrunning and \\\\titlerunning prior to \\\\maketitle\\n\\nTable 6: Detection Performance of Various One-Class Classifiers in case of IPv6 Network\\n\\nClassifer Accuracy Recall Precision Time taken to build model (in seconds)\\nGaussian Density based Classifier 97.92\\u00b12.02 94.79\\u00b15.04 100\\u00b10 0.0553\\u00b10.0488\\nNaive Bayes Classifier 97.08\\u00b12.50 92.71\\u00b16.25 100\\u00b10 0.2954\\u00b10.2715\\nKNN Classifier 97.22\\u00b12.65 93.06\\u00b16.62 100\\u00b10 0.0431\\u00b10.0337\\nK-means Clustering 97.92\\u00b12.02 94.79\\u00b15.04 100\\u00b10 0.0474\\u00b10.0370\\nPCA 94.17\\u00b12.47 85.42\\u00b16.17 100\\u00b10 0.1853\\u00b10.1544\\nSVDD 84.49\\u00b111.75 61.23\\u00b129.38 100\\u00b10 0.1858\\u00b10.1284\\n\\nLimiting Number of IP Addresses on a\\n\\nSwitch Port: The mitigation approaches proposed in\\n\\n[26] and [27] involve limiting the number of IP addresses\\n\\nthat can be assigned on a switch port. However, since\\n\\nthe proposed attack does not require consuming IP ad-\\n\\ndresses by completing several DORA process, it can not\\n\\nbe prevented by these mitigation approaches.\\n\\nCryptographic Techniques: Due to absence of\\n\\nany in-built authentication scheme in DHCP protocol,\\n\\nstarvation attacks are very easy to launch. To mit-\\n\\nigate the attacks, various cryptography-based works\\n\\n[14\\u201316, 22, 24] are proposed in the literature to secure\\n\\nthe DHCP communication using authentication and en-\\n\\ncryption. These techniques can easily mitigate iden-\\n\\ntity spoofing based attacks including proposed stealth\\n\\nDHCP starvation attack. However, implementation of\\n\\nthese techniques in real network is a very cumbersome\\n\\nprocess. Some of these techniques which involve certifi-\\n\\ncate and key distribution, require intervention of net-\\n\\nwork administrators. This straightaway conflicts the\\n\\npurpose of using DHCP in the network. Moreover, other\\n\\nthird party modules like Authentication Server, Confir-\\n\\nmation Server and Detection Server add extra complex-\\n\\nity and significant traffic load. Due to these limitations,\\n\\ncryptographic techniques are rarely implemented in lo-\\n\\ncal networks.\\n\\n7.2 Comparison\\n\\nIn this subsection, we evaluate the detection/mitigation\\n\\nperformance of different countermeasures which can be\\n\\ndeployed to detect/mitigate proposed stealth DHCP\\n\\nstarvation attack.\\n\\nTesting Various Security Features:To test port\\n\\nsecurity, DHCP snooping and DAI, we created a\\n\\ntestbed setup similar to the one shown in Figure 8.\\n\\nThis setup was having a HP Aruba 2920 layer-3 man-\\n\\naged switch and two computers. One computer was\\n\\ndesignated as malicious client while other one as vic-\\n\\ntim client. Malicious client was using Ubuntu 16.04LTS\\n\\nwhile victim client was using Windows 7 operating sys-\\n\\ntem. Both malicious and victim clients were having 4\\n\\nGB of physical memory and Core i5 quad core proces-\\n\\nFig. 8: Testbed Setup for testing Security Features\\n\\nsor. The switch\\u2019s in-built DHCP server was enabled and\\n\\nconfigured with a pool of 256 IP addresses. Using this\\n\\nsetup, we tested the security features and the experi-\\n\\nmental observations are presented below.\\n\\nTesting Port Security: We enabled port security on\\n\\nthe switch port to which malicious client was connected\\n\\nby limiting the number of MAC addresses allowed at\\n\\nthat port. We also chose the security parameter that\\n\\nif security violation occurred, an SNMP trap must be\\n\\ngenerated and port must be disabled immediately. Af-\\n\\nter this, we released and then renewed the IP address\\n\\nlease of victim client by executing \\u201cipconfig /release\\u201d\\n\\nand \\u201cipconfig /renew\\u201d commands on victim client re-\\n\\nspectively. As soon as victim client performed conflict\\n\\nchecking after obtaining an IP address from in-built\\n\\nDHCP server of switch, malicious client sent fake ARP\\n\\nrequest. This ARP request was not dropped by switch\\n\\ndue to which attack was successful and victim client\\n\\nwas not able to configure its interface with IP address\\n\\neven after several attempts. Thus, port security was not\\n\\nable to mitigate the proposed attack.\\n\\nTesting DHCP Snooping: We enabled DHCP snoop-\\n\\ning globally on the switch and designated the in-built\\n\\nDHCP server of switch as an authorized DHCP server.\\n\\nAfter this, we released and renewed the IP address lease\\n\\nof victim client. This lead to conflict checking by victim\\n\\nclient. Again, malicious client sent fake ARP request in\\n\\nresponse to the ARP request sent by victim client. Since\\n\\nDHCP snooping did not drop the fake ARP request sent\\n\\nby malicious client, proposed attack was successful due\\n\\nto which victim client was not able to configure its in-\\n\\nterface with an IP address.\\n\\nTesting DAI with DHCP Snooping: We further\\n\\ntested the DAI security feature on the switch by first\\n\\nconfiguring switch ports as trusted because ARP mes-\\n\\n\\n\\nDetecting Stealth DHCP Starvation Attack using Machine Learning Approach 11\\n\\nsages received on untrusted ports were not forwarded\\n\\nin the network due to which victim client could not\\n\\nperform IP conflict checking. DHCP snooping was also\\n\\nenabled along with DAI as DAI used the trusted DHCP\\n\\nsnooping database to check if IP-MAC bindings present\\n\\nin ARP messages were genuine. We then released and\\n\\nrenewed the IP address lease of victim client and\\n\\nlaunched the attack from malicious client by sending\\n\\nfake ARP request in response to ARP request sent by\\n\\nvictim client for conflict checking. Again, the attack\\n\\nwas found to be successful as DAI did not drop the\\n\\nfake ARP request sent by malicious client. Thus, pro-\\n\\nposed stealth DHCP starvation attack was found to be\\n\\neffective even if DAI was enabled on the switch.\\n\\nDetection Performance of DHCPREQUEST\\n\\nThreshold based Mechanism [23]: We evaluated\\n\\nthe detection performance of the DHCPREQUEST\\n\\nthreshold based mechanism [23] in both IPv4 and IPv6\\n\\nnetworks. The obtained results are given below.\\n\\nDetection Performance in IPv4 Network: To eval-\\n\\nuate the detection performance of DHCPREQUEST\\n\\nthreshold based mechanism in IPv4 network, we used\\n\\nthe same 3 days of normal DHCP and ARP dataset\\n\\n(discussed in Section 6.3) which was used to eval-\\n\\nuate the detection performance of our proposed de-\\n\\ntection scheme. From this dataset, we calculated the\\n\\nmean number of DHCPREQUEST messages received\\n\\nin different time intervals of a day where each time\\n\\ninterval was of 10 minutes. After this, we compared\\n\\nthe number of DHCPREQUEST messages received in\\n\\neach time interval of each day with mean number of\\n\\nDHCPREQUEST messages received in that time in-\\n\\nterval. For example, number of DHCPREQUEST mes-\\nsages, nd, received in time interval 12:00am-12:10am\\n\\nof a day d was compared with the mean number of\\n\\nDHCPREQUEST messages, nm received in time inter-\\n\\nval 12:00am-12:10am. If the difference nm\\u2212nd crossed a\\n\\npredefined threshold, anomaly was detected in that in-\\n\\nterval. From our experiments, we noticed that, in pres-\\n\\nence of stealth DHCP starvation attack, victim client\\n\\ntried 53 times in a time interval of 10 minutes to ac-\\n\\nquire an IP address. This resulted into generation of\\n\\n53 extra DHCPREQUEST messages. If the number of\\n\\nvictim clients which were to be targeted increased, the\\n\\ngenerated number of DHCPREQUEST messages also\\n\\nincreased. Thus, we considered 53 as the threshold value\\n\\nfor the difference nm \\u2212 nd. Since we took time interval\\n\\nsize of 10 minutes, there were 144 time intervals in one\\n\\nday. The dataset was having 3 days DHCP traffic thus\\n\\nthere were total 144 \\u2217 3 = 432 intervals that were to\\n\\nbe tested. Out of these 432 intervals, we injected at-\\n\\ntack traffic in 258 time intervals by targeting only one\\n\\nvictim client. Thus, remaining 174 intervals contained\\n\\nonly normal DHCP traffic. First row of Table 7 shows\\n\\nthe obtained detection accuracy of this approach while\\n\\ndetecting the proposed stealth DHCP starvation attack\\n\\nin IPv4 network. We can notice that putting a threshold\\n\\nlimit on a DHCP message type is not a sound approach\\n\\nas we achieved only 65.28% Accuracy.\\n\\nDetection Performance in IPv6 Network: To eval-\\n\\nuate the detection performance of DHCPREQUEST\\n\\nthreshold based mechanism in IPv6 network, we used\\n\\nthe same 3 days of normal DHCPv6 and NS dataset\\n\\n(discussed in Section 6.5) which was used to evaluate\\n\\nthe detection performance of our proposed detection\\n\\nscheme. In IPv6 network also, we calculated the mean\\n\\nnumber of REQUEST messages received in different\\n\\ntime intervals of a day where each time interval was\\n\\nof 10 minutes. After this, we compared the number of\\n\\nREQUEST messages received in each time interval of\\n\\neach day with mean number of REQUEST messages re-\\n\\nceived in that time interval. In case of IPv6 network, we\\n\\nconsidered 15 as the threshold value for the difference\\n\\nnm\\u2212nd as victim client tried 15 times in a time interval\\n\\nof 10 minutes to acquire an IP address from a DHCPv6\\n\\nserver. In case of IPv6 network also, we injected at-\\n\\ntack traffic in 258 time intervals out of 432 intervals\\n\\nby targeting only one victim client whereas remaining\\n\\n174 intervals contained only normal DHCPv6 traffic.\\n\\nSecond row of Table 7 shows the obtained detection ac-\\n\\ncuracy of this approach while detecting the proposed\\n\\nstealth DHCP starvation attack in IPv6 network. We\\n\\ncan notice that this approach could detect the proposed\\n\\nattack with only 69.44% Accuracy.\\n\\nMitigation approaches which involve limiting num-\\n\\nber of IP addresses on a switch port can not mitigate\\n\\nthe proposed attack as it does not require consuming IP\\n\\naddresses by completing several DORA process. Cryp-\\n\\ntographic techniques can mitigate all identity spoofing\\n\\nbased attacks including the proposed stealth DHCP\\n\\nstarvation attack. However, due to their well-known is-\\n\\nsues like high implementation and computational com-\\n\\nplexity and requirement of third party modules like Au-\\n\\nthentication Servers, we did not test these techniques\\n\\nin our experiments.\\n\\n8 Conclusion\\n\\nIn this paper, we proposed a stealth DHCP starva-\\n\\ntion attack that is difficult to detect by known detec-\\n\\ntion mechanisms. The proposed attack is effective and\\n\\nstealthier in both IPv4 and IPv6 networks independent\\n\\nof wired and/or wireless topology. We showed that the\\n\\nattack can easily bypass known detection/mitigation\\n\\nmechanisms and prevent clients from acquiring IP ad-\\n\\ndress. We also proposed a ML based anomaly detection\\n\\n\\n\\n12 Please give a shorter version with: \\\\authorrunning and \\\\titlerunning prior to \\\\maketitle\\n\\nTable 7: Detection Performance of DHCPREQUEST Threshold based Mechanism\\n\\nNetwork Accuracy Recall Precision\\nIPv4 65.28 54.26 81.40\\nIPv6 69.44 56.20 88.42\\n\\nframework to detect the proposed attack. The frame-\\n\\nwork uses one-class classifiers in order to classify the\\n\\ntraffic within different time intervals. We tested the\\n\\ndetection performance of proposed framework in both\\n\\nIPv4 and IPv6 networks using traffic captured from\\n\\na real network that connected thousands of heteroge-\\n\\nneous devices and showed that the framework can de-\\n\\ntect the proposed attack with very high accuracy.\\n\\nWe believe that this work will motivate researchers\\n\\nin the networking community to further assess DHCP\\n\\nprotocol as it may lead to finding new vulnerabilities\\n\\nand thus, mitigating any resulting threat vectors with\\n\\nrobust detection/mitigation mechanisms.\\n\\nReferences\\n\\n1. Dynamic ARP Inspection.\\nhttp://www.cisco.com/c/en/us/td/docs/switches/lan/\\ncatalyst6500/ios/12-2SX/configuration/guide/book/\\ndynarp.html. Accessed: 2017-09-23\\n\\n2. Droms, R.: RFC2131: Dynamic Host Configuration Pro-\\ntocol. Internet Engineering Task Force (1997).\\n\\n3. Droms, R., Bound, J., Volz, B., Lemon, T., Perkins, C.,\\nCarney, M.: RFC3315: Dynamic Host Configuration Pro-\\ntocol for IPv6 (DHCPv6). Internet Engineering Task Force\\n(2003).\\n\\n4. Gobbler. http://gobbler.sourceforge.net/. Accessed: 2017-\\n09-23\\n\\n5. DHCPIG. https://github.com/kamorin/DHCPig. Ac-\\ncessed: 2017-09-23\\n\\n6. Tripathi, N., Hubballi, N.: Exploiting DHCP Server-side\\nIP Address Conflict Detection: A DHCP Starvation Attack.\\nIn: International Conference on Advanced Networks and\\nTelecommunication Systems (ANTS), pp. 1-3, (2015).\\n\\n7. Aburomman, A.A., Reaz, M.B.I.: A Survey of Intrusion\\nDetection Systems based on Ensemble and Hybrid Classi-\\nfiers, In Computers & Security, Volume 65, Pages 135-152,\\n(2017).\\n\\n8. Al-Yaseen, W.L., Othman, Z.A., Nazri, Z.A.: Multi-level\\nHybrid Support Vector Machine and Extreme Learning Ma-\\nchine based on Modified K-means for Intrusion Detection\\nSystem, In Expert Systems with Applications, Volume 67,\\nPages 296-303, (2017).\\n\\n9. Liu, L., Zuo, W.L., Peng, T.: Detecting Outlier Pairs in\\nComplex Network based on Link Structure and Semantic\\nRelationship, In Expert Systems with Applications, Volume\\n69, Pages 40-49, (2017).\\n\\n10. DHCP Snooping. http://www.cisco.com/c/en/us/td/\\ndocs/switches/lan/catalyst6500/ios/12-\\n2SX/configuration/guide/book/snoodhcp.html. Accessed:\\n2017-09-23\\n\\n11. Xing, X., Shakshuki, E., Benoit, D., Sheltami, T.: Se-\\ncurity Analysis and Authentication Improvement for IEEE\\n802.11i Specification. In: Global Telecommunications Con-\\nference (GLOBECOM), pp. 1-5, (2008).\\n\\n12. JNetPcap. http://jnetpcap.com/docs/javadocs/jnetpcap-\\n1.4/index.html. Accessed: 2017-09-23\\n\\n13. Scapy. http://www.secdev.org/projects/scapy/. Ac-\\ncessed: 2017-09-23\\n\\n14. Issac, B.: Secure ARP and Secure DHCP Protocols to\\nMitigate Security Attacks. International Journal of Net-\\nwork Security, 8(2), pp. 107-118, (2009).\\n\\n15. Droms, R., Arbaugh, W.: RFC3118: Authentication for\\nDHCP Messages. Internet Engineering Task Force (2001).\\n\\n16. Jerschow, Y. I., Lochert, C., Scheuermann, B., Mauve,\\nM.: CLL: A Cryptographic Link Layer for Local Area Net-\\nworks. In: International Conference on Security and Cryp-\\ntography for Networks (SCN), pp. 21-38, (2008).\\n\\n17. Hubballi, N., Tripathi, N.: A Closer Look into DHCP\\nStarvation Attack in Wireless Networks, In Computers &\\nSecurity, Volume 65, Pages 387-404, (2017).\\n\\n18. Bishop, C., M.: Neural Networks for Pattern Recognition.\\nOxford University Press, Inc., (1995).\\n\\n19. Chien, Y.: Pattern Classification and Scene Analysis, In\\nIEEE Transactions on Automatic Control, 19(4), pp. 462-\\n463, (1974)\\n\\n20. Friedman, N., Geiger, D., Goldszmidt, M.: Bayesian Net-\\nwork Classifiers. Machine learning, 29(2-3), 131-163, (1997).\\n\\n21. Martinus, D., Tax, J.: One-class Classification: Concept-\\nlearning in the Absence of Counterexamples, PhD Thesis,\\nDelft University of Technology, (2001).\\n\\n22. Demerjian, J., Serhrouchni, A.: DHCP Authentication\\nusing Certificates. In: Security and Protection in Informa-\\ntion Processing Systems, Springer US, pp. 456-472, (2004).\\n\\n23. OConnor, T.: Detecting and Responding to Data\\nLink Layer Attacks. http://www.sans.org/reading-\\nroom/whitepapers/intrusion/detecting-responding-data-\\nlink-layer-attacks-33513. Accessed: 2017-09-23.\\n\\n24. de Graaf, K., Liddy, J., Raison, P., Scano, J., Wadhwa, S.:\\nDynamic Host Configuration Protocol (DHCP) Authenti-\\ncation using Challenge Handshake Authentication Protocol\\n(CHAP) Challenge. US Patent 8,555,347, (2013).\\n\\n25. Port Security. http://www.cisco.com/c/en/us/td/docs/\\nswitches/lan/catalyst6500/ios/12-\\n2SX/configuration/guide/book/port sec.html. Accessed:\\n2017-09-23\\n\\n26. Patrick, M.: RFC3046: DHCP Relay Agent Information\\nOption. Internet Engineering Task Force (2001).\\n\\n27. Mukhtar, H., Salah, K., Iraqi, Y.: Mitigation of DHCP\\nStarvation Attack. Computers & Electrical Engineering,\\n38(5), pp. 1115-1128, (2012).\\n\\n28. Tax, D.M.J., Muller, K.R.: A Consistency-based Model\\nSelection for One-class Classification,\\u201d In: International\\nConference on Pattern Recognition (ICPR), pp. 363-366,\\n(2004).\\n\\n29. Tax, D.M.J.: DDtools, the Data Description Toolbox for\\nMatlab, version 2.1.2, (2015).\\n\\nView publication stats\\n\\nhttps://www.researchgate.net/publication/320558117\\n\\n\",\n          \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSee discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/325468290\\n\\nTowards a Better Understanding of Chess Players\\u2019 Personalities: A Study Using\\n\\nVirtual Chess Players\\n\\nChapter \\u00b7 June 2018\\n\\nDOI: 10.1007/978-3-319-91250-9_34\\n\\nCITATIONS\\n\\n17\\nREADS\\n\\n11,568\\n\\nSome of the authors of this publication are also working on these related projects:\\n\\nAnt colonies for image analysis and representation View project\\n\\nVirtual humans to explore personalities View project\\n\\nAll content following this page was uploaded by Khaldoon Dhou on 16 July 2018.\\n\\nThe user has requested enhancement of the downloaded file.\\n\\nhttps://www.researchgate.net/publication/325468290_Towards_a_Better_Understanding_of_Chess_Players%27_Personalities_A_Study_Using_Virtual_Chess_Players?enrichId=rgreq-be4fcd8ba0be9f2e68cc7f76c9ccb2fe-XXX&enrichSource=Y292ZXJQYWdlOzMyNTQ2ODI5MDtBUzo2NDkwODAxOTE2MDY3ODRAMTUzMTc2NDE4NTIxMQ%3D%3D&el=1_x_2&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/publication/325468290_Towards_a_Better_Understanding_of_Chess_Players%27_Personalities_A_Study_Using_Virtual_Chess_Players?enrichId=rgreq-be4fcd8ba0be9f2e68cc7f76c9ccb2fe-XXX&enrichSource=Y292ZXJQYWdlOzMyNTQ2ODI5MDtBUzo2NDkwODAxOTE2MDY3ODRAMTUzMTc2NDE4NTIxMQ%3D%3D&el=1_x_3&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/project/Ant-colonies-for-image-analysis-and-representation?enrichId=rgreq-be4fcd8ba0be9f2e68cc7f76c9ccb2fe-XXX&enrichSource=Y292ZXJQYWdlOzMyNTQ2ODI5MDtBUzo2NDkwODAxOTE2MDY3ODRAMTUzMTc2NDE4NTIxMQ%3D%3D&el=1_x_9&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/project/Virtual-humans-to-explore-personalities?enrichId=rgreq-be4fcd8ba0be9f2e68cc7f76c9ccb2fe-XXX&enrichSource=Y292ZXJQYWdlOzMyNTQ2ODI5MDtBUzo2NDkwODAxOTE2MDY3ODRAMTUzMTc2NDE4NTIxMQ%3D%3D&el=1_x_9&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/?enrichId=rgreq-be4fcd8ba0be9f2e68cc7f76c9ccb2fe-XXX&enrichSource=Y292ZXJQYWdlOzMyNTQ2ODI5MDtBUzo2NDkwODAxOTE2MDY3ODRAMTUzMTc2NDE4NTIxMQ%3D%3D&el=1_x_1&_esc=publicationCoverPdf\\nhttps://www.researchgate.net/profile/Khaldoon_Dhou?enrichId=rgreq-be4fcd8ba0be9f2e68cc7f76c9ccb2fe-XXX&enrichSource=Y292ZXJQYWdlOzMyNTQ2ODI5MDtBUzo2NDkwODAxOTE2MDY3ODRAMTUzMTc2NDE4NTIxMQ%3D%3D&el=1_x_10&_esc=publicationCoverPdf\\n\\n\\nTowards a Better Understanding of Chess\\nPlayers\\u2019 Personalities: A Study Using Virtual\\n\\nChess Players\\n\\nKhaldoon Dhou\\n\\nDepartment of Mathematics and Computer Science, University of Missouri \\u2013 St. Louis\\n\\nAbstract. Virtual humans emerged as a topic of research in HCI and\\nthey have been used for various purposes. This paper explores the be-\\nhavior of chess players in a virtual chess environment to gain more un-\\nderstanding about chess personalities. In particular, the focus of this\\nresearch is investigating attack and defense strategies used by virtual\\nchess grandmasters against different virtual class-B personalities who\\nvary in their strength in the different stages of a game. These attack\\nand defense strategies have attracted much attention in the chess com-\\nmunity and are considered among the main aspects to chess players.\\nThey occur in different phases of the game: opening, middle game and\\nendgame. The researcher examines virtual chess players to understand\\nthe psychology of competition between two grandmasters (attacker, de-\\nfender) and three class-B chess players with different personalities: (a)\\nstrong at openings; (b) weak at openings, but strong at endgames and\\n(c) balanced player. The virtual humans in this research represent per-\\nsonalities of real players. The empirical players\\u2019 results showed that the\\npersonalities could influence the error and the number of moves of the\\ngame for both grandmasters and class-B players. Such findings can be\\nused in designing virtual chess players.\\n\\nKeywords: virtual humans, chess, attack, defense, personality\\n\\n1 Introduction\\n\\nComputer chess has grown very rapidly in the last 30 years. This was the result\\nof many achievements which demonstrated how computers could change the\\ngame of chess including a computer being able to defeat the world champion,\\nGary Kasparov [21]. One of the most significant achievements in the AI chess\\nis the development of virtual chess players. These are imaginary players with\\nvarious personalities associated with chess (i.e. attacker and defender). They are\\nintroduced by companies such as Titus and Ubisoft [4, 30]. Such virtual humans\\nrepresent real chess players such as Kasparov and Anand and play at almost the\\nsame level of strength.\\n\\nIn this research study, chess personality is defined as the attitude of the\\nvirtual chess player during the chess game such as attack or defense. This can\\nreflect the player\\u2019s strength and weakness during certain phases of the game (i.e.\\n\\n\\n\\n2 Khaldoon Dhou\\n\\nopening, middle and endgame). Different chess players have different attitudes\\ntowards the opponent which shape their personalities during the game.\\n\\nExploring chess personalities is important for many reasons. First, it can help\\nchess developers design chess programs which simulate humans. Second, it can\\nhelp in understanding the behavior of human players by identifying the reasons\\non why players lose games which can be a challenging task [29]. Interestingly,\\nunderstanding chess personalities can pave the way for a new direction of research\\nin the medical field. Research shows that the Chessmaster software improved\\nsurgical training undertaken in a virtual environment [26].\\n\\nEach virtual player has a rating, which is a number that measures the relative\\nskill of the player in his games against other players in the chess community [11];\\nthe higher the rating, the stronger the chess player. Organizations such as United\\nStates Chess Federation (USCF) and the World Chess Federation provide these\\nratings to chess players. Players can be distributed into different classes based\\non their rating. For example, in the USCF System, a senior master has a rating\\nof 2400 and above; a master\\u2019s rating ranges from 2200 to 2399; expert from 2000\\nto 2199; class A from 1800 to 1999; class B from 1600 to 1799 and class C from\\n1400 to 1599 [12].\\n\\nIn this paper, five Ubisoft virtual humans are examined: two grandmasters\\nand three class-B players [30]. These players vary in their personalities. Two\\nchess grandmaster personalities were selected: Supreme Defender and and Early\\nAttacker against three class-B player personalities: A personality that is strong\\nat the endgame, but plays weak openings; A personality with strong openings;\\nand a balanced player. This research explores the following questions:\\n\\n\\u2013 How do virtual grandmasters with identical ratings and different personali-\\nties (attacking and defensive) perform while playing against various chess-B\\nplayers with different strengths at each phase of the game?\\n\\n\\u2013 How do virtual class-B chess players of different personalities reflected in\\ndifferent phases throughout the game, behave whilst playing against virtual\\ngrandmasters with different personalities (i.e. attacker and defender)?\\n\\n\\u2013 Do virtual grandmasters make more errors when they play against a virtual\\nclass-B player who is strong at the opening as opposed to a class-B player\\nwho is strong at the endgame and plays vulnerable openings?\\n\\n2 Related work\\n\\nChess has been an attractive domain for researchers of different fields. An early\\nwork aiming to understand the psychology of chess players was done by de Groot,\\na psychologist who studied chess players of various skills [9]. de Groot constructed\\nmeaningful chess patterns and then asked chess players of various skills to re-\\nconstruct them from memory. He found that chess masters outperformed novice\\nplayers. A follow up was done by Chase and Simon [6] who studied chess players\\nof different strengths and found that masters learn more than novice players. The\\ntwo studies show that chess masters are able to detect patterns of pieces over the\\n\\n\\n\\nHow can virtual players help us understand the outcome of chess games? 3\\n\\nchess board, which allows them to play a stronger game using memorized moves\\nin a short amount of time as opposed to novice players. These classical studies\\nwere subjected to further research and development in many perspectives. Pow-\\nell et al. [22] utilized fMRI in male beginner chess players to locate the cortical\\nparts in the brain related to chess. Similarly, Burgoyne et al. [3] investigated\\nthe connection between chess and cognitive skills. Their findings revealed a pos-\\nitive correlation between chess and fluid reasoning, comprehension-knowledge,\\nshort-term memory and processing speed.\\n\\nChess rating is used as a metric to measure the strength of each player against\\nothers in the community. Players can be classified into different categories (i.e.\\ngrandmasters, class-A players) according to their rating. A considerable amount\\nof classical literature has been published on chess ratings and class-B players.\\nFor example, Calderwood et al. [5] investigated the influence of time pressure on\\nclass-B players and grandmasters. Their research supports the classical findings\\nthat players with high rating depend on pattern recognition more than less\\nskilled players. Similarly, Goldin [15] investigated chess players of various classes\\nand their findings were consistent with those of Calderwood et al. [5]. Likewise,\\nSheridan and Reingold [27] investigated eye movements of chess players that vary\\nin their skill and found that reaction times were faster for experts than less skilled\\nplayers. In a different vein, Lane and Chang [18] conducted tests involving chess\\npositions and their findings support the evidence of the significance of pattern\\nrecognition in making strong moves among skilled chess players.\\n\\nChess players of various levels (i.e. grandmasters and class-B players) vary in\\ntheir personalities. For example, some players might have an attacking attitude,\\nwhile others might be more defensive and some could be a mixture of both. These\\nattitudes can be clearly seen in different phases of the game. Various studies in\\nthe literature of artificial intelligence and cognitive psychology investigated these\\nattitudes. For example, Botvinnik [2] explored attack and defense in solving\\nchess tasks and calculated a function for an attack path. Attack and defense\\nare also among the most important characteristics of chess personalities. These\\npersonalities have not only attracted the attention of researchers from the AI\\ndomain, but they are also of great interest to cognitive psychologists. An example\\nis the study by Saariluoma [25] who asked players to detect quickly whether the\\nking was attacked or not. He found that skilled players were faster than others\\nwho were less skilled in completing the task. Chess pieces are essential pieces\\nof attack and defense in chess and have also been explored in the literature by\\nRasskin-Gutman [23].\\n\\nVirtual chess players have been examined in research through different per-\\nspectives. Kova\\u0301cs et al [17] developed a virtual chess player which applies differ-\\nent styles in communication such as perceiving the emotions and talking with the\\nopponent. While developing a virtual human is a commendable work, their study\\ndid not provide any quantitative analysis of its performance. Along with virtual\\nchess players, the extensive research revealed existing literature in the area of\\nvirtual humans and their applications in different fields. One example is SimSen-\\nsei Kiosk, which acts like a human nurse and communicates with patients [10].\\n\\n\\n\\n4 Khaldoon Dhou\\n\\nSimilarly, Kenny et al. [16] applied virtual humans in diagnosis and training.\\nThese virtual humans used in the medical field can have many advantages. For\\ninstance, a person can disclose more information to an agent than what they\\nprovide to a human [20, 24, 28]. Virtual humans behave to a significant extent\\nlike real human beings [28].\\n\\nAlthough there has been existing research on virtual humans in chess and\\nother domains, the extensive literature review did not reveal any study that\\ninvolved analyzing the personalities of virtual chess players and the differences\\nbetween them when they play against each other. In other words, all the pre-\\nvious studies did not explore the error rates of different chess personalities and\\nhow they perform while playing against other chess personalities. The extensive\\nliterature review revealed that this is the first HCI study that analyzes chess\\npersonalities of virtual chess players. Chess players\\u2019 personalities may have an\\nimpact on the chess game, thus they need to be explored further. In addition,\\nthey might help researchers further understand the behavior of people in real\\nlife settings.\\n\\n3 Experiment\\n\\nThe focus of this experiment is to understand the relationship between errors,\\nnumber of moves and personalities of different virtual humans. The goals of this\\nexperiment are:\\n\\n\\u2013 Explore the errors made by virtual grandmasters (attacking, defensive) while\\nplaying against virtual class-B players with various strengths at different\\nphases of the game.\\n\\n\\u2013 Examine the errors made by virtual class-B players with different strengths\\nin the game phases while playing against virtual grandmasters of different\\npersonalities.\\n\\n\\u2013 Investigate the relationship between the numbers of moves in a game and\\nthe two virtual chess personalities which compete against each other\\n\\n3.1 Participants\\n\\nFor this experiment, five virtual chess players offered by Ubisoft [30] were se-\\nlected. These players come with different personalities. In this study, there are\\ntwo grandmaster personalities: Anderssen (Early Attacker) and Leko (Supreme\\nDefense). The three class-B player personalities the researcher picked are: Josh\\n(vulnerable openings, but strong endgame), John (strong openings), and Kanna\\n(Balanced player).\\n\\n3.2 Materials\\n\\nThe design involves the manipulation of two independent variables as follows:\\n\\n\\n\\nHow can virtual players help us understand the outcome of chess games? 5\\n\\n\\u2013 The personality of the grandmasters was manipulated. Half of the games\\nin this experiment were played by a grandmaster with an early attacking\\npersonality (Anderssen) and the other half were played by a grandmaster\\nwith a supreme defensive personality (Leko). The two virtual grandmasters\\nhave the same USCF rating of 2971.\\n\\n\\u2013 The class-B player personality was manipulated according to the strength\\nhe demonstrated in different game phases. Three virtual personalities were\\nselected: Waitzkin who is strong in endgame but weak at openings, John\\nwho plays strong openings, and Kanna who is a balanced player. The USCF\\nratings of the three players are almost identical: Josh\\u2019s USCF rating is 1600,\\nJohn\\u2019s rating is 1631, and Kanna\\u2019s rating is 1623.\\n\\nThe color of the grandmaster was used as between-subjects factor. In this\\nexperiment, there are five dependent variables measured by the Chessmaster [30]:\\n\\n\\u2013 The total error of the virtual grandmaster\\n\\u2013 The total error of the virtual class-B player\\n\\u2013 The relevant error of the virtual grandmaster\\n\\u2013 The relevant error of the virtual class-B player\\n\\u2013 The number of moves in the game\\n\\nIn this research, the move is defined as the white player\\u2019s move followed by\\nthe black player\\u2019s move. The metrics total and relevant errors [7] are calculated\\nby the Chessmaster offered by Ubisoft [30]. The total error is defined as the\\ndifference between the moves players choose and the optimal ones they could\\nmake. The relevant error is defined in the same way, but it focuses on the moves\\nwhere the outcome of the game is still not clear yet. The Chessmaster did not\\nprovide any details on how the total and relevant errors are calculated and the\\ntwo definitions were obtained from [7].\\n\\nThe criteria for choosing the virtual chess players in this research are the\\nfollowing:\\n\\n\\u2013 All virtual grandmasters have the same rating. They only differ in person-\\nalities.\\n\\n\\u2013 The virtual class-B players have almost an identical rating. However, they\\nvary in their strength in different game phases.\\n\\n3.3 Procedure\\n\\nEach grandmaster played 102 games against each class-B player. In order to\\nreduce the chance that a player\\u2019s color influences the design, each player plays\\nhalf of the games in the experiment with white color and the other half with\\nblack.\\n\\n4 Results\\n\\nThe dependent variables were recorded for each game in the experiment. Each\\ndependent variable was submitted to two grandmaster personality styles (Leko\\n\\n\\n\\n6 Khaldoon Dhou\\n\\nand Anderssen) by 3 class-B player personality styles (Josh, John and Kanna)\\nrepeated measures ANOVA. The color of the grandmaster was used as between-\\nsubjects factor. All effects were reported as significant at p < 0.05. Mauchly\\u2019s test\\nwas used for sphericity testing and Greenhouse-Geisser correction was applied if\\nthe assumption of sphericity was violated.\\n\\n4.1 The total error of the virtual grandmaster\\n\\nThe main effect of the grandmaster\\u2019s personality is significant, F (1, 100) =\\n13.975, p < 0.001, indicating that the total errors of Leko (M = 1.874, SD =\\n0.119) are larger than the total errors of Anderssen (M = 0.954, SD = 0.119).\\nMoreover, there is a significant main effect of the class-B player\\u2019s personal-\\nity, F (1.809, 180.910) = 5.338, p = 0.007. Contrasts reveal that the total er-\\nrors of grandmasters when playing against John (M = 2.010, SD = 0.289)\\nare larger than the total errors of grandmasters when playing against Josh\\n(M = 1.102, SD = 0.170), F (1, 100) = 7.826, p = 0.006 and when playing\\nagainst Kanna (M = 1.117, SD = 0.187), F (1, 100) = 6.251, p = 0.014.\\n\\n4.2 The total error of the virtual class-B player\\n\\nThe main effect of the grandmaster\\u2019s personality is significant, F (1, 100) =\\n74.734, p < 0.001. This indicates that the total errors of the three class-B players\\nagainst a grandmaster with a defensive style (M = 11.885) are larger than the\\ntotal errors of the class-B players against a grandmaster with an attacking style\\n(M = 8.534). In addition, the main effect of the class-B player\\u2019s personality\\nis significant, F (1.873, 187.281) = 12.356, p < 0.001. Contrasts reveal that the\\ntotal errors of Kanna (M = 11.738, SD = 0.384) are larger that the total errors\\nof Josh (M = 9.189, SD = 0.303), F (1, 100) = 26.911, p < 0.001 and the total\\nerrors of John (M = 9.701, SD = 0.434), F (1, 100) = 11.248, p = 0.001.\\n\\nFigure 1 shows the mean total error of grandmasters and class-B players while\\nplaying against each other. A defensive grandmaster makes more mistakes than\\nan attacking grandmaster. Furthermore, a balanced player makes more mistakes\\nthan the two players who are strong in opening and endgame.\\n\\n4.3 The relevant error of the virtual grandmaster\\n\\nThe main effect of the grandmaster\\u2019s personality on the relevant error of grand-\\nmasters is significant, F (1, 100) = 13.958, p < 0.001. This indicates that the\\nrelevant errors of Leko (M = 0.316, SD = 0.059) are larger than the relevant\\nerrors of Anderssen (M = 0.069, SD = 0.028).\\n\\n4.4 The relevant error of the class-B player\\n\\nThe main effect of the grandmaster\\u2019s personality is significant, F (1, 100) =\\n30.177, p < 0.001. This indicates that the relevant errors of class-B players\\n\\n\\n\\nHow can virtual players help us understand the outcome of chess games? 7\\n\\nFig. 1. Mean total error of virtual class-B players and grandmasters\\n\\nplaying against Leko (M = 3.390, SD = 0.115) are larger than the relevant\\nerrors of class-B players playing against Anderssen (M = 2.543, SD = 0.096).\\nFurthermore, the main effect of the class-B player\\u2019s personality is significant,\\nF (2, 200) = 16.104, p < 0.001. Contrasts revealed that the relevant errors made\\nby Kanna (M = 3.581, SD = 0.110) are larger than the relevant errors made\\nby Josh (M = 2.6, SD = 0.13), F (1, 100) = 31.166, p < 0.001 and John\\n(M = 2.717, SD = 0.15), F (1, 100) = 19.862, p < 0.001. Figure 2 shows the\\nmean relevant error of virtual class-B players and grandmasters in this experi-\\nment.\\n\\n4.5 The number of moves in the game\\n\\nThe main effect of the grandmaster\\u2019s personality is significant, F (1, 100) =\\n12.740, p = 0.001. This indicates that the number of moves played by a grand-\\nmaster with an attacking style (M = 45.516, SD = 0.601) is higher than\\nthe number of moves played by a grandmaster with a defensive style (M =\\n42.729, SD = 0.0.538). The main effect of the class-B player\\u2019s personality is\\nalso significant, F (1.676, 167.627) = 30.361, p < 0.001. Contrasts reveal that\\nthe number of moves in games played by grandmasters against Josh (M =\\n46.368, SD = 0.639) or John (M = 46.093, SD = 0.882) is larger than the num-\\nber of moves in games played by grandmasters against Kanna (M = 39.907, SD =\\n\\n\\n\\n8 Khaldoon Dhou\\n\\nFig. 2. Mean relevant error of virtual class-B players and grandmasters\\n\\n0.459). Figure 3 shows the mean number of moves in games between virtual\\ngrandmasters and class-B players where more moves are associated with an at-\\ntacking grandmaster. Additionally, games against Kanna have the least number\\nof moves.\\n\\n5 Discussion\\n\\nThe empirical results showed that a grandmaster with a defensive style tends\\nto have a higher error than a grandmaster with an attacking style when playing\\nagainst players with different strengths in the various game stages (Figures 1\\nand 2). This is probably because an attacker grandmaster has built-in attacking\\ntraps and can take control of the game, which makes him less subject to errors\\nthan a grandmaster who plays defensively and must respond to the attacks from\\nan opponent.\\n\\nIn addition, this study explores the performance of three class-B players\\nwho differ in their strength during the different game phases (i.e. player who\\nplays strong endgames and weak openings as opposed to a player who plays\\nstrong openings and weak endgames). The findings revealed that the players\\u2019\\nstrength in different phases influenced the total and relevant errors of class-B\\nchess players during their games against attacking and defensive grandmasters.\\n\\n\\n\\nHow can virtual players help us understand the outcome of chess games? 9\\n\\nFig. 3. Mean number of moves\\n\\nThe results showed that a class-B chess player who is strong at either the opening\\nor endgame performs better than a player who is balanced when playing against\\na grandmaster with attacking or defensive style. For example, Josh who is very\\nsharp at the endgame and John who had strong openings performed better than\\nKanna despite that the three players have almost the same rating. These findings\\nshow the importance of studying chess openings and endgames.\\n\\nThe grandmaster personality had an effect on the class-B players at different\\nstages of the game: The errors of the class-B players were always higher when\\nthey played against a grandmaster with a defensive style. A plausible psycholog-\\nical explanation is that people naturally try to understand the cause of difficult\\nevents and how they can affect their lives when they encounter them [31, p. 285].\\nThus, an initial attack during the game probably makes class-B players try to\\nunderstand the scenarios deeper at an early stage than when they are playing\\nagainst a defensive grandmaster who does not initiate an attack.\\n\\nAs for class-B players, the player who was balanced during all stages of the\\ngame had a higher error than a chess player who was sharp at either the opening\\nor the endgame stage. On average, grandmasters could beat a class-B player with\\na balanced style with less number of moves as compared to a player who is strong\\nat either opening or endgame. This shows the importance of the opening training\\nin the game, which is also supported by other researchers in the chess literature\\n\\n\\n\\n10 Khaldoon Dhou\\n\\n[13, 19]. Similarly, the results indicate that the endgame is as important as the\\nopening phase to the game\\u2019s outcome [13].\\n\\nAs for the two groups of players, grandmasters performed better than class-\\nB players and they made less errors. This matches with the findings of Cowley\\nand Byrne [8] that chessmasters tend to evaluate their moves more realistically\\nthan novice players. This can be explained by the Chunking Model, according to\\nwhich the skill of the chess player is based on two factors: the ability to search\\nthe tree of moves for a strong move (i.e. success is rated against the subsequent\\nmoves derived from it); and the ability to recognize chess patterns quickly and to\\ndiscover the strong moves [14]. The recognition of chess patterns allows players\\nto save time, which is important to their chance of success, given the limited\\ntime available and the number of relevant possibilities through the game. The\\nchess master typically has a huge quantity of large chunks in his Long Term\\nMemory (LTM), therefore he is able to to identify the strong moves based on\\nthe positions viewed on the chessboard [14].\\n\\nThese findings highlight the need for continued research on chess personalities\\nin virtual environments. Further investigations to studies of competition between\\nvirtual humans of different personalities might help researchers identify how\\nplayers of different personalities can perform against others who vary in their\\nchess skill. For example, one approach can be via designing a chess software to\\nassess the performance of a chess player against different personalities in the\\ncommunity and provide a feedback on the weakness points and how they can\\nbe addressed. In addition, chess software designers could consider building a\\nsoftware which detects chess personality based on chess opponents and then\\nsuggesting different virtual opponents based on the personality and not just the\\nrating.\\n\\nThe findings in this research can be beneficial to design virtual humans to\\nassist in chess training. For example, research indicated the power of virtual\\nhumans in assisting students in learning topics in different areas such as medical\\neducation [1]. Virtual environments can also play an educational role in chess\\nitself, by utilizing virtual humans in helping chess students gain many topics\\ndepending on the shortcomings they have. Virtual players can also be used to\\nassess chess players and get a history about them after they play against players\\nof different personalities. For example, the focus of a new research direction could\\nbe identifying the style of chess players while they play against different chess\\nhumans. This makes it mandatory to analyze more personalities to be used by\\nchess players to identify the best training techniques.\\n\\n6 Conclusion\\n\\nThis paper has explored different virtual chess players and how they perform\\nwhen they play against each other. In this research, the researcher chose two\\nvirtual grandmasters and three class-B virtual players. The findings show that\\nthe attacker personality tends to make less errors than a defensive personality\\n\\n\\n\\nHow can virtual players help us understand the outcome of chess games? 11\\n\\nwhen playing against different class-B players. In addition, the games are longer\\nwhen playing against an attacker personality.\\n\\nThe results showed that class-B players who are strong at either the opening\\nor endgame perform better than players who are balanced when playing against\\na grandmaster with attacking or defensive style. In addition the number of moves\\nis the least when playing against a balanced player.\\n\\nThis research allows us to gain a deeper understanding of different chess per-\\nsonalities, and thus to further understand the psychology of competition between\\ndifferent personalities. Understanding chess personalities enables future research\\nto predict chess results based on the personality of the players. A future topic\\nof research could examine the physiology of different chess personalities, just as\\nthis study examined the psychological aspects of those personalities.\\n\\nReferences\\n\\n1. Berman, N.B., Durning, S.J., Fischer, M.R., Huwendiek, S., Triola, M.M.: The role\\nfor virtual patients in the future of medical education. Academic medicine 91(9),\\n1217\\u20131222 (2016)\\n\\n2. Botvinnik, M.: A mathematical representation of chess. In: Computers, Chess and\\nLong-Range Planning, pp. 11\\u201325. Springer (1970)\\n\\n3. Burgoyne, A.P., Sala, G., Gobet, F., Macnamara, B.N., Campitelli, G., Hambrick,\\nD.Z.: The relationship between cognitive ability and chess skill: A comprehensive\\nmeta-analysis. Intelligence 59, 72\\u201383 (2016)\\n\\n4. Butts, S.: Virtaul kasparov. Online (April 2002), http://www.ign.com/articles/\\n2002/04/19/virtual-kasparov, retrieved on March 20, 2017\\n\\n5. Calderwood, R., Klein, G.A., Crandall, B.W.: Time pressure, skill, and move qual-\\nity in chess. The American journal of psychology pp. 481\\u2013493 (1988)\\n\\n6. Chase, W.G., Simon, H.A.: The mind\\u2019s eye in chess. (1973)\\n\\n7. Chess.com: chessmaster 9000 questions. Online, retrieved on Nov 1, 2015\\n\\n8. Cowley, M.B., Byrne, R.M.: Chess masters\\u2019 hypothesis testing. In: International\\nPeer Reviewed Conference Proceedings (2004)\\n\\n9. De Groot, A.D.: Thought and choice in chess, vol. 4. Walter de Gruyter GmbH &\\nCo KG (1978)\\n\\n10. DeVault, D., Artstein, R., Benn, G., Dey, T., Fast, E., Gainer, A., Georgila, K.,\\nGratch, J., Hartholt, A., Lhommet, M., et al.: Simsensei kiosk: A virtual human\\ninterviewer for healthcare decision support. In: Proceedings of the 2014 interna-\\ntional conference on Autonomous agents and multi-agent systems. pp. 1061\\u20131068.\\nInternational Foundation for Autonomous Agents and Multiagent Systems (2014)\\n\\n11. Elo, A.E.: The rating of chessplayers, past and present. Arco Pub. (1978)\\n\\n12. Federation, U.: Uscf ratings distribution charts. Online (October 2015), retrieved\\nfrom: http://archive.uschess.org/ratings/ratedist.php\\n\\n13. Gobet, F., Jansen, P.J.: Training in chess: A scientific approach. Education and\\nchess (2006)\\n\\n14. Gobet, F., Simon, H.A.: Templates in chess memory: A mechanism for recalling\\nseveral boards (1996)\\n\\n15. Goldin, S.E.: Recognition memory for chess positions: Some preliminary research.\\nThe American Journal of Psychology pp. 19\\u201331 (1979)\\n\\n\\n\\n12 Khaldoon Dhou\\n\\n16. Kenny, P., Parsons, T., Gratch, J., Rizzo, A.: Virtual humans for assisted health\\ncare. In: Proceedings of the 1st international conference on PErvasive Technologies\\nRelated to Assistive Environments. p. 6. ACM (2008)\\n\\n17. Kova\\u0301cs, G., Ruttkay, Z., Fazekas, A.: Virtual chess player with emotions. In: 4th\\nHungarian Conference on Computer Graphics and Geometry (2007)\\n\\n18. Lane, D.M., Chang, Y.H.A.: Chess knowledge predicts chess memory even after\\ncontrolling for chess experience: Evidence for the role of high-level processes. Mem-\\nory & cognition pp. 1\\u201312 (2017)\\n\\n19. Levene, M., Bar-Ilan, J.: Comparing typical opening move choices made by humans\\nand chess engines. The Computer Journal 50(5), 567\\u2013573 (2007)\\n\\n20. Lucas, G.M., Gratch, J., King, A., Morency, L.P.: Its only a computer: virtual\\nhumans increase willingness to disclose. Computers in Human Behavior 37, 94\\u2013100\\n(2014)\\n\\n21. Newborn, M.: Kasparov versus Deep Blue: Computer chess comes of age. Springer\\nScience & Business Media (2012)\\n\\n22. Powell, J.L., Grossi, D., Corcoran, R., Gobet, F., Garcia-Finana, M.: The neural\\ncorrelates of theory of mind and their role during empathy and the game of chess:\\nA functional magnetic resonance imaging study. Neuroscience 355, 149\\u2013160 (2017)\\n\\n23. Rasskin-Gutman, D.: Chess metaphors: artificial intelligence and the human mind.\\nMIT Press (2009)\\n\\n24. Rizzo, A., Lucas, G., Gratch, J., Stratou, G., Morency, L.P., Chavez, K., Shilling,\\nR., Scherer, S.: Automatic behavior analysis during a clinical interview with a\\nvirtual human. Stud. Healthc. Technol 220, 316 (2016)\\n\\n25. Saariluoma, P.: Chess players\\u2019 intake of task-relevant cues. Memory & Cognition\\n13(5), 385\\u2013391 (1985)\\n\\n26. Schlickum, M.K., Hedman, L., Enochsson, L., Kjellin, A., Fella\\u0308nder-Tsai, L.: Sys-\\ntematic video game training in surgical novices improves performance in virtual\\nreality endoscopic surgical simulators: a prospective randomized study. World jour-\\nnal of surgery 33(11), 2360\\u20132367 (2009)\\n\\n27. Sheridan, H., Reingold, E.M.: Chess players\\u2019 eye movements reveal rapid recogni-\\ntion of complex visual patterns: Evidence from a chess-related visual search task.\\nJournal of vision 17(3), 4\\u20134 (2017)\\n\\n28. Swartout, W.R.: Virtual humans as centaurs: Melding real and virtual. In: In-\\nternational Conference on Virtual, Augmented and Mixed Reality. pp. 356\\u2013359.\\nSpringer (2016)\\n\\n29. Thomas, J.C., Richards, J.T.: Achieving psychological simplicity: Measures and\\nmethods to reduce cognitive complexity. Human-Computer Interaction: Design Is-\\nsues, Solutions, and Applications 161 (2009)\\n\\n30. UBISoft: Chessmaster grandmaster edition, http://chessmaster.uk.ubi.com/\\n\\nxi/index.php\\n\\n31. Wise, D.M., Rosqvist, J.: Explanatory style and well-being. Comprehensive hand-\\nbook of personality and psychopathology p. 285 (2006)\\n\\nView publication stats\\n\\nhttps://www.researchgate.net/publication/325468290\\n\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"df"},"text/html":["\n","  <div id=\"df-1a23d881-84b7-499e-a886-db3a7834646f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>FileName</th>\n","      <th>Author</th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Generalizing Surrogate-Assisted Evolutionary.pdf</td>\n","      <td>Yew-Soon Ong</td>\n","      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2204.11835.pdf</td>\n","      <td>Om Prakash Patel</td>\n","      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>OCC-90253_AM.pdf</td>\n","      <td>Om Prakash Patel</td>\n","      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1299-1309.pdf</td>\n","      <td>Om Prakash Patel</td>\n","      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>bharill2018.pdf</td>\n","      <td>Om Prakash Patel</td>\n","      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a23d881-84b7-499e-a886-db3a7834646f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-1a23d881-84b7-499e-a886-db3a7834646f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-1a23d881-84b7-499e-a886-db3a7834646f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-235019a7-b42c-411c-a6ae-04725b218236\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-235019a7-b42c-411c-a6ae-04725b218236')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-235019a7-b42c-411c-a6ae-04725b218236 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                                           FileName            Author  \\\n","0  Generalizing Surrogate-Assisted Evolutionary.pdf      Yew-Soon Ong   \n","1                                    2204.11835.pdf  Om Prakash Patel   \n","2                                  OCC-90253_AM.pdf  Om Prakash Patel   \n","3                                     1299-1309.pdf  Om Prakash Patel   \n","4                                   bharill2018.pdf  Om Prakash Patel   \n","\n","                                                Text  \n","0  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n","1  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n","2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n","3  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n","4  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  "]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"id":"508bddea","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1713587200956,"user":{"displayName":"Shivam","userId":"00171756857051516066"},"user_tz":-330},"id":"508bddea","outputId":"452045d9-82c9-4fd4-e967-dcea85f50534"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nD:\\\\IEEE_FINAL\\\\22-4-2010\\\\forTEVC-paginatedpdf\\\\tevc-dlim-2027359\\\\tevc-dlim-2027359-proof\\n\\n\\nIEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 14, NO. 3, JUNE 2010 329\\n\\nGeneralizing Surrogate-Assisted Evolutionary\\nComputation\\n\\nDudy Lim, Yaochu Jin, Senior Member, IEEE, Yew-Soon Ong, Member, IEEE, and\\nBernhard Sendhoff, Senior Member, IEEE\\n\\nAbstract—Using surrogate models in evolutionary search pro-\\nvides an efficient means of handling today’s complex applica-\\ntions plagued with increasing high-computational needs. Recent\\nsurrogate-assisted evolutionary frameworks have relied on the\\nuse of a variety of different modeling approaches to approximate\\nthe complex problem landscape. From these recent studies, one\\nmain research issue is with the choice of modeling scheme used,\\nwhich has been found to affect the performance of evolutionary\\nsearch significantly. Given that theoretical knowledge available\\nfor making a decision on an approximation model a priori is very\\nmuch limited, this paper describes a generalization of surrogate-\\nassisted evolutionary frameworks for optimization of problems\\nwith objectives and constraints that are computationally expen-\\nsive to evaluate. The generalized evolutionary framework unifies\\ndiverse surrogate models synergistically in the evolutionary\\nsearch. In particular, it focuses on attaining reliable search\\nperformance in the surrogate-assisted evolutionary framework\\nby working on two major issues: 1) to mitigate the ‘curse\\nof uncertainty’ robustly, and 2) to benefit from the ‘bless of\\nuncertainty.’ The backbone of the generalized framework is\\na surrogate-assisted memetic algorithm that conducts simulta-\\nneous local searches using ensemble and smoothing surrogate\\nmodels, with the aims of generating reliable fitness prediction\\nand search improvements simultaneously. Empirical study on\\ncommonly used optimization benchmark problems indicates that\\nthe generalized framework is capable of attaining reliable, high\\nquality, and efficient performance under a limited computational\\nbudget.\\n\\nIndex Terms—Approximation models, computationally expen-\\nsive problems, memetic algorithms, metamodels, surrogate mod-\\nels, surrogate-assisted evolutionary algorithms.\\n\\nI. Introduction\\n\\nOVER the years, evolutionary algorithms (EAs) have\\nbecome one of the well-established optimization tech-\\n\\nniques, especially in the fields of art and design, business and\\n\\nManuscript received January 30, 2008; revised May 14, 2008. First version\\npublished December 11, 2009; current version published May 28, 2010. This\\npaper was supported by Honda Research Institute Europe GmbH, Germany.\\n\\nD. Lim is with Center for Computational Intelligence, School of Computer\\nEngineering, Nanyang Technological University, 639798, Singapore (e-mail:\\ndlim@ntu.edu.sg).\\n\\nY. Jin and B. Sendhoff are with Honda Research Institute Europe\\nGmbH, Offenbach 63073, Germany (e-mail: yaochu.jin@honda-ri.de;\\nbernhard.sendhoff@honda-ri.de).\\n\\nY. S. Ong is with the Division of Information Systems, School of Computer\\nEngineering, Nanyang Technological University, 639798, Singapore. He is\\nalso with the Center for Computational Intelligence (C2I), Nanyang Techno-\\nlogical University, 639798, Singapore (e-mail: asysong@ntu.edu.sg).\\n\\nColor versions of one or more of the figures in this paper are available\\nonline at http://ieeexplore.ieee.org.\\n\\nDigital Object Identifier 10.1109/TEVC.2009.2027359\\n\\nfinance, science and engineering. Many successful applications\\nof EAs have been reported, ranging from music composi-\\ntion [1] to financial forecasting [2], aircraft design [3], job-\\nshop scheduling [4], and drug design [5]. Although well estab-\\nlished as credible and powerful optimization tools, researchers\\nin this area are now facing new challenges of increasing\\ncomputational needs by today’s applications. For instance,\\na continuing trend in science and engineering is the use of\\nincreasingly high-fidelity accurate analysis codes in the de-\\nsign and simulation process. Modern computational structural\\nmechanics (CSM), computational electro-magnetics (CEM),\\ncomputational fluid dynamics (CFD) and first principle sim-\\nulations have been shown to be reasonably accurate. Such\\nanalysis codes play a central role in the design process since\\nthey aid designers and scientists in validating new designs\\nand studying the effect of altering key parameters on prod-\\nuct and/or system performance. However, such moves may\\nprove to be cost-prohibitive or impractical in the evolutionary\\ndesign optimization process, leading to intractable design\\ncycle times.\\n\\nAn intuitive way to reduce the search time of evolutionary\\noptimization algorithms when dealing with computationally\\nexpensive solver, is the use of high-performance comput-\\ning technologies and/or computationally efficient surrogate\\nmodels. In recent years, there have been increasing research\\nactivities in the design of surrogate-assisted evolutionary\\nframeworks for handling complex optimization problems with\\ncomputationally expensive objective functions and constraints.\\nIn particular, since the modeling and design optimization\\ncycle time is roughly proportional to the number of calls\\nto the computationally expensive solver, many evolutionary\\nframeworks have turned to the deployment of computationally\\ncheap approximation models in the search to replace in part\\nthe original solvers [6]– [8]. Using approximation models\\nalso known as surrogates or metamodels, the computational\\nburden can be greatly reduced since the efforts required to\\nbuild the surrogates and to use them are much lower than\\nthose in the standard approach that directly couples the EA\\nwith the expensive solvers. Among the approximation models,\\npolynomial regression (PR), also known as response surface\\nmethodology (RSM), support vector machine (SVM), artificial\\nneural networks (ANNs), radial basis function (RBF), and\\nGaussian process (GP), also referred to as Kriging or design\\nand analysis of computer experiment (DACE) models, are the\\nmost prominent and commonly used [9]–[11].\\n\\n1089-778X/$26.00 c© 2009 IEEE\\n\\n\\n\\n330 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 14, NO. 3, JUNE 2010\\n\\nIn the context of EA, various approaches for working with\\ncomputationally expensive problems using surrogate models\\nhave been reported. Early techniques include the use of fitness\\ninheritance or imitation [12], [13], where the fitness of an\\nindividual is defined by either the parents or other individuals\\npreviously encountered along the search. Another common\\napproach is to preselect a subset of individuals that would\\nundergo exact function evaluations, while all others are pre-\\ndicted based on surrogate models. Some of the simple schemes\\nintroduced are based on random individual selection [14] or\\nselecting the best/most promising individuals based on the\\npredictions made by the surrogate models [7], [11], [15],\\n[16]. Other schemes include identifying some cluster cen-\\nters [17], [18], or uncertain individuals that are predicted\\nto have poor estimates [19] as representatives that will un-\\ndergo exact function evaluations subsequently. Such forms of\\nmodel management schemes are termed as ‘evolution control’\\nin [7] and [20]. An alternative approach adopted in [21]\\ninvolves the refinement of the surrogate used from coarse-\\nto-fine grained models as the search evolves. Online localized\\nsurrogate models are also deployed within the local search\\nphase of memetic algorithms (MAs) [8], [22]. The synergy of\\nonline global and local surrogate in the memetic search was\\nalso investigated in [11]. To enhance the prediction accuracy\\nof fitness predictions based on surrogates, the inclusion of\\ngradient information in surrogate building was also studied\\nin [23] and [24], independently. More recently, Schmidt and\\nLipson [25] proposed the use of coevolution technique to\\naddress issues such as level of approximation and accuracy\\nof fitness predictors.\\n\\nMore recently, the idea of using surrogate to speed up\\nevolutionary search process has found its way into the field\\nof evolutionary multiobjective optimization (MOO). Many\\nof the schemes introduced in the context of single-objective\\noptimization (SOO) have been extended to their corresponding\\nMOO variants. The Kriging-based surrogate-assisted evolu-\\ntionary multiobjective algorithm in [26] represents an ex-\\ntension of the efficient global optimization framework [27]\\nintroduced for handling SOO problems, whereas [28] and [29]\\nextended the coarse-to-fine grained approximation and prese-\\nlection schemes to its MOO variants, respectively. The co-\\nevolution of genetic algorithms (GAs) for multiple objectives\\nbased on online surrogates was introduced in [30]. After some\\nfixed search intervals, the surrogates produced that represent\\nthe different objectives are then exchanged and shared among\\nmultiple GAs. In [31], a multiobjective EA is run for a\\nnumber of iterations on a surrogate model before the model\\nis updated using exact evaluation from some selected points.\\nFor greater details on surrogate-assisted EAs for handling\\noptimization problems with computationally expensive ob-\\njective/constraint functions, the readers are referred to [9]\\nand [32].\\n\\nIn spite of the extensive research efforts on this topic,\\nexisting surrogate-assisted evolutionary frameworks remains\\nopen for further improvement. Jin et al. [14] have shown that\\nexisting surrogate-assisted evolutionary frameworks proposed\\nare often flawed by introduction of false optima since the\\nparametric approximation technique used may not be capable\\n\\nof modeling the problem landscapes accurately, thus producing\\nunreliable search. Generally, the ‘curse of dimensionality’\\ncreates significant difficulties in the construction of accurate\\nsurrogate models for fitness prediction. Further, recent studies\\nhave shown that the choice of approximation technique used\\naffects the performance of evolutionary searches [33]. On\\nthe other hand, it is worth keeping in mind that approxi-\\nmation error in the surrogate model does not always harm.\\nA surrogate model capable of smoothing the multimodal or\\nnoisy landscape of the complex problem may contribute more\\nbeneficially to the evolutionary search than one that models\\nthe original fitness function accurately. For instance, the study\\nin [44] has emphasized the importance of predicting search\\nimprovement as opposed to the usual practice of improving\\nonly the quality of the surrogate in the context of evolution-\\nary optimization. Based on these recent studies, it is worth\\nhighlighting the influence of the approximation method used\\non the performance of any surrogate-assisted evolutionary\\nsearch. The greatest barrier to further progress is that, with\\nso many approximation techniques available in the literature,\\nit is almost impossible to know which is most relevant for\\nmodeling the problem landscape or generating reliable fitness\\npredictions when one has only limited knowledge of its\\nfitness space before the search starts. Moreover, approximation\\ntechniques by themselves may model differently on different\\nproblem landscapes. Depending on the complexity of a design\\nproblem, a single approximation model that may have proven\\nto be successful in an instance might not work so well, or\\nat all, on others. In the field of multidisciplinary optimiza-\\ntion, such observations have also been reported [34]–[41]. In\\nthose studies, this issue is commonly handled by performing\\nmultiple optimization runs, each on different surrogate model\\nor ensemble model. In [34], [35] and [39], a set of surrogate\\nmodels consisting Kriging, PR, RBF, and weighted average\\nensemble is used to demonstrate that multiple surrogates can\\nimprove robustness of optimization at minimal cost. Similarly,\\n[36] uses PR and RBF surrogate models in the context of\\nmultiobjective optimization and shows that each of the models\\nperforms better at different region of the Pareto front. Others\\nin [37], [38], [40] and [41] resolve this issue by introducing\\nvarious ensemble model building techniques. It is shown from\\nthese studies that ensemble models generally outperform most\\nof the individual surrogates.\\n\\nThis paper introduces a generalized framework for unifying\\ndiverse surrogate models synergistically in the evolutionary\\nsearch. In contrast to existing efforts, we focus on predicting\\nsearch improvement in the context of optimization as opposed\\nto solely on improving the prediction quality of the approx-\\nimation. In particular, we generalize the problem to attain\\nreliable search improvement in surrogate-assisted evolutionary\\nframework as two major goals: 1) to mitigate the ‘curse of\\nuncertainty’; 2) to benefit from the ‘bless of uncertainty’. The\\n‘curse of uncertainty’1 refers to the negative consequences\\nintroduced by the approximation error of the surrogate models\\n\\n1In the present context, the definition of ‘uncertainty’ refers to the approxi-\\nmation errors in the fitness function due to the use of surrogate models based\\non the definitions given in [45].\\n\\n\\n\\nLIM et al.: GENERALIZING SURROGATE-ASSISTED EVOLUTIONARY COMPUTATION 331\\n\\nFig. 1. Curse and bless of uncertainty in single-objective EA using surrogates. (a) ‘Curse of uncertainty’ in single-objective EA using surrogates. Approximated\\nfunction in the figure is obtained using spline interpolation technique. (b) ‘Bless of uncertainty’ in single-objective EA using surrogates. Approximated function\\nin the figure is obtained using a low order polynomial regression.\\n\\nused. On the other hand, ‘bless of uncertainty’ refers to the\\nbenefits attained by the use of surrogate models. Particularly,\\nwe seek for surrogate models that are capable of generating\\nreliable fitness predictions on diverse problems of different\\nlandscapes to mitigate the ‘curse of uncertainty’ on one hand,\\nand on the other hand surrogate models that are capable of\\nsmoothing rugged fitness landscapes to prevent the search\\nfrom getting stuck in local optima [44]. Previous studies\\nby Yao et al. [42], [43] have also confirmed that smoothed\\nlandscape of rugged fitness landscape can lead the search\\nto optimum solutions easier than using the exact fitness\\nlandscape.\\n\\nThe rest of this paper is organized as follows. Section II\\ndiscusses the impacts of uncertainty due to approximation\\nerrors in evolutionary frameworks that employ surrogates.\\nBased on the discussion, Section III provides a generaliza-\\ntion of surrogate-assisted evolutionary search for both SOO\\nand MOO subsequently. We summarize the empirical studies\\non some popular SOO and MOO benchmark problems in\\nSection IV. Finally, Section V concludes this paper.\\n\\nII. Impacts of Approximation Errors in\\n\\nSurrogate-Assisted Evolutionary Algorithms\\n\\nIn this section, we briefly discuss the effects of uncertainty\\nintroduced by inaccurate approximation models on surrogate-\\nassisted evolutionary algorithms (SAEA) search performance.\\nWithout loss of generality, here we consider computationally\\nexpensive minimization problems under limited computational\\nbudget with bound constraints of the following form:\\n\\nminimize: f1(x), f2(x), . . . , fr(x)\\n\\nsubject to: xl\\ni ≤ xi ≤ xu\\n\\ni (1)\\n\\nwhere i = 1, 2, . . . , d, d is the dimensionality of the search\\nproblem, r is the number of objective functions, and xl\\n\\ni, xu\\ni are\\n\\nthe lower and upper bounds of the ith dimension of vector x,\\nrespectively.\\n\\nNote that when more than one objective is involved for\\napproximation, there are two commonly adopted strategies,\\ni.e., 1) one approximation model per objective function; 2) one\\napproximation model for an aggregated (linear or nonlinear\\ncombination) objective function, faggr(x). In this paper, we\\nconsider the second strategy. Since in single-objective context,\\nfaggr(x) = f (x) = f1(x), the term f (x) might be used\\ninterchangeably to faggr(x) for brevity purposes when only\\nsingle-objective context is considered.\\n\\nIf faggr(x) denotes the original fitness function and the\\napproximated function is f̂aggr(x), the approximation errors at\\nany solution vector x is e(x), i.e., the uncertainty introduced\\nby the surrogate at x, may then be defined as\\n\\ne(x) = |faggr(x) − f̂aggr(x)|. (2)\\n\\nHere, we highlight the negative and positive impacts in-\\ntroduced by the approximation inaccuracies of the surrogates\\non SAEA search [44]. The negative impact or otherwise\\nknown as the ‘curse of uncertainty’ on SAEA search can\\nbe briefly defined as the phenomenon where the inaccura-\\ncies of the surrogates used results in the SAEA search to\\nstall or converge to false optimum. To illustrate the ‘curse’\\neffect, we refer to Fig. 1(a), where the SAEA is likely to\\nconverge to the false optimum of the spline interpolation\\nmodel due to inaccuracy. On the other hand, the positive\\nimpact, i.e., the ‘bless of uncertainty’ in SAEA materializes\\nwhen the use of surrogate(s) brings about greater search\\nimprovements over the use of original exact objective/fitness\\nfunction. For instance, the surrogate can help to traverse the\\nsearch across valleys and hills of local optima by smoothing\\nthe ruggedness/multimodality of the problem landscape. To\\nillustrate the blessing effect, we refer to the example in\\nFig. 1(b), where a low-order polynomial regression scheme\\nis used to approximate the exact objective function. Due to\\n\\n\\n\\n332 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 14, NO. 3, JUNE 2010\\n\\nFig. 2. Curse and bless of uncertainty in multiobjective EA using surrogates. (a) ‘Curse of uncertainty’ in multiobjective EA using surrogates. (b) ‘Bless of\\nuncertainty’ in multiobjective EA using surrogates.\\n\\nthe smoothing effect of the polynomial surrogate, the search\\nleads to an improved solution that is unlikely to be attained\\neven if the exact objective function is used. Hence, the ‘bless of\\nuncertainty’ brings about possible acceleration in the search.\\nBesides a faster convergence, recent study in [32] revealed\\nthat the ‘bless of uncertainty’ in SAEA also exists in the form\\nof improving evolutionary search diversity through the use of\\nsurrogate model.\\n\\nNext, to illustrate ‘curse and bless of uncertainty’ in the\\ncontext of multiobjective optimization, we refer to the exam-\\nples in Fig. 2(a) and (b). Fig. 2(a) depicts the effect of ‘curse of\\nuncertainty’ in MOEA search due to the presence of inaccurate\\nsurrogate models. In Fig. 2(a), the surrogate-assisted MOEA\\nsearch is observed to be evolving toward poor nondominated\\nsolutions in comparison to that based on exact fitness func-\\ntions. Moreover, those labeled as x1 and x2 in Fig. 2(a) suggest\\nthat some solutions might stall, while others fail to converge\\noptimally. On the other hand, Fig. 2(b) illustrates the presence\\nof ‘bless of uncertainty’ where the errors in the surrogate\\nused is observed to improve the MO evolutionary search in\\nboth convergence and diversity measures. Particularly, some\\nimproved solutions of the surrogate-assisted search is shown\\nto dominate at least one of its initial solutions, while others\\nsuch as x3 and x4 are newly found nondominated solutions.\\n\\nIII. Generalizing Surrogate-Assisted\\n\\nEvolutionary Search\\n\\nIn this section, we present a generalization of surrogate-\\nassisted evolutionary frameworks for optimization of problems\\nwith objective(s) and constraint(s) that are computationally\\nexpensive to evaluate. The generalized framework illustrated\\nhere for unifying diverse approximation concept synergisti-\\ncally is a surrogate-assisted memetic algorithm that conducts\\nsimultaneous local searches on separate ensemble and smooth-\\ning surrogate models. MAs are population-based metaheuristic\\nsearch methods that are inspired by Darwinian principles of\\nnatural evolution and Dawkins notion of a meme defined as a\\n\\nAlgorithm 1 Memetic Algorithm (for SOO)\\n\\n1: Initialization: Generate and evaluate a population of\\ndesign vectors.\\n\\n2: while computational budget is not exhausted do\\n3: Apply evolutionary operators (selection, crossover, mu-\\n\\ntation) to create a new population.\\n4:\\n\\n5: / ∗ ∗ ∗ ∗ Local Search Phase ∗ ∗ ∗ ∗ /\\n\\n6:\\n\\n7: for each individual x in current population do\\n8: • Apply local search to find an improved solution,\\n\\nxopt.\\n9: • Perform replacement using Lamarckian learning,\\n\\ni.e.,\\n10: if f (xopt) < f (x) then\\n11: x = xopt\\n\\n12: end if\\n13: end for\\n14:\\n\\n15: / ∗ ∗ End of Local Search Phase ∗ ∗ /\\n\\n16:\\n\\n17: end while\\n\\nunit of cultural evolution capable of local refinements [46].2\\n\\nFor example, the brief outline of a traditional MA is provided\\nin Algorithm 1.\\n\\nIn the generalized framework, we introduce first the idea of\\nemploying online local ensemble surrogate models constructed\\nfrom diverse approximation concepts using data points that lie\\nin the vicinity of an initial guess. The surrogate or approxi-\\n\\n2Note that the rationale behind using a memetic framework over a traditional\\nevolutionary framework is multifold [46], [50]. First, we aim to exploit\\nMAs’ capability of locating the local and global optima efficiently. Second,\\na memetic model of adaptation exhibits the plasticity of individuals that a\\npure genetic model fails to capture. Further, by limiting the use of surrogate\\nmodels within the local search procedures, the global convergence property\\nof EAs can be ensured. For a greater exposition of local metaheuristics in\\noptimization, the reader is referred to [47]–[49].\\n\\n\\n\\nLIM et al.: GENERALIZING SURROGATE-ASSISTED EVOLUTIONARY COMPUTATION 333\\n\\nmation models are then used to replace the expensive function\\nevaluations performed in the local search phase. The improved\\nsolution generated by the local search procedure then replaces\\nthe genotype and/or fitness of the original individual.3\\n\\nA. Ensemble Model\\n\\nTo mitigate the ‘curse of uncertainty’ caused by the effect\\nof using imperfect surrogate models, we seek for surrogate\\nmodels that are capable of generating reliable fitness predic-\\ntions on diverse problems. In particular, since it is almost\\nimpossible to know in advance which approximation technique\\nbest suits the optimization problem at hand, we consider a\\nsynergy of diverse approximation methods through the use\\nof ensemble models to generate reliable accurate predictions\\nacross problems of differing problem landscapes [18], [37],\\n[51], as opposed to single surrogate models created by specific\\napproximation scheme that may not be appropriate for the\\nproblem at hand. In what follows, we consider online local\\nweighted average ensembles. For instance, in the single-\\nobjective context, the predicted ensemble output of f (x) is\\nformulated as\\n\\nf̂ens(x) =\\nn∑\\n\\ni=1\\n\\ncif̂i(x)\\n\\nn∑\\ni=1\\n\\nci = 1 (3)\\n\\nwhere f̂ens(x) and f̂i(x) are the fitness prediction made by the\\nensemble and ith surrogate model, respectively. The same for-\\nmulation applies in the multiobjective context where faggr(x) is\\nconsidered. ci is the weight coefficient associated with the ith\\nsurrogate model. A model can be assigned a larger weight if it\\nis found or deemed to be more accurate. Hence, the weighting\\nfunction becomes\\n\\nci =\\n\\n∑n\\nj=1,j �=i εj\\n\\n(n − 1)\\n∑n\\n\\nj=1 εj\\n\\n(4)\\n\\nwhere εj is the error measurement for the jth surrogate model.\\nHere, the root mean square error (RMSE) is used as the error\\nmeasurement. The RMSE of each surrogate model is then of\\nthe form\\n\\nRMSE =\\n\\n√∑m\\ni=1 e2(xi)\\n\\nm\\n(5)\\n\\nwhere m is the number of data samples compared, e(xi) is\\nthe error of prediction for data point xi, as shown in (2). For\\ngreater details on other ensemble model building techniques,\\ninterested readers are referred to [37], [38], [40], [41] and [51].\\n\\n3There are two basic replacement strategies in MAs [50].\\n\\n1) Lamarckian learning forces the genotype to reflect the result of\\nimprovement in local search by placing the locally improved individual\\nback into the population to compete for reproductive opportunities.\\n\\n2) Baldwinian learning only alters the fitness of the individuals and the\\nimproved genotype is not encoded back into the population.\\n\\nFor the sake of brevity, we consider Lamarckian learning in this paper.\\n\\nB. Landscape Smoothing Model\\n\\nMeanwhile, to benefit from the ‘bless of uncertainty,’\\nsmoothing techniques including global convex underestima-\\ntion, tunneling and filling methods are some appropriate alter-\\nnatives [52] that may be used. Given a problem landscape,\\nsmoothing methods transform the function into one with\\nnoticeably fewer minima, thus speeding up the evolutionary\\nsearch. In the generalized framework, global convex under-\\nestimation is used for successive smoothing of the problem\\nlandscape within the local search phase which is realized\\nthrough low-order polynomial regression (PR). Besides the\\ngeneralization property of PR models on rugged landscape, the\\nlow-computational costs incurred makes them very efficient as\\nonline surrogate models. Note that the PR model may be used\\nin both ensemble and the smoothing models, hence only a\\none-time model building cost is involved.\\n\\nC. GSM Framework for Single-Objective Optimization\\n\\nIn this subsection, we describe the generalized surrogate\\nmemetic framework for single-objective optimization. A brief\\noutline of the generalized surrogate single-objective memetic\\nalgorithm (GS-SOMA) is presented in Algorithm 2. Note that\\nthe difference between the GS-SOMA and a traditional MA\\nlies in the local search phase of the algorithms.\\n\\nGS-SOMA begins with the initialization of a population\\nof design points. During the database building phase, the\\nsearch operates like a traditional evolutionary algorithm based\\non the original exact fitness function for some initial Gdb\\n\\ngenerations. Up to this stage, no form of surrogates are\\nused, and all exact fitness function evaluations made are\\narchived in a central database. Subsequently, the algorithm\\nproceeds into the local search phase. For each individual\\nx, n online surrogates that model the fitness function are\\ncreated dynamically using m training data points, which lie\\nin the vicinity of x, extracted from the archived database of\\npreviously evaluated design points. From the n surrogates,\\nan ensemble model is built. From here, two separate local\\nsearches are conducted on: 1) M1, the ensemble of n surrogate\\nmodels, and 2) M2, a low-order PR model. If improved so-\\nlutions are achieved, GS-SOMA proceeds with the individual\\nreplacement scheme. Since we adopt the Lamarckian scheme\\nhere, the genotype/phenotype of the initial individual is then\\nreplaced by the higher quality solutions among the two that\\nare locally improved based on M1 and M2, i.e., x1\\n\\nopt or x2\\nopt.\\n\\nThe search cycle is then repeated until the allowed maximum\\ncomputational budget is exhausted.\\n\\nD. GSM Framework for Multiobjective Optimization\\n\\nNext, we describe the generalized surrogate memetic frame-\\nwork in the context of multiobjective optimization (MOO). In\\nMOO, a solution x(1) is said to dominate solution x(2) in the\\nobjective space, i.e., x(1) � x(2) if the following two conditions\\nhold:\\n\\n1) x(1) is no worse than x(2) on all objectives or fj(x(1)) ≤\\nfj(x(2)) for all j = 1, 2, . . . , r;\\n\\n2) x(1) is strictly better than x(2) on at least one objective,\\nor fj(x(1)) < fj(x(2)) for at least one j ∈ 1, 2, . . . , r.\\n\\n\\n\\n334 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 14, NO. 3, JUNE 2010\\n\\nAlgorithm 2 Generalized Surrogate Single-Objective Memetic\\nAlgorithm (GS-SOMA)\\n\\n1: initialization: Generate and evaluate a database contain-\\ning a population of designs, archive all exact evaluations\\ninto the database.\\n\\n2: while computational budget is not exhausted do\\n3: if generation count < database building phase (Gdb)\\n\\nthen\\n4: Evolve the population using exact fitness function\\n\\nevaluations, archive all exact evaluations into the\\ndatabase.\\n\\n5: else\\n6: Apply evolutionary operators (selection, crossover,\\n\\nmutation) to create a new population.\\n7:\\n\\n8: / ∗ ∗ ∗ ∗ Local Search Phase ∗ ∗ ∗ ∗ /\\n\\n9:\\n\\n10: for each individual x in the population do\\n11: • Find m nearest points to x in database as training\\n\\npoints for surrogate models.\\n12: • Build model-1: M1, as an ensemble of all M ′\\n\\nj for\\nj = 1, . . . , n where n is the number of surrogate\\nmodels used.\\n\\n13: • Build model-2: M2, which is a low-order PR\\nmodel.\\n\\n14: • Apply local search in M1 to arrive at x1\\nopt, and\\n\\nM2 to arrive at x2\\nopt.\\n\\n15: • Replace x with the locally improved solution,\\ni.e.,\\n\\n16: if f (x1\\nopt) < f (x2\\n\\nopt) then\\n17: x = x1\\n\\nopt\\n18: else\\n19: x = x2\\n\\nopt\\n20: end if\\n21: • Archive all new exact function evaluations into\\n\\nthe database.\\n22: end for\\n23:\\n\\n24: / ∗ ∗ End of Local Search Phase ∗ ∗ /\\n\\n25:\\n\\n26: end if\\n27: end while\\n\\nIf set P is the entire feasible search space, the nondominated\\nset P∗ is labeled as the Pareto-optimal set. Any two solu-\\ntions in P∗ must nondominate each other, i.e., x(1) ∼ x(2).\\nOn the other hand, Pareto front (PF ∗) is the image of the\\nPareto-optimal set in objective space. The brief outline of\\na typical multiobjective memetic algorithm (MOMA) using\\nweighting (scalarization) technique [58]–[60] is illustrated in\\nAlgorithm 3. In contrast, the studied GSM framework for\\nmultiobjective optimization (GS-MOMA) is outlined in Algo-\\nrithm 4. Note that the key differences of the two algorithms\\nlie in the local search phase and selection pool forming\\nphase.\\n\\nGS-MOMA begins with the population initialization phase\\nand evolutionary search based on exact fitness function for a\\n\\nAlgorithm 3 Multiobjective Memetic Algorithm\\n\\n1: initialization: Generate and evaluate a population of de-\\nsign vectors.\\n\\n2: while computational budget is not exhausted do\\n3: Apply MO evolutionary operators (selection, crossover,\\n\\nmutation) to create a new population.\\n4:\\n\\n5: / ∗ ∗ ∗ ∗ Local Search Phase ∗ ∗ ∗ ∗ /\\n\\n6:\\n\\n7: for each individual x in the population do\\n8: • Generate a random weight vector w =\\n\\n(w1, w2, . . . , wr),\\n∑r\\n\\ni=1 wi = 1 where r is the number\\nof objectives.\\n\\n9: • Apply local search in faggr =\\n∑r\\n\\ni=1 wifi(x) to find\\nan improved solution, xopt.\\n\\n10: • Perform Lamarckian learning, i.e.,\\n11: if faggr(xopt) < faggr(x) then\\n12: x = xopt\\n\\n13: end if\\n14: end for\\n15:\\n\\n16: / ∗ ∗ End of Local Search Phase ∗ ∗ /\\n\\n17:\\n\\n18: end while\\n\\nnumber of early generations, Gdb, before entering the local\\nsearch phase. In the local search phase, independent local\\nsearches are conducted on: 1) M1, the ensemble of n surrogate\\nmodels; 2) M2, the smoothing low-order PR model on each\\nindividual of the generated offspring population. For the sake\\nof brevity, the core distinguishing feature of GS-MOMA can\\nbe noted in line 17 of Algorithm 4, i.e., the existence of the\\nReplace&Archive procedure.\\n\\nThe Replace&Archive procedure performs replacements\\nbased on domination between the original offspring and the\\ntwo local optima found. The original offspring will only be\\nreplaced by one dominating optimum found. Any other local\\noptima are then saved into the learning archive, Al. Note that\\nthe result of GS-MOMAs local searches is either xopt � x or\\nxopt ∼ x. Otherwise, there is no improvement to the original\\noffspring, and hence we get xopt == x.\\n\\nBased on the procedure in Algorithm 5, the possible\\nlocal search outcomes and corresponding actions taken by\\nthe scheme are summarized in Table I. Note that there exist\\nsix possible actions to be taken by GS-MOMA which are\\nsummarized as follows:\\n\\n1) replacement is performed once [e.g., Fig. 3(a)];\\n2) two subsequent replacements are performed [e.g.,\\n\\nFig. 3(b)];\\n3) both replacement and archiving are performed [e.g.,\\n\\nFig. 3(c)];\\n4) archiving is performed once [e.g., Fig. 3(d)];\\n5) archiving is performed twice [e.g., Fig. 3(e)];\\n6) neither replacement nor archiving is performed [e.g.,\\n\\nFig. 3(f)].\\n\\n\\n\\nLIM et al.: GENERALIZING SURROGATE-ASSISTED EVOLUTIONARY COMPUTATION 335\\n\\nTABLE I\\n\\nActions Taken by the Replace&Archive Scheme in GS-MOMA for Corresponding Results of Local Searches. Note That Irrelevant\\n\\nCases Have Been Excluded for Brevity\\n\\nx1\\nopt vs x x2\\n\\nopt vs x x1\\nopt vs x2\\n\\nopt Actions taken by GS-MOMA\\n\\n� � � x = x1\\nopt\\n\\n� � � x = x2\\nopt\\n\\n� � ∼ x = x1\\nopt, archive x2\\n\\nopt\\n\\n� � == x = x1\\nopt\\n\\n� == � x = x1\\nopt\\n\\n� ∼ � x = x1\\nopt\\n\\n� ∼ ∼ x = x1\\nopt, archive x2\\n\\nopt\\n\\n== � � x = x2\\nopt\\n\\n== == == No changes\\n\\n== ∼ ∼ Archive x2\\nopt\\n\\n∼ � � x = x2\\nopt\\n\\n∼ � ∼ x = x2\\nopt, archive x1\\n\\nopt\\n\\n∼ == ∼ Archive x1\\nopt\\n\\n∼ ∼ � Archive x1\\nopt\\n\\n∼ ∼ � Archive x2\\nopt\\n\\n∼ ∼ ∼ Archive x1\\nopt and x2\\n\\nopt\\n\\n∼ ∼ == Archive x1\\nopt\\n\\nAt the end of each GS-MOMA generation, Al is combined\\nwith the current parent population, Pc, and the offspring\\npopulation, Po to form the entire pool of individuals, Ps\\n\\nthat will then undergo the MOEA selection mechanism, i.e.,\\nPs = Pc\\n\\n⋃\\nPo\\n\\n⋃\\nAl. From here, the process described repeats\\n\\nuntil the maximum computational budget of the GS-MOMA\\nis exhausted.\\n\\nE. Local Search Scheme\\n\\nIn the GSM framework for SO/MOO, a trust-region-\\nregulated search strategy is utilized to ensure convergence\\nto some local optimum or the global optimum of the exact\\ncomputationally expensive fitness function [8], [53], [61], even\\nthough surrogate models are deployed in the local search.\\nFor each individual in the GS-SO/MOMA population, the\\nlocal search (refer to line 14 of Algorithm 2 and line 16\\nof Algorithm 4) proceeds with a sequence of trust-region\\nsubproblems of the form\\n\\nminimize : f̂ k(xk\\nc + s)\\n\\nsubject to : ‖s‖ ≤ �k (6)\\n\\nwhere k = 0, 1, 2, . . . , kmax, f̂ (x) is the approximation function\\ncorresponding to the objective function f (x). Meanwhile, xk\\n\\nc ,\\ns, and �k represent the initial guess (current best solution)\\nat iteration k, an arbitrary step, and the trust-region radius at\\niteration k, respectively. In our experiments, the Sequential\\nQuadratic Programming (SQP) [54] is used to minimize the\\nsequence of subproblems on the approximated landscape.\\n\\nDuring the local search, the initial trust-region radius � is\\ninitialized based on the minimum and maximum values of the\\nm design points used to construct the surrogate model (refer\\nto line 11 of Algorithm 2 and line 13 of Algorithm 4). The\\ntrust-region radius for iteration k, i.e., �k is updated based\\n\\non a measure which indicates the accuracy of the surrogate\\nmodel at the kth local optimum, xk\\n\\nopt. This measure, ρk,\\nprovides a measure of the actual versus predicted change in\\nthe exact fitness function values at the kth local optimum and\\nis calculated as\\n\\nρk =\\nf (xk\\n\\nc) − f (xk\\nopt)\\n\\nf̂ (xk\\nc) − f̂ (xk\\n\\nopt)\\n. (7)\\n\\nThe value of ρk is then used to update the trust-region radius\\nas follows [61]:\\n\\n�k+1 = C1�\\nk, if ρk ≤ C2\\n\\n= �k, if C2 < ρk ≤ C3 (8)\\n\\n= C4�\\nk, if ρk > C3\\n\\nwhere C1, C2, C3, and C4 are constants. Typically, C1 ∈ (0, 1)\\nand C4 ≥ 1 for the scheme to work efficiently. From experi-\\nence, we set C1 = 0.25, C2 = 0.25, C3 = 0.75, and C4 = 2, if\\n||xk\\n\\nopt − xk\\nc ||∞ = �k or C4 = 1, if ||xk\\n\\nopt − xk\\nc ||∞ < �k.\\n\\nThe trust-region radius for the next iteration, �k+1, is\\nreduced if the accuracy of the surrogate, measured by ρk is\\nlow. On the other hand, �k is doubled if the surrogate is found\\nto be accurate and the kth local optimum, xk\\n\\nopt, lies on the\\ntrust-region bounds. Otherwise the trust-region radius remains\\nunchanged.\\n\\nThe initial guess of the optimum at iteration k + 1 becomes\\n\\nxk+1\\nc = xk\\n\\nopt, if ρk > 0\\n\\n= xk\\nc, if ρk ≤ 0. (9)\\n\\nThe trust-region process for an individual terminates when the\\ntermination condition is satisfied. For instance, this termination\\ncondition could be when the trust-region radius � approaches\\nε, where ε represents some small trust-region radius, or when\\na maximum number of iteration kterm is reached.\\n\\n\\n\\n336 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 14, NO. 3, JUNE 2010\\n\\nFig. 3. Examples of the six different actions taken by the Replace&Archive scheme in GS-MOMA for corresponding results of local searches. (a) An\\nexample of the case where replacement is performed only once by GS-MOMA. (x1\\n\\nopt � x) ∧ (x1\\nopt � x2\\n\\nopt) ∧ (x ∼ x2\\nopt). x1\\n\\nopt replaces x. (b) An example\\nof the case where two subsequent replacements are performed by GS-MOMA. (x1\\n\\nopt � x) ∧ (x2\\nopt � x1\\n\\nopt). x1\\nopt replaces x, followed by x2\\n\\nopt replaces x.\\n(c) An example of the case where both replacement and archiving are performed by GS-MOMA. (x1\\n\\nopt � x) ∧ (x2\\nopt � x) ∧ (x1\\n\\nopt ∼ x2\\nopt). x1\\n\\nopt replaces\\nx, x2\\n\\nopt is archived in Al. (d) An example of the case where archiving is performed only once by GS-MOMA. (x ∼ x1\\nopt) ∧ (x ∼ x2\\n\\nopt) ∧ (x1\\nopt � x2\\n\\nopt).\\nx1\\n\\nopt is archived in Al. (e) An example of the case where archiving is performed twice by GS-MOMA. (x ∼ x1\\nopt) ∧ (x ∼ x2\\n\\nopt) ∧ (x1\\nopt ∼ x2\\n\\nopt).\\nBoth x1\\n\\nopt and x2\\nopt are archived in Al. (f) An example of the case where neither replacement nor archiving is performed. No new optimum\\n\\nis found.\\n\\nIV. Empirical Study\\n\\nIn this section, we present an empirical study on the GSM\\nframework for solving single and multiobjective optimization\\nproblems. In the present study, we considered a diverse set of\\nexact interpolating and generalizing approximation techniques\\nfor constructing the local surrogate models, i.e., M1 and\\nM2. These include the interpolating Kriging/Gaussian process\\n(GP), interpolating linear spline radial basis function (RBF)\\nand second-order polynomial regression (PR). For greater\\ndetails on GP, PR, and RBF, the reader is referred to [55]–\\n[57] and Appendix A.\\n\\nA. Parameters of GSM Framework\\n\\nIn this subsection, we discuss the user-specified parameters\\nof the GSM framework. Apart from the parameters of the\\nunderlying SO/MOEA, the generalized framework has three\\nadditional user-specified parameters: m, Gdb, and kterm.\\n\\nSince model accuracy is highly dependent on the sufficiency\\nof the m data points used for model building, the size of\\nnearest neighboring points used (based on Euclidean distance)\\nis defined by d +(d +1)(d +2)/2, where d is the dimensionality\\n\\nof the optimization problem. It is worth noting that the com-\\nplexity for identifying these m points is negligible compared\\nto the cost of surrogate model building. Moreover, since our\\nemphasis here is with regard to a framework that is tailored\\nfor solving computationally expensive problems, i.e., problems\\nthat may cost from minutes to hours of computational time per\\nevaluation, such overheads are considered to be insignificant.\\nFrom these m data points, as many as (d + 1)(d + 2)/2 among\\nthem4 are chosen uniformly as the training data for building\\nthe surrogates, the remaining data points then form the set for\\nvalidating the prediction quality of the surrogate.\\n\\nParameter Gdb, on the other hand, defines the period of the\\ndatabase building phase (refer to lines 3–5 in Algorithms 2\\nand 4) before the core operation of the GSM framework\\nbegins to take effect. Hence, Gdb can be adapted for different\\noptimization problems according to the fulfillment on the\\nrequirement of parameter m. The lower bound of Gdb is\\ndefined by the period to acquire a minimum of m data points\\nfor construction of reliable surrogate models.\\n\\n4This amount corresponds to the minimum number of data points required\\nfor building quadratic regression models.\\n\\n\\n\\nLIM et al.: GENERALIZING SURROGATE-ASSISTED EVOLUTIONARY COMPUTATION 337\\n\\nAlgorithm 4 Generalized Surrogate Multiobjective Memetic Algo-\\nrithm (GS-MOMA)\\n\\n1: initialization: Generate and evaluate an initial population\\nwith Npop individuals, archive all exact evaluations into a\\ndatabase.\\n\\n2: while computational budget is not exhausted do\\n3: if generation count < database building phase (Gdb)\\n\\nthen\\n4: Evolve the population using exact fitness function\\n\\nevaluations, archive all exact evaluations into the\\ndatabase.\\n\\n5: else\\n6: Generate the offspring population, Po using MO evo-\\n\\nlutionary operators (selection, crossover, mutation)\\non the selection pool.\\n\\n7:\\n\\n8: / ∗ ∗ ∗ ∗ Local Search Phase ∗ ∗ ∗ ∗ /\\n\\n9:\\n\\n10: Initialize the learning archive, Al to empty state.\\n11: for each individual x in the offspring population do\\n12: • Generate a random weight vector w =\\n\\n(w1, w2, . . . , wr),\\n∑r\\n\\ni=1 wi = 1 where r is the\\nnumber of objectives.\\n\\n13: • Find m nearest points to x in database as training\\npoints for surrogate models.\\n\\n14: • Build model-1: M1, as an ensemble of all M ′\\nj for\\n\\nj = 1, . . . , n where n is the number of surrogate\\nmodels used of faggr =\\n\\n∑r\\ni=1 wifi(x)\\n\\n15: • Build model-2: M2, which is a low-order PR\\nmodel, of faggr =\\n\\n∑r\\ni=1 wifi(x)\\n\\n16: • Apply local search in M1 to arrive at x1\\nopt, and\\n\\nM2 to arrive at x2\\nopt\\n\\n17: • Replace&Archive( x, x1\\nopt, x2\\n\\nopt, Al )\\n18: end for\\n19:\\n\\n20: / ∗ ∗ End of Local Search Phase ∗ ∗ /\\n\\n21:\\n\\n22:\\n\\n23: / ∗ ∗ ∗ ∗ Selection pool forming ∗ ∗ ∗ ∗ /\\n\\n24:\\n\\n25: Form selection pool, Ps = Pc\\n\\n⋃\\nPo\\n\\n⋃\\nAl.\\n\\n26:\\n\\n27: / ∗ ∗ End of selection pool forming ∗ ∗ /\\n\\n28:\\n\\n29: end if\\n30: end while\\n\\nTheoretically, the trust-region local search scheme termi-\\nnates when the trust-region radius, � approaches ε, where ε\\n\\nrepresents some very small value for termination condition\\n(refer to Section III-E). Nevertheless, for practical reason,\\nunder limited computational budget, it is more appropriate to\\nderive a suitable value for kterm as the termination condition\\nin the trust-region local search. In what follows, we present a\\ntheoretical bound for kterm\\n\\n�1\\nmin (C1)kmin ≤ ε (10)\\n\\nAlgorithm 5 Procedure Replace&Archive(x, x1\\nopt, x2\\n\\nopt, Al)\\n\\n1: if x1\\nopt � x then\\n\\n2: x = x1\\nopt\\n\\n3: if x2\\nopt � x1\\n\\nopt then\\n4: x = x2\\n\\nopt\\n\\n5: else if x2\\nopt ∼ x1\\n\\nopt then\\n6: Archive x2\\n\\nopt in Al\\n\\n7: end if\\n8: else if x2\\n\\nopt � x then\\n9: x = x2\\n\\nopt\\n\\n10: if x2\\nopt ∼ x1\\n\\nopt then\\n11: Archive x1\\n\\nopt in Al\\n\\n12: end if\\n13: else if (x1\\n\\nopt ∼ x) ∧ (x2\\nopt == x) then\\n\\n14: Archive x1\\nopt in Al\\n\\n15: else if (x2\\nopt ∼ x) ∧ (x1\\n\\nopt == x) then\\n16: Archive x2\\n\\nopt in Al\\n\\n17: else if (x1\\nopt ∼ x) ∧ (x2\\n\\nopt ∼ x) then\\n18: if (x1\\n\\nopt � x2\\nopt) ‖ (x1\\n\\nopt == x2\\nopt) then\\n\\n19: Archive x1\\nopt in Al\\n\\n20: else if x2\\nopt � x1\\n\\nopt then\\n21: Archive x2\\n\\nopt in Al\\n\\n22: else\\n23: Archive x1\\n\\nopt and x2\\nopt in Al\\n\\n24: end if\\n25: end if\\n\\n⇒ (C1)kmin ≤ ε\\n\\n�1\\nmin\\n\\n(11)\\n\\n⇒ kmin log C1 ≤ log ε\\n\\n�1\\nmin\\n\\n. (12)\\n\\nSince C1 ∈ (0, 1) → log C1 < 0, we arrive at\\n\\n⇒ kmin ≥\\n(\\n\\nlog\\n(\\n\\nε\\n\\n�1\\nmin\\n\\n))\\n/ (log C1) (13)\\n\\n⇒ kmin ≥ logC1\\n\\n(\\nε\\n\\n�1\\nmin\\n\\n)\\n. (14)\\n\\nSimilarly, the maximum number of trust-region iterations in\\nthe local search, i.e., kmax, is estimated by\\n\\nkmax < Nmax\\nsucc + Nmax\\n\\nsucc logC1\\n\\n(\\nε\\n\\n�1\\nmax\\n\\n)\\n(15)\\n\\n⇒ kmax < Nmax\\nsucc\\n\\n(\\n1 + logC1\\n\\n(\\nε\\n\\n�1\\nmax\\n\\n))\\n. (16)\\n\\nNote that Nmax\\nsucc is the maximum number of successful itera-\\n\\ntions, while �1\\nmin and �1\\n\\nmax are the lower and upper bounds\\nof the initial trust-region radius. In effect, the bounds for kterm\\n\\nas the termination condition can be derived as\\n\\nlogC1\\n\\n(\\nε\\n\\n�1\\nmin\\n\\n)\\n≤ kterm < Nmax\\n\\nsucc\\n\\n(\\n1 + logC1\\n\\n(\\nε\\n\\n�1\\nmax\\n\\n))\\n. (17)\\n\\nIn the trust-region-regulated local search, �1 depends on the\\nlocal region of interest where the initial m nearest neighbors\\nare located. Hence, it is not possible to define this term\\nprecisely for any new optimization problem. For instance, if\\n�1\\n\\nmin ≈ 10ε and C1 = 0.25, we arrive at\\n\\nkterm ≥ log 0.1\\n\\nlog 0.25\\nkterm ≥ 1.66. (18)\\n\\n\\n\\n338 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 14, NO. 3, JUNE 2010\\n\\nTABLE II\\n\\nThe Benchmark Problems Used (F1−F10) for the Empirical\\n\\nStudy of Single-Objective Optimization\\n\\nBenchmark Description Global\\nProblem Optimum\\n\\nf (x∗)\\nF1 Ackley 0.0\\nF2 Griewank 0.0\\nF3 Rosenbrock 0.0\\nF4 Shifted Rotated Rastrigin (F10 in [63]) −330.0\\nF5 Shifted Rotated Weierstrass (F11 in [63]) 90.0\\nF6 Shifted Expanded Griewank −130.0\\n\\nplus Rosenbrock (F13 in [63])\\nF7 Hybrid Composition Function (F15 in\\n\\n[63])\\n120.0\\n\\nF8 Rotated Hybrid Composition Function\\n(F16 in [63])\\n\\n120.0\\n\\nF9 Rotated Hybrid Composition Function 10.0\\nwith Narrow Basin Global Optimum\\n(F19 in [63])\\n\\nF10 Noncontinuous Rotated Hybrid 360.0\\nComposition Function (F23 in [63])\\n\\nTABLE III\\n\\nDefinition of the Single-Objective MAs (SOMAs) Compared\\n\\nAlgorithms Definition\\nGA No surrogate is used\\nSS-SOMA-GP Single surrogate SOMA with M1: GP\\nSS-SOMA-PR Single surrogate SOMA with M1: PR\\nSS-SOMA-RBF Single surrogate SOMA with M1: RBF\\nSS-SOMA-Perfect Single surrogate SOMA with M1: Perfect\\n\\nmodel\\nGS-SOMA Generalized surrogate SOMA with\\n\\nM1: Weighted-average ensemble of GP, PR,\\nand RBF\\nM2: PR\\n\\nAs opposed to using kterm = 1 which translates to a single\\niteration local search, a minimum value of kterm ≥ 2 is more\\npractical to allow the mechanisms of the trust-region-regulated\\nlocal search to take effect.\\n\\nB. Single-Objective Optimization\\n\\nEmpirical study on the GS-SOMA is performed using 10\\nbenchmark problems (F1–F10) reported in [62], [63] and\\nsummarized here in Table II. More detailed description of\\nthe problems is also provided in Appendix B. It consists\\nof problems with diverse properties in terms of separability,\\nmultimodality, and continuity.\\n\\nIn this paper, all the benchmark problems are configured\\nwith a dimensionality of d = 30 for SOO. Performance\\ncomparisons are then made between the GA, SS-SOMA-GP,\\nSS-SOMA-PR, SS-SOMA-RBF, SS-SOMA-Perfect, and GS-\\nSOMA (refer to Table III for the definition of the algorithms\\ninvestigated here). Note that to facilitate a fair comparison,\\nthe surrogate memetic variants are built on top of the same\\nGA used in the study, which ensures that any improvement\\nobserved is a direct contribution of the surrogate framework\\nconsidered. SS-SOMA-XXX refers to the different surrogate-\\nassisted single-objective MA variants, each with a unique\\napproximation method used to generate the surrogate model.\\n\\nTABLE IV\\n\\nSetting of Experiments for GA, SS-SOMA, SS-SOMA-Perfect,\\n\\nand GS-SOMA\\n\\nParameters Setting\\nPopulation size (Npop) 100\\nCrossover probability (Pcross) 0.9\\nMutation probability (Pmut) 0.1\\nMaximum number of exact evaluations 8000\\nEvolutionary operators Uniform crossover\\n\\n& mutation,\\nelitism and ranking selection\\n\\nNumber of trust-region iteration(kterm)\\nfor SS-SOMA and GS-SOMA 3\\nDatabase building phase (Gdb)\\nfor SS-SOMA and GS-SOMA 20\\n(in number of generations)\\nNumber of independent runs 20\\n\\nFor instance, XXX in SS-SOMA-XXX refers to GP, PR, or\\nRBF. On the other hand, SS-SOMA-Perfect refers to an SS-\\nSOMA that employs an imaginary approximation technique\\nthat generates error-free surrogates,5 i.e., RMSE = 0. Hence,\\nthe notion of ‘curse or blessing of uncertainty’ does not exist\\nin the SS-SOMA-Perfect search. As such, any SS-SOMA-\\nXXX that under/out-performs SS-SOMA-Perfect is clearly\\nattributed to the effects of curse and bless of uncertainty,\\nrespectively. Last but not least, GS-SOMA refers to the\\nGeneralized Surrogate framework for single-objective opti-\\nmization. The common parameter settings of the algorithms\\nused in the present experimental study are summarized in\\nTable IV.\\n\\n1) Experimental Results: In Tables V–XIV, the detailed\\nstatistical results of 20 independent runs for SS-SOMAs,\\nGS-SOMA, and GA are presented. The GS-SOMA and best\\nperforming SS-SOMA are highlighted in the tables. Note that\\nnone of the SS-SOMAs always dominates in performance\\non all 10 benchmark problems. This makes good sense since\\nthe performance of any surrogate-assisted evolutionary search\\nwould depend on the match between the characteristics of\\nthe problem landscape and approximation scheme used. For\\ninstance, in the tables, it is shown that SS-SOMA-PR serves to\\nbe best suited for F1, F5, and F9 since it outperforms all other\\nalgorithms on these problems. Similarly, this also applies\\nto SS-SOMA-GP which excels on F3. On the other hand,\\nSS-SOMA-RBF, though not superior, performs relatively well\\non F3, F4, F7, and F8. Moreover, it is worth noting that the\\nSS-SOMAs are observed to have performed much poorly on\\nseveral occasions. For instance, SS-SOMA-PR fares badly on\\nF3, F4, F7, and F8. The same is true for SS-SOMA-GP on\\nF1, F4–F8, and F10, and SS-SOMA-RBF on F1, F2, F5, F6,\\nF9, and F10.\\n\\nOn the other hand, the results in Tables V–XIV, indicate that\\nGS-SOMA consistently performs well on all the benchmark\\nproblems. The t-test results, i.e., at 95% confidence level, for\\nthe different algorithms as reported in Table XV confirm that\\n\\n5An error-free surrogate model can be realized by using exact fitness\\nfunction in the portion of SS-SOMA where a surrogate model should be used,\\nbut the incurred fitness evaluation is counted only as many as in SS-SOMA.\\n\\n\\n\\nLIM et al.: GENERALIZING SURROGATE-ASSISTED EVOLUTIONARY COMPUTATION 339\\n\\nFig. 4. Convergence trends for F1–F10 obtained from GS-SOMA, SS-SOMA-Perfect, and SS-SOMA-AV.\\n\\n\\n\\n340 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 14, NO. 3, JUNE 2010\\n\\nTABLE V\\n\\nStatistics of the Final Solution Quality at the End of 8000 Exact Function Evaluations for F1 Using GA, SS-SOMA-GP,\\n\\nSS-SOMA-PR, SS-SOMA-RBF, and GS-SOMA\\n\\nOptimization Statistical Values\\nAlgorithm Mean Std. Dev. Median Best Worst\\nGA 1.24e+01 9.50e−01 1.23e+01 1.12e+01 1.42e+01\\nSS-SOMA-GP 6.43e+00 9.73e−01 3.98e+00 2.87e+00 1.56e+01\\nSS-SOMA-PR 1.39e+00 1.93e−01 1.36e+00 1.14e+00 1.75e+00\\nSS-SOMA-RBF 4.91e+00 7.57e−01 4.86e+00 3.78e+00 6.09e+00\\nGS-SOMA 3.58e+00 5.09e−01 3.67e+00 2.87e+00 4.28e+00\\n\\nTABLE VI\\n\\nStatistics of the Final Solution Quality at the End of 8000 Exact Function Evaluations for F2 Using GA, SS-SOMA-GP,\\n\\nSS-SOMA-PR, SS-SOMA-RBF, and GS-SOMA\\n\\nOptimization Statistical Values\\nAlgorithm Mean Std. Dev. Median Best Worst\\nGA 4.58e+01 8.61e+00 4.67e+01 2.15e+01 6.19e+01\\nSS-SOMA-GP 1.79e+01 8.58e+00 1.07e+01 5.15e−09 3.00e+01\\nSS-SOMA-PR 1.18e−02 2.78e−02 4.29e−08 7.48e−10 1.19e−01\\nSS-SOMA-RBF 7.49e−01 8.98e−02 7.51e−01 6.02e−01 8.72e−01\\nGS-SOMA 2.2e−03 4.60e−03 8.95e−09 1.40e−10 1.54e−02\\n\\nTABLE VII\\n\\nStatistics of the Final Solution Quality at the End of 8000 Exact Function Evaluations for F3 Using GA, SS-SOMA-GP,\\n\\nSS-SOMA-PR, SS-SOMA-RBF, and GS-SOMA\\n\\nOptimization Statistical Values\\nAlgorithm Mean Std. Dev. Median Best Worst\\nGA 4.10e+02 1.01e+02 3.85e+02 2.33e+02 5.73e+02\\nSS-SOMA-GP 2.99e+01 7.73e−01 3.00e+01 2.87e+01 3.11e+01\\nSS-SOMA-PR 6.73e+01 2.55e+01 5.62e+01 3.72e+01 1.04e+02\\nSS-SOMA-RBF 4.90e+01 2.92e+01 3.97e+01 2.92e+01 1.57e+02\\nGS-SOMA 4.63e+01 2.92e+01 3.02e+01 2.83e+01 1.26e+02\\n\\nTABLE VIII\\n\\nStatistics of the Final Solution Quality at the End of 8000 Exact Function Evaluations for F4 Using GA, SS-SOMA-GP,\\n\\nSS-SOMA-PR, SS-SOMA-RBF, and GS-SOMA\\n\\nOptimization Statistical Values\\nAlgorithm Mean Std. Dev. Median Best Worst\\nGA −5.46e+01 3.01e+01 −5.48e+01 −1.11e+02 5.19e−01\\nSS-SOMA-GP −1.19e+02 1.87e+01 −1.17e+02 −1.50e+02 −8.71e+01\\nSS-SOMA-PR −1.19e+02 1.23e+01 −1.21e+02 −1.43e+02 −9.01e+01\\nSS-SOMA-RBF −1.65e+02 1.86e+01 −1.66e+02 −1.91e+02 −1.36e+02\\nGS-SOMA −1.26e+02 1.60e+01 −1.23e+02 −1.64e+02 −9.97e+01\\n\\nTABLE IX\\n\\nStatistics of the Final Solution Quality at the End of 8000 Exact Function Evaluations for F5 Using GA, SS-SOMA-GP,\\n\\nSS-SOMA-PR, SS-SOMA-RBF, and GS-SOMA\\n\\nOptimization Statistical Values\\nAlgorithm Mean Std. Dev. Median Best Worst\\nGA 1.26e+02 2.85e+00 1.26e+02 1.20e+02 1.32e+02\\nSS-SOMA-GP 1.19e+02 4.29e+00 1.20e+02 1.12e+02 1.25e+02\\nSS-SOMA-PR 1.16e+02 3.79e+00 1.16e+02 1.13e+02 1.25e+02\\nSS-SOMA-RBF 1.21e+02 2.61e+00 1.21e+02 1.18e+02 1.24e+02\\nGS-SOMA 1.19e+02 3.05e+00 1.19e+02 1.14e+02 1.24e+02\\n\\n\\n\\nLIM et al.: GENERALIZING SURROGATE-ASSISTED EVOLUTIONARY COMPUTATION 341\\n\\nTABLE X\\n\\nStatistics of the Final Solution Quality at the End of 8000 Exact Function Evaluations for F6 Using GA, SS-SOMA-GP,\\n\\nSS-SOMA-PR, SS-SOMA-RBF, and GS-SOMA\\n\\nOptimization Statistical Values\\nAlgorithm Mean Std. Dev. Median Best Worst\\nGA −9.57e+01 9.43e+00 −9.79e+01 −1.06e+02 −7.28e+01\\nSS-SOMA-GP −1.02e+02 2.99e+00 −1.03e+02 −1.05e+02 −9.74e+02\\nSS-SOMA-PR −1.06e+02 2.45e+00 −1.07e+02 −1.09e+02 −1.02e+02\\nSS-SOMA-RBF −1.03e+02 2.43e+00 −1.03e+02 −1.07e+02 −9.96e+01\\nGS-SOMA −1.12e+02 1.05e+00 −1.23e+02 −1.13e+02 −1.11e+02\\n\\nTABLE XI\\n\\nStatistics of the Final Solution Quality at the End of 8000 Exact Function Evaluations for F7 Using GA, SS-SOMA-GP,\\n\\nSS-SOMA-PR, SS-SOMA-RBF, and GS-SOMA\\n\\nOptimization Statistical Values\\nAlgorithm Mean Std. Dev. Median Best Worst\\nGA 7.29e+02 5.92e+01 7.27e+02 6.43e+02 8.21e+02\\nSS-SOMA-GP 6.81e+02 7.23e+01 6.95e+02 6.02e+02 8.23e+02\\nSS-SOMA-PR 6.42e+02 5.80e+01 6.34e+02 5.73e+02 7.09e+02\\nSS-SOMA-RBF 6.27e+02 7.93e+01 5.99e+02 5.95e+02 8.49e+02\\nGS-SOMA 6.07e+02 3.06e+01 6.00e+02 5.79e+02 6.59e+02\\n\\nTABLE XII\\n\\nStatistics of the Final Solution Quality at the End of 8000 Exact Function Evaluations for F8 Using GA, SS-SOMA-GP,\\n\\nSS-SOMA-PR, SS-SOMA-RBF, and GS-SOMA\\n\\nOptimization Statistical Values\\nAlgorithm Mean Std. Dev. Median Best Worst\\nGA 4.83e+02 6.3e+01 4.62e+02 4.19e+02 6.06e+02\\nSS-SOMA-GP 4.52e+02 9.66e+01 4.35e+02 3.40e+02 5.63e+02\\nSS-SOMA-PR 3.94e+02 4.41e+01 3.75e+02 3.43e+02 4.52e+02\\nSS-SOMA-RBF 3.79e+02 3.3e+01 3.69e+02 3.51e+02 4.41e+02\\nGS-SOMA 3.25e+02 1.17e+02 2.86e+02 2.32e+02 5.54e+02\\n\\nTABLE XIII\\n\\nStatistics of the Final Solution Quality at the End of 8000 Exact Function Evaluations for F9 Using GA, SS-SOMA-GP,\\n\\nSS-SOMA-PR, SS-SOMA-RBF, and GS-SOMA\\n\\nOptimization Statistical Values\\nAlgorithm Mean Std. Dev. Median Best Worst\\nGA 1.02e+03 2.35e+01 1.02e+03 9.86e+02 1.08e+03\\nSS-SOMA-GP 9.42e+02 1.71e+01 9.37e+02 9.25e+02 9.81e+02\\nSS-SOMA-PR 9.32e+02 8.26e+00 9.31e+02 9.22e+02 9.48e+02\\nSS-SOMA-RBF 9.81e+02 1.43e+01 9.80e+02 9.67e+02 1.00e+03\\nGS-SOMA 9.42e+02 1.75e+01 9.37e+02 9.30e+02 9.86e+02\\n\\nTABLE XIV\\n\\nStatistics of the Final Solution Quality at the End of 8000 Exact Function Evaluations for F10 Using GA, SS-SOMA-GP,\\n\\nSS-SOMA-PR, SS-SOMA-RBF, and GS-SOMA\\n\\nOptimization Statistical Values\\nAlgorithm Mean Std. Dev. Median Best Worst\\nGA 1.51e+03 5.52e+01 1.52e+03 1.40e+03 1.58e+03\\nSS-SOMA-GP 1.26e+03 1.88e+02 1.22e+03 1.03e+03 1.54e+03\\nSS-SOMA-PR 1.07e+03 1.07e+02 1.04e+03 9.42e+02 1.29e+03\\nSS-SOMA-RBF 1.12e+03 1.16e+02 1.15e+03 9.59e+02 1.28e+03\\nGS-SOMA 1.01e+03 7.85e+01 9.53e+02 9.09e+02 1.51e+03\\n\\n\\n\\n342 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 14, NO. 3, JUNE 2010\\n\\nTABLE XV\\n\\nResult of t-Test With 95% Confidence Level Comparing Statistical Values for GS-SOMA and Those of SS-SOMA-GP, SS-SOMA-PR,\\n\\nSS-SOMA-RBF, SS-SOMA-Perfect on F1−F10 (s+, s−, and ≈ Indicates That GS-SOMA is Significantly Better, Significantly Worse,\\n\\nand Indifferent, Respectively)\\n\\nGA SS-SOMA-GP SS-SOMA-PR SS-SOMA-RBF SS-SOMA-Perfect\\nF1 s+ s+ s− s+ s+\\nF2 s+ s+ ≈ s+ s−\\nF3 s+ s− s+ ≈ s−\\nF4 s+ ≈ ≈ s− s+\\nF5 s+ ≈ ≈ s+ s+\\nF6 s+ s+ s+ s+ s+\\nF7 s+ s+ s+ ≈ s+\\nF8 s+ s+ s+ ≈ s+\\nF9 s+ ≈ s− s+ s+\\nF10 s+ s+ ≈ s+ s+\\n\\nGS-SOMA outperforms or is competitive to the SS-SOMAs\\non 44/50 cases. On the remaining six cases, GS-SOMA also\\ndisplays solution qualities close to that of the superior SS-\\nSOMA, see the highlighted results in Tables V–XIV. Note\\nthat this is a significant achievement considering that no a\\npriori knowledge is available to select an appropriate surrogate\\nmodeling scheme for the problems considered. This highlights\\nthe reliability of the generalized framework.\\n\\nThe search convergence trends of GS-SOMA, SS-SOMA-\\nAV, and SS-SOMA-Perfect are also plotted in Fig. 4. Note\\nthat SS-SOMA-AV represents the estimated performance one\\nmight expect to get when an approximation technique is\\nrandomly chosen for use. Hence, SS-SOMA-AV is generated\\nfrom the average of the results obtained by all three SS-\\nSOMAs, i.e., SS-SOMA-GP, SS-SOMA-PR, and SS-SOMA-\\nRBF. It is evident from the search convergence trends that GS-\\nSOMA is superior over SS-SOMA-AV on the 10 benchmark\\nproblems. This indicates that the generalized framework is\\nmore reliable when one has no knowledge about the suitability\\nof the approximation scheme for the problem at hand.\\n\\n2) Analyzing the Generalized Evolutionary Framework in\\nSingle-Objective Optimization: To gain a better understanding\\nof the generalized framework, we further analyze the reliability\\nand effectiveness of the ensemble (M1) and smoothing (M2)\\nsurrogate models in contributing to the evolutionary search.\\n\\nTo facilitate the analysis, the normalized root mean square\\nerrors (N-RMSE) of fitness predictions based on the ensemble\\nsurrogate model, i.e., M1 in GS-SOMA search, for the bench-\\nmark problems are presented in Fig. 5. The N-RMSE of model\\ni is determined as follows:\\n\\nNormalized RMSEi =\\nRMSEi∑n\\nj=1 RMSEj\\n\\n(19)\\n\\nwhere n is the total approximation methods used in shaping the\\nensemble. From this figure, the consistently low N-RMSE of\\nthe ensemble model generated in the GS-SOMA search across\\nall benchmark problems demonstrates the high reliability of\\nthe fitness prediction generated by M1 across the different\\noptimization problems over any single surrogates.\\n\\nFurther, it is worth noting that the use of M2 contributes\\nto the fitness improvement in GS-SOMA, which confirms\\n\\nFig. 5. Normalized RMSE by GP, PR, RBF, and weighted average ensemble.\\n\\nFig. 6. Normalized fitness improvement during the runs of GS-SOMA\\ncontributed by M1 (ImpM1\\n\\n) and M2 (ImpM2\\n).\\n\\n\\n\\nLIM et al.: GENERALIZING SURROGATE-ASSISTED EVOLUTIONARY COMPUTATION 343\\n\\nFig. 7. Normalized RMSE by GP, PR, RBF, and weighted average ensemble\\non MF1–MF6.\\n\\nthe possible benefits of bless of uncertainty in surrogate\\nmodel. The normalized average fitness improvement of the\\nlocal searches contributed via the use of M1 (ImpM1\\n\\n) and M2\\n\\n(ImpM2\\n) during the GS-SOMA searches are summarized in\\n\\nFig. 6 and is defined by\\n\\nNormalized ImpM1\\n=\\n\\nImpM1\\n\\nImpM1\\n+ ImpM2\\n\\nNormalized ImpM2\\n=\\n\\nImpM2\\n\\nImpM1\\n+ ImpM2\\n\\n. (20)\\n\\nImpM1\\nis the total fitness improvements attained by local\\n\\nrefinements, i.e., through Lamarckian learning, when f (x1\\nopt) <\\n\\nf (x2\\nopt), while ImpM2\\n\\nis the total fitness improvements when\\nf (x2\\n\\nopt) < f (x1\\nopt).\\n\\nFrom the statistical results given in Fig. 6, it is notable\\nthat M1 and M2 surrogates have contributed to the surrogate-\\nassisted memetic search in their unique ways. This provides a\\nmeans for explaining the results that were obtained in Fig. 4\\nand Tables V–XIV. In particular, the reason for the fact that\\nall surrogate-assisted SOMAs outperform SS-SOMA-Perfect\\non F1 (Ackley) suggests the presence of ‘bless of uncertainty’\\nthrough the use of surrogate(s), since the notion of ‘curse or\\nbless of uncertainty’ cannot exist in the latter. Further, the\\nfact that SS-SOMA-PR is the most superior on F1 (Ackley)\\nhighlights the strength of the PR model in contributing to\\nthe search via smoothing the rugged landscape of the Ackley\\nfunction. This hypothesis is clearly supported by the large\\nportion of fitness improvements that are contributed by M2\\n(i.e., the PR model) on F1, see Fig. 6. On the other hand,\\nneither SS-SOMAs nor GS-SOMA manage to outperform\\nthe SS-SOMA-Perfect on F3(Rosenbrock), suggesting the\\npresence of ‘curse of uncertainty’ due to the surrogate(s).\\nFurther, the results in F3 of Fig. 6 also indicate that M2\\n\\n(i.e., the smoothing PR model) did not contribute significantly\\nto the search since the problem landscape of this function\\nis originally smooth. Rather, the use of ensemble model in\\nGS-SOMA had contributed to reliable fitness improvement\\n\\nFig. 8. Archiving to replacement ratio of GS-MOMA on MF1–MF6.\\n\\non F3(Rosenbrock) by generating reliable prediction accuracy.\\nOn the other test problems, both M1 and M2 surrogates were\\nshown to contribute significantly to GS-SOMA in their own\\nunique ways.\\n\\nC. Multiobjective Optimization\\n\\nIn this subsection, we present the empirical study of the GS-\\nMOMA on six moderate- to high-dimensional MO benchmark\\nproblems, labeled here as MF1–MF6 [64]. The MO benchmark\\nproblems used in the study are summarized in Table XVI.\\n\\nPerformance comparisons are then made between the stan-\\ndard nondominated sorting genetic algorithm-II (NSGA-II)\\n[65] and variants of MOMA. For fair comparison, we compare\\nGS-MOMA with several SS-MOMAs and the NSGA-II since\\nthe formers are demonstrated with NSGA-II as the baseline by\\nbuilding on top of it. Hence, all algorithms compared inherit\\nthe same evolutionary operators as the NSGA-II used in our\\nexperiment. In SS-MOMAs, an offspring will be replaced in\\nthe spirit of Lamarckian learning during local search if its ag-\\ngregated fitness function is found to be better than the original\\noffspring. Similarly, SS-MOMA-Perfect is introduced here to\\nassess the effects of approximation error on surrogate-assisted\\nevolutionary search performance. For the sake of brevity, the\\nnotations and definitions of the MO algorithms studied are\\ntabulated in Table XVII while the common parameter settings\\nof the MO algorithms used in the experimental study are\\ndefined in Table XVIII.6\\n\\nMany performance indicators exist for assessing the perfor-\\nmance of MOEAs, such as those summarized in [66], [67].\\nHere, the following three performance indicators are used.\\n\\n1) Generational Distance (GD) [68], [69]: This measure-\\nment indicates the gap between the true Pareto front\\n(PF ∗) and the evolved Pareto front (PF ). Mathemati-\\n\\n6Since MF3 and MF4 have higher dimensionality, i.e., d = 50, greater initial\\ndatabase size is required. For these cases, Gdb is set to 20.\\n\\n\\n\\n344 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 14, NO. 3, JUNE 2010\\n\\nTABLE XVI\\n\\nMultiobjective Benchmark Problems (MF1−MF6). Parametric Domain Used is [0, 1]d , Where d is the Problem Dimensionality\\n\\nConsidered in This Paper\\n\\nBenchmark Formulation Characteristics\\nFunction\\nMF1 (d = 30) f1(x) = x1 Convex, 2-objective Pareto front\\n\\nf2(x) = g(x)[1 −\\n√\\n\\nf1(x)/g(x)]\\n\\ng(x) = 1 + 9(\\n∑d\\n\\ni=2 xi)/(d − 1)\\n\\nMF2 (d = 30) f1(x) = x1 Nonconvex, 2-objective Pareto front\\n\\nf2(x) = g(x)[1 − f1(x)/g(x)2]\\n\\ng(x) = 1 + 9(\\n∑d\\n\\ni=2 xi)/(d − 1)\\n\\nMF3 (d = 50) f1(x) = x1 Convex, disconnected, 2-objective Pareto front\\n\\nf2(x) = g(x)[1 −\\n√\\n\\nf1/g − (f1/g)sin(10πf1)]\\n\\ng(x) = 1 + 9(\\n∑d\\n\\ni=2 xi)/(d − 1)\\n\\nMF4 (d = 50) f1(x) = 1 − exp(−4x1)sin6(6πx1) Nonconvex, 2-objective Pareto front\\n\\nf2(x) = g(x)[1 − (f1(x)/g(x))2]\\n\\ng(x) = 1 + 9[\\n∑d\\n\\ni=2 xi/(d − 1)]0.25\\n\\nMF5 (d = 20) f1(x) = cos( π\\n2 x1)cos( π\\n\\n2 x2)(1 + g(x)) Nonconvex, 3-objective, Pareto front\\n\\nf2(x) = cos( π\\n2 x1)sin( π\\n\\n2 x2)(1 + g(x))\\n\\nf3(x) = cos( π\\n2 x1)(1 + g(x))\\n\\ng(x) =\\n∑d\\n\\ni=3(xi − x1)2\\n\\nMF6 (d = 10) f1(x) = x1 Convex, 2-objective, multiple local Pareto front\\n\\nf2(x) = g(x)[1 −\\n√\\n\\nf1(x)/g(x)]\\n\\ng(x) = 1 + 10(d − 1) +\\n∑d\\n\\ni=2(x2\\ni − 10 cos(4πxi))\\n\\ncally, it can be formulated as\\n\\nGD =\\n1\\n\\nnPF\\n\\n√√√√ nPF∑\\ni=1\\n\\ndi\\n2 (21)\\n\\nwhere nPF is the number of members in PF , di is\\nthe Euclidean distance (in objective space) between\\nmember i of PF and its nearest member in PF ∗. A low\\nvalue of GD is more desirable since it reflects a good\\nconvergence to the true Pareto fronts.\\n\\n2) Maximum Spread (MS) [70]: It is used to measure\\nhow well the true Pareto front (PF ∗) is covered by the\\nevolved Pareto front (PF ). The MS measurement used\\nin this paper is formulated as\\n\\nMS =\\n\\n√√√√1\\n\\nr\\n\\nr∑\\ni=1\\n\\n[\\nmin(f max\\n\\ni , Fmax\\ni ) − max(f min\\n\\ni , Fmin\\ni )\\n\\nFmax\\ni − Fmin\\n\\ni\\n\\n]2\\n\\n(22)\\n\\nwhere f max\\ni and f min\\n\\ni are the maximum and minimum\\nof the ith objective in the evolved PF , respectively.\\nFmax\\n\\ni and Fmin\\ni are the maximum and minimum of the\\n\\nith objective in PF ∗, respectively. Higher value of MS\\n\\nreflects a larger area of PF ∗ covered by PF , which is\\ndesirable.\\n\\n3) Hypervolume Ratio (HR) [69]: This indicates the ratio\\nbetween the hyperarea or hypervolume (H) [71] domi-\\nnated by the evolved PF and PF ∗, where HR is defined\\nas\\n\\nHR =\\nH(PF )\\n\\nH(PF ∗)\\n\\nH = volume\\n(⋃nPF\\n\\ni=1 vi\\n\\n)\\n. (23)\\n\\nHere, vi denotes the hypercube constructed from mem-\\nber i of a particular Pareto front and the reference point.\\nA HR value close to 1 indicates that the evolved Pareto\\nfront is quite close to the true Pareto front, in both\\nconvergence and spread of solutions.\\n\\n1) Experimental Results: The obtained Pareto fronts of the\\nbenchmark problems for 20 independent runs are combined\\nand depicted in Figs. 9–14. The respective performance met-\\nrics are then summarized in Figs. 15–20. From these results, all\\nsurrogate-assisted multiobjective EAs, i.e., SS-MOMAs and\\nGS-MOMA, are shown to outperform the standard NSGA-\\nII on MF1, MF2, MF5, and MF6. MF6 (ZDT4) is generally\\nregarded as a challenging problem and hence commonly used\\n\\n\\n\\nLIM et al.: GENERALIZING SURROGATE-ASSISTED EVOLUTIONARY COMPUTATION 345\\n\\nFig. 9. Pareto front evolved for benchmark problem MF1 in (a) NSGA-II, (b) GS-MOMA, (c) SS-MOMA-I, (d) SS-MOMA-II, and (e) SS-MOMA-Perfect.\\n\\nFig. 10. Pareto front evolved for benchmark problem MF2 in (a) NSGA-II, (b) GS-MOMA, (c) SS-MOMA-I, (d) SS-MOMA-II, and (e) SS-MOMA-Perfect.\\n\\n\\n\\n346 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 14, NO. 3, JUNE 2010\\n\\nTABLE XVII\\n\\nDefinition of the Multiobjective MAs (MOMAs) Compared\\n\\nAlgorithms Definition\\nNSGA-II No surrogate is used\\nGS-MOMA Generalized surrogate MOMA with\\n\\nM1: Weighted-average ensemble of GP, PR, and\\nRBF\\nM2: PR\\n\\nSS-MOMA-I Single surrogate MOMA with\\nM1: Ensemble of GP, PR, and RBF\\n\\nSS-MOMA-II Single surrogate MOMA with\\nM1: PR\\n\\nSS-MOMA-Perfect Single surrogate MOMA with\\nM1: Perfect model\\n\\nTABLE XVIII\\n\\nSetting of Experiments for NSGA-II, GS-MOMA, and SS-MOMA\\n\\nParameters Setting\\nPopulation size (Npop) 100\\nCrossover probability (Pcross) 0.9\\nMutation probability (Pmut) 0.1\\nMaximum number of exact evaluations MF1–MF2: 8000\\n\\nMF3–MF4: 16 000\\nMF5: 30 000\\nMF6: 20 000\\n\\nEvolutionary operators Simulated binary crossover,\\npolynomial mutation,\\nbinary tournament selection,\\nelitism, nondomination rank,\\nand crowded distance\\n\\nNumber of trust-region iteration (kterm) 2\\nfor SS-MOMA and GS-MOMA\\nDatabase building phase (Gdb) MF1–MF2, MF5–MF6: 10\\nfor SS-MOMA and GS-MOMA MF3–MF4: 20\\n(in number of generations)\\nNumber of independent runs 20\\n\\nTABLE XIX\\n\\nRadial Basis Kernels\\n\\nLinear splines ||x − ci||\\nThin plate splines ||x − ci||kln||x − ci||\\nCubic splines ||x − ci||3\\nGaussian exp− ||x−ci||2\\n\\nβi\\n\\nMultiquadrics\\n\\n√\\n1 + ||x−ci||2\\n\\nβi\\n\\nInverse multiquadrics (1 + ||x−ci||2\\nβi\\n\\n)−\\n1\\n2\\n\\nby many in the literature. Here, we validate our results on\\nZDT4 against those obtained by Deb et al. [28]. While\\n[28] reported to solve ZDT4 with from 21 781 to 22 730\\nexact function evaluations with an achieved spread measure7\\n\\nof 0.332 to 0.422, GS-MOMA requires only 20 000 exact\\nevaluations at a competitive spread measure of 0.410 ± 0.046.\\nOn MF3 and MF4, some SS-MOMAs perform competitively\\nor slightly poorer than NSGA-II [see Figs. 11(d) and 12(d)].\\nOn the other hand, GS-MOMA searches more efficiently than\\nall the SS-MOMA variants and NSGA-II on the six benchmark\\n\\n7The spread metric [72] considers the distance between two extreme ends\\nof Pareto front as well as the uniformity of distribution for solutions between\\nthe two extremes. This metric may be used for measuring the diversity of\\nconverged Pareto fronts. Note that a lower spread metric is desirable.\\n\\nproblems considered. Note that GS-MOMA also outperforms\\nthe SS-MOMA-Perfect on a majority of the MOO benchmarks\\nwith respect to all three performance metrics, thus suggesting\\nthe positive synergy of the ensemble and smoothing surrogate\\nmodels in the GSM framework.\\n\\n2) Analyzing the Generalized Evolutionary Framework in\\nMultiobjective Optimization: To arrive at better understanding\\nof the generalized framework in the context of multiobjective\\noptimization, we analyze next the reliability and effectiveness\\nof the ensemble (M1) and smoothing (M2) surrogate models\\nin contributing to evolutionary search.\\n\\nThe N-RMSE, i.e., see (19), of fitness predictions based on\\nGP, PR, RBF, or ensemble in GS-MOMA is summarized in\\nFig. 7. From the results, the ensemble model M1 is shown to\\narrive at low N-RMSE on all the multiobjective test problems\\nconsidered, which is consistent with observations obtained\\nin the single-objective context. M1 generates high-reliability\\npredictions in comparison to the other single surrogate model\\ncounterparts, i.e., GP, PR or RBF.\\n\\nBesides N-RMSE, the solution archiving to replacement\\nratio, labeled here as �, of the GS-MOMA search is also\\nreported in Fig. 8. � indicates the degree of solution diversity\\n(through archival of new nondominating solutions) against\\nsearch convergence (through the process of Lamarckian learn-\\ning replacement) in the GS-MOMA search. While Lamarckian\\nlearning helps to speedup convergence toward the desired\\nPareto front, the large � ratio observed on all benchmark prob-\\nlems implies frequent discovery of potential nondominating\\nsolutions when using both M1 and M2 with local refinements.\\nThis suggests ‘bless of uncertainty’ may take the form of faster\\nsearch convergence and better solution diversity in the context\\nof multiobjective evolutionary search.\\n\\nD. Computational Complexity of GSM Framework\\n\\nIn this subsection, we present an analytical study on the\\ncomputational complexity of the GSM framework. The com-\\nputational effort, referred here by Tcomp, of GS-SOMA or\\nGS-MOMA is formulated as follows:\\n\\nTcomp = GdbNpop\\n∑r\\n\\ni=1 Fi + (Gmax − Gdb)\\n\\n[Npop(Tens + TPR + 2kterm\\n∑r\\n\\ni=1 Fi + Toverhead)] (24)\\n\\nwhere\\n\\nGdb number of standard SO/MOEA search gen-\\nerations configured for building the database\\nof training data points at the initial search\\nphase of the GSM framework;\\n\\nGmax maximum number of search generations;\\nNpop population size;\\nr number of objectives to optimize;\\nkterm number of iterations made in the trust-\\n\\nregion-regulated local searches;\\nF original/exact function evaluation cost;\\nTens time to build M1, i.e., the ensemble model;\\nTPR time to build M2, i.e., the polynomial regres-\\n\\nsion model, which is not applicable if PR is\\nalready built when constructing M1;\\n\\n\\n\\nLIM et al.: GENERALIZING SURROGATE-ASSISTED EVOLUTIONARY COMPUTATION 347\\n\\nFig. 11. Pareto front evolved for benchmark problem MF3 in (a) NSGA-II, (b) GS-MOMA, (c) SS-MOMA-I, (d) SS-MOMA-II, and (e) SS-MOMA-Perfect.\\n\\nFig. 12. Pareto front evolved for benchmark problem MF4 in (a) NSGA-II, (b) GS-MOMA, (c) SS-MOMA-I, (d) SS-MOMA-II, and (e) SS-MOMA-Perfect.\\n\\n\\n\\n348 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 14, NO. 3, JUNE 2010\\n\\nFig. 13. Pareto front evolved for benchmark problem MF5 in (a) NSGA-II, (b) GS-MOMA, (c) SS-MOMA-I, (d) SS-MOMA-II, and (e) SS-MOMA-Perfect.\\n\\nFig. 14. Pareto front evolved for benchmark problem MF6 in (a) NSGA-II, (b) GS-MOMA, (c) SS-MOMA-I, (d) SS-MOMA-II, and (e) SS-MOMA-Perfect.\\n\\n\\n\\nLIM et al.: GENERALIZING SURROGATE-ASSISTED EVOLUTIONARY COMPUTATION 349\\n\\nFig. 15. Performance metrics for benchmark problem MF1. (a) Generational distance (GD). (b) Maximum spread (MS). (c) Hypervolume ratio (HR).\\n(A:NSGA-II, B:GS-MOMA, C:SS-MOMA-I, D:SS-MOMA-II, E:SS-MOMA-Perfect.)\\n\\nFig. 16. Performance metrics for benchmark problem MF2. (a) Generational distance (GD). (b) Maximum spread (MS). (c) Hypervolume ratio (HR).\\n(A:NSGA-II, B:GS-MOMA, C:SS-MOMA-I, D:SS-MOMA-II, E:SS-MOMA-Perfect.)\\n\\nFig. 17. Performance metrics for benchmark problem MF3. (a) Generational distance (GD). (b) Maximum spread (MS). (c) Hypervolume ratio (HR).\\n(A:NSGA-II, B:GS-MOMA, C:SS-MOMA-I, D:SS-MOMA-II, E:SS-MOMA-Perfect.)\\n\\nFig. 18. Performance metrics for benchmark problem MF4. (a) Generational distance (GD). (b) Maximum spread (MS). (c) Hypervolume ratio (HR).\\n(A:NSGA-II, B:GS-MOMA, C:SS-MOMA-I, D:SS-MOMA-II, E:SS-MOMA-Perfect.)\\n\\n\\n\\n350 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 14, NO. 3, JUNE 2010\\n\\nFig. 19. Performance metrics for benchmark problem MF5. (a) Generational distance (GD). (b) Maximum spread (MS). (c) Hypervolume ratio (HR).\\n(A:NSGA-II, B:GS-MOMA, C:SS-MOMA-I, D:SS-MOMA-II, E:SS-MOMA-Perfect.)\\n\\nFig. 20. Performance metrics for benchmark problem MF6. (a) Generational distance (GD). (b) Maximum spread (MS). (c) Hypervolume ratio (HR).\\n(A:NSGA-II, B:GS-MOMA, C:SS-MOMA-I, D:SS-MOMA-II, E:SS-MOMA-Perfect.)\\n\\nToverhead other additional costs such as for fitness\\npredictions and finding nearest points, which\\nare often negligible.\\n\\nOn the other hand, the computational cost for SS-SOMA or\\nSS-MOMA variants is\\n\\nTcomp = GdbNpop\\n∑r\\n\\ni=1 Fi + (Gmax − Gdb)\\n\\n[Npop(Tm + kterm\\n∑r\\n\\ni=1 Fi + Toverhead)] (25)\\n\\nwhere Tm is the time taken to build the particular surrogate\\nmodel used.\\n\\nAlthough there are several elements in (24) and (25), it\\nis worth noting that when working with computationally\\nexpensive problems, the most significant part contributing to\\nthe total computational effort incurred is F . Hence, when F\\n\\nis significantly large, which is assumed to be fulfilled in any\\nsurrogate-assisted optimization framework, Tens, TPR, Toverhead\\n\\nand Tm are generally considered to be negligible, otherwise\\nsuch frameworks should never be used.\\n\\nV. Conclusion\\n\\nWith a plethora of approximation/surrogate modeling ap-\\nproaches available in the literature, the choice of technique\\nto use greatly affects the performance of surrogate-assisted\\nevolutionary searches. It is argued that every approximation\\n\\ntechnique introduces some unique characteristics suitable for\\nmodeling some classes of problems accurately but not for\\nothers. Given that a priori knowledge about the problem land-\\nscape is often scarce, the ability to tackle new problems in a\\nreliable way is of significant value. This paper has investigated\\na generalized framework that unifies diverse surrogate models\\nsynergistically in the memetic evolutionary search. In contrast\\nto existing studies, the studied memetic framework empha-\\nsizes not only on 1) mitigating the impact of ‘curse of un-\\ncertainty’ robustly, but also 2) benefitting from the ‘bless\\nof uncertainty,’ through the use of ensemble and landscape\\nsmoothing surrogate models, respectively.\\n\\nThe core purpose of proposing any new search strate-\\ngies, including the GSM framework, is to solve real-world\\noptimization problems more robustly, effectively and/or effi-\\nciently. Hence, to facilitate possible systematic study and gain\\ndeeper understanding of the proposed methods for solving\\ncomplex real-world problems plagued with computationally\\nexpensive functions, benchmark problems of diverse known\\nproperties have been employed. In this paper, we have pre-\\nsented extensive numerical studies on commonly used single/\\nmultiobjective optimization benchmark problems which have\\ndemonstrated the competitiveness of the generalized frame-\\nwork. Overall, the ensemble model is shown to be capable of\\nattaining reliable, accurate surrogate models, while smoothing\\nmodel speeds up evolutionary search performance by travers-\\ning through the multimodal landscape of complex problems.\\n\\n\\n\\nLIM et al.: GENERALIZING SURROGATE-ASSISTED EVOLUTIONARY COMPUTATION 351\\n\\nStatistically, the generalized framework achieved significantly\\nbetter performance on SOO/MOO when compared to SS-\\nSOMA/MOMA and their underlying SO/MOEA.\\n\\nPresently, the GSM framework is used for solving real-\\nworld problems plagued with computationally expensive func-\\ntions, particularly in the field of aerodynamic and molecular\\nstructural designs. Based on our experiences with both bench-\\nmark and real-world problems that range from turbine blade\\n[7], [20] to airfoil designs [8], [11], [22], [32], the observations\\nobtained from the use of benchmark problems do not deviate\\nsignificantly from those in the real-world problems we have\\nexperimented. Some of the observations and problems we have\\nnoted when dealing with real-world problems are listed as\\nfollows.\\n\\n1) In contrast to benchmark problems, the time taken to\\ncollect adequate amount of database points when dealing\\nwith real-world problems can be relatively significant if\\nunsupported by sufficient machines capability. A possi-\\nble solution is to directly utilize an external database of\\npreviously evaluated design points, if available, instead\\nof building the database from scratch in the initial Gdb\\n\\ngenerations of evolutionary optimization. When existing\\ndatabase are unavailable, or the design points available\\nare insufficient for building reliable surrogates, a smaller\\nGdb can be used to obtain the initial design points\\nnecessary for the reliable surrogate building to facilitate\\ntime saving.\\n\\n2) When parallel machines capability is available, multi-\\nlevel parallelization can be leveraged through the GSM\\nframework, namely, 1) generation level, i.e., individuals\\nat the same generation are sent to multiple computing\\nnodes for evaluation; 2) individual level, independent\\nlocal searches utilizing M1 and M2 respectively, are\\nexecuted in parallel. Hence, further acceleration can be\\nexpected.\\n\\nAcknowledgment\\n\\nD. Lim and Y.-S. Ong would like to thank the members of\\nNanyang Technological University, Singapore, for providing\\nthe computing resources.\\n\\nAppendix A\\n\\nApproximation/Surrogate Modeling Techniques\\n\\nHere, we provide a brief review on three different\\nsurrogate modeling techniques used in this paper, namely:\\nKriging/Gaussian process (GP), polynomial regression (PR),\\nand radial basis function (RBF). Throughout this section,\\nlet D = {xi, ti}, i = 1, . . . , m denote the training dataset,\\nwhere xi ∈ Rd is an input design vector and ti ∈ R is the\\ncorresponding target value.\\n\\nA. Kriging/Gaussian Process (GP)\\n\\nThe GP surrogate model [55] assumes the presence of an\\nunknown true modeling function f (x) and an additive noise\\nterm v to account for anomalies in the observed data. Thus\\n\\nt = f (x) + v. (26)\\n\\nThe standard analysis requires the specification of prior\\nprobabilities on the modeling function and the noise model.\\nFrom a stochastic process viewpoint, the collection t =\\n{t1, t2, . . . , tm} is called a Gaussian process if every subset\\nof t has a joint Gaussian distribution. More specifically\\n\\nP(t|C, {xm}) =\\n1\\n\\nZ\\nexp\\n\\n(\\n−1\\n\\n2\\n(t − µ)T C−1(t − µ)\\n\\n)\\n(27)\\n\\nwhere C is a covariance matrix parameterized in terms of\\nhyperparameters θ, i.e., Cij = k(xi, xj; θ) and µ is the process\\nmean. The Gaussian process is characterized by this covari-\\nance structure since it incorporates prior beliefs both about\\nthe true underlying function as well as the noise model. In the\\npresent study, we use the following exponential covariance\\nmodel:\\n\\nk(xi, xj) = exp −(xi − xj)T �(xi − xj) + θd+1 (28)\\n\\nwhere � = diag{θ1, θ2, . . . , θd} ∈ Rd×d is a diagonal matrix of\\nundetermined hyperparameters, and θd+1 ∈ R is an additional\\nhyperparameter arising from the assumption that noise in the\\ndataset is Gaussian (and output dependent). We shall hence-\\nforth use the symbol θ to denote the vector of undetermined\\nhyperparameters, i.e., θ = {θ1, θ2, . . . , θd+1}. In practice, the\\nundetermined hyperparameters are tuned to the data using the\\nevidence maximization framework. Once the hyperparameters\\nhave been estimated from the data, predictions can be readily\\nmade for a new testing point.\\n\\nB. Polynomial Regression (PR)\\n\\nIn PR metamodeling technique [56], we define an exponent\\nvector ε containing positive integers (π1, π2, . . . , πd) and\\ndefine xε\\n\\ni as an exponent input vector (xi1\\nπ1 , xi2\\n\\nπ2 , . . . , xid\\nπd ).\\n\\nGiven a set of exponent vectors ε1, ε2, . . . , εo and the set\\nof data (xi, ti), where i = 1, 2, . . . , m, the polynomial model\\nof (o − 1)th order has the form\\n\\nt̂i = C1xε1\\ni + C2xε2\\n\\ni + · · · + Cmxεo\\n\\ni (29)\\n\\nwhere C1, C2, . . . , Co are the coefficient vectors to be esti-\\nmated, and Cj = (cj1 , cj2 , . . . , cjd\\n\\n), j = 1, 2, . . . , o.\\nThe least square method is then used to estimate the\\n\\ncoefficients of the polynomial model. By definition, the least\\nsquare error E to be minimized is\\n\\nE =\\nm∑\\ni=1\\n\\n[ti − t̂i]\\n2. (30)\\n\\nIt may be easily shown that ti = f (xi), and by multiplying\\nboth sides of (29) with xεj\\n\\ni and taking the sum of m pairs of\\ninput-output data, we arrive at\\n\\nC1\\n\\n∑\\ni\\n\\nxε1+εj\\n\\ni + · · · + Co\\n\\n∑\\ni\\n\\nxεo+εj\\n\\ni =\\n∑\\n\\ni\\n\\ntix\\nεj\\n\\ni . (31)\\n\\nFor j = 1, 2, . . . , o, the polynomial model for the training\\ndataset can be represented in the matrix notation as follows:\\n\\nAγT = bT (32)\\n\\n\\n\\n352 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 14, NO. 3, JUNE 2010\\n\\nwhere\\n\\nA =\\n\\n⎡\\n⎢⎣\\n∑\\n\\ni xε1+ε1\\ni . . .\\n\\n∑\\ni xε1+εo\\n\\ni\\n\\n...\\n...∑\\n\\ni xεo+ε1\\ni . . .\\n\\n∑\\ni xεo+εo\\n\\ni\\n\\n⎤\\n⎥⎦ (33)\\n\\nb = (\\n∑\\n\\ntix\\nε1\\ni , . . . ,\\n\\n∑\\ntix\\n\\nεo\\n\\ni ) (34)\\n\\nγ = (C1, C2, . . . , Co). (35)\\n\\nThen the coefficient matrix of the polynomial is\\n\\nγ = (A−1bT )T . (36)\\n\\nLet Bi = (xε1\\ni , . . . , xεo\\n\\ni ), the following equations may be\\nderived:\\n\\nA =\\n∑\\n\\ni\\n\\nBT\\ni Bi\\n\\nb =\\n∑\\n\\ni\\n\\ntiBi\\n\\nt̂i = γ.BT\\ni .\\n\\nThe predicted output for a new input pattern is then given\\nby t̂i = γ.BT\\n\\ni .\\n\\nC. Radial Basis Function\\n\\nThe surrogate models of RBF used in this paper are inter-\\npolating radial basis function networks of the form\\n\\nt̂ = f̂ (x) =\\nm∑\\ni=1\\n\\nαiK(||x − xi||) (37)\\n\\nwhere K(||x − xi||) : Rd → R is a RBF and α =\\n{α1, α2, . . . , αm} ∈ Rm denotes the vector of weights. Hence,\\nthe number of hidden nodes in the RBF here is as many as\\nthe number of training points.\\n\\nTypical choices for the kernel include linear splines, cubic\\nsplines, multiquadrics, thin-plate splines, and Gaussian func-\\ntions [57]. Recent studies in [73], [74], indicate that the\\nlinear, cubic, and thin plate spline RBFs have better theoretical\\nproperties than the multiquadric and Gaussian RBFs. Hence,\\nin this paper, we opt to use linear spline kernel function.\\nThe structure of some commonly used radial basis kernels\\nand their parameterization are shown in Table XIX Given\\na suitable kernel, the weight vector can be computed by\\nsolving the linear algebraic system of equations Kα = t, where\\nt = {t1, t2, . . . , tm} ∈ Rm denotes the vector of outputs and\\nK ∈ Rm×m denotes the Gram matrix formed using the training\\ninputs (i.e., the ijth element of K is computed as K(||xi−xj||)).\\n\\nAppendix B\\n\\nSingle-Objective Benchmark Functions\\n\\nSingle-objective benchmark functions used in this paper are\\npresented in this section. The shifted and/or rotated functions\\nare taken from [62] and [63]. Note that due to the long\\ndescription for F7–F10, reader is referred directly to [63]\\nfor those functions. From F4–F6, the following nomenclature\\napplies:\\n\\no = [o1, o2, . . . , od]: the shifted global optimum\\nM: linear transformation matrix, obtained from [63].\\nF1: Ackley\\n\\nF (x) = 20 + e − 20 exp\\n\\n(\\n−0.2\\n\\n√\\n1\\nd\\n\\nd∑\\ni=1\\n\\nx2\\ni\\n\\n)\\n\\n− exp\\n\\n(\\n1\\nd\\n\\nd∑\\ni=1\\n\\ncos(2πxi)\\n\\n)\\n(38)\\n\\n−32.768 ≤ xi ≤ 32.768, i = 1, 2, . . . , d.\\n\\nGlobal optimum x∗\\ni = 0.0 for i = 1, . . . , d, F (x∗) = 0.0.\\n\\nF2: Griewank\\n\\nF (x) = 1 +\\n∑d\\n\\ni=1 x2\\ni /4000 −∏d\\n\\ni=1 cos(xi/\\n√\\n\\ni) (39)\\n\\n−600 ≤ xi ≤ 600, i = 1, 2, . . . , d.\\n\\nGlobal optimum x∗\\ni = 0.0 for i = 1, . . . , d, F (x∗) = 0.0.\\n\\nF3: Rosenbrock\\n\\nF (x) =\\n∑d−1\\n\\ni=1 (100 × (xi+1 − x2\\ni )2 + (1 − xi)2) (40)\\n\\n−2.048 ≤ xi ≤ 2.048, i = 1, 2, . . . , d.\\n\\nGlobal optimum x∗\\ni = 1.0 for i = 1, . . . , d, F (x∗) = 0.0.\\n\\nF4: Shifted Rotated Rastrigin\\n\\nF (x) =\\n∑d\\n\\ni=1(z2\\ni − 10cos(2πzi) + 10) − 330 (41)\\n\\nz = (x − o) ∗ M,\\n\\n−5 ≤ xi ≤ 5, i = 1, 2, . . . , d.\\n\\nGlobal optimum x∗ = o, F (x∗) = fbias = −330.\\nF5: Shifted Rotated Weierstrass\\n\\nF (x) =\\n∑d\\n\\ni=1(\\n∑kmax\\n\\nk=0 [akcos(2πbk(zi + 0.5))]) (42)\\n\\n−d\\n∑kmax\\n\\nk=0 [akcos(2πbk.0.5)] + 90\\n\\nz = (x − o) ∗ M,\\n\\n−0.5 ≤ xi ≤ 0.5, i = 1, 2, . . . , d.\\n\\nGlobal optimum x∗ = o, F (x∗) = fbias = 90. a = 0.5, b = 3,\\nkmax=20.\\nF6: Shifted Expanded Griewank Plus Rosenbrock\\n\\nF (x) = F2(F3(z1, z2)) + F2(F3(z2, z3)) + . . . (43)\\n\\n+F2(F3(zd−1, zd)) + F2(F3(zd, z1)) − 130\\n\\nz = x − o + 1,\\n\\n−3 ≤ xi ≤ 1, i = 1, 2, . . . , d.\\n\\nGlobal optimum x∗ = o, F (x∗) = fbias = −130.\\nF7: Hybrid Composition Function [63, F15].\\nF8: Rotated Hybrid Composition Function of F7 [63, F16].\\nF9: Rotated Hybrid Composition Function with Narrow Basin\\nGlobal Optimum [63, F19].\\nF10: Noncontinuous Rotated Hybrid Composition Function\\n[63, F23].\\n\\n\\n\\nLIM et al.: GENERALIZING SURROGATE-ASSISTED EVOLUTIONARY COMPUTATION 353\\n\\nReferences\\n\\n[1] C. G. Johnson and J. J. R. Caldalda, “Introduction: Genetic algorithms\\nin visual art and music,” Leonardo, vol. 35, no. 2, pp. 175–184, Apr.\\n2002.\\n\\n[2] C. Aranha and H. Iba, “The memetic tree-based genetic algorithm and its\\napplication to portfolio,” Memetic Comput., vol. 1, no. 2, pp. 139–151,\\nJun. 2009.\\n\\n[3] D. Simon, “Biogeography-based optimization,” IEEE Trans. Evol. Com-\\nput., vol. 12, no. 6, pp. 702–713, Dec. 2008.\\n\\n[4] S.M.K. Hasan, R. Sarker, D. Essam, and D. Cornforth, “Memetic algo-\\nrithms for solving job-shop scheduling problems,” Memetic Comput., vol.\\n1, no. 1, pp. 69–83, Mar. 2009.\\n\\n[5] E. W. Lameijer, T. Baeck, J. N. Kok, and A. P. Ijzerman, “Evolutionary\\nalgorithms in drug design,” Nat. Comput., vol. 4, no. 3, pp. 177–243,\\nSep. 2005.\\n\\n[6] M. Olhofer, T. Arima, T. Sonoda, and B. Sendhoff, “Optimization of\\na stator blade used in a transonic compressor cascade with evolution\\nstrategies,” Adaptive Computing in Design and Manufacture (ACDM).\\nBerlin, Germany: Springer-Verlag, 2000, pp. 45–54.\\n\\n[7] Y. Jin, M. Olhofer, and B. Sendhoff, “A framework for evolutionary opti-\\nmization with approximate fitness function,” IEEE Trans. Evol. Comput.,\\nvol. 6, no. 5, pp. 481–494, Oct. 2002.\\n\\n[8] Y. S. Ong, P. B. Nair, and A. J. Keane, “Evolutionary optimization of\\ncomputationally expensive problems via surrogate modeling,” Am. Inst.\\nAeronaut. Astronaut. J., vol. 41, no. 4, pp. 687–696, 2003.\\n\\n[9] Y. Jin, “A comprehensive survey of fitness approximation in evolutionary\\ncomputation,” Soft Comput., vol. 9, no. 1, pp. 3–12, Jan. 2005.\\n\\n[10] A. Ratle, “Kriging as a surrogate fitness landscape in evolutionary\\noptimization,” Artif. Intell. Eng. Design Manufact., vol. 15, no. 1, pp.\\n37–49, May 2001.\\n\\n[11] Z. Zhou, Y. S. Ong, P. B. Nair, A. J. Keane, and K. Y. Lum,\\n“Combining global and local surrogate models to accelerate evolutionary\\noptimization,” IEEE Trans. Syst., Man, Cybern. C, Appl. Rev., vol. 37,\\nno. 1, pp. 66–76, Jan. 2007.\\n\\n[12] R. Smith, B. Dike, and S. Stegmann, “Fitness inheritance in ge-\\nnetic algorithms,” in Proc. ACM Symp. Appl. Comput., 1995, pp. 345–\\n350.\\n\\n[13] J. H. Chen, D. E. Goldberg, S. Y. Ho, and K. Sastry, “Fitness inheritance\\nin multiobjective optimization,” in Proc. Genet. Evol. Comput. Conf.,\\n2002, pp. 319–326.\\n\\n[14] Y. Jin, M. Olhofer, and B. Sendhoff, “On evolutionary optimization\\nwith approximate fitness functions,” in Proc. Genet. Evol. Comput. Conf.,\\n2000, pp. 786–792.\\n\\n[15] M. Emmerich, A. Giotis, M. Oezdenir, T. Baeck, and K. Giannakoglou,\\n“Metamodel-assisted evolution strategies,” in Proc. Parallel Problem\\nSolving from Nature, vol. 2439. 2002, pp. 371–380.\\n\\n[16] H. Ulmer, F. Streichert, and A. Zell, “Evolution strategies assisted by\\nGaussian processes with improved preselection criterion,” in Proc. IEEE\\nCongr. Evol. Comput., 2003, pp. 692–699.\\n\\n[17] H. S. Kim and S. B. Cho, “An efficient genetic algorithms with less\\nfitness evaluation by clustering,” in Proc. Congr. Evol. Comput., 2001,\\npp. 887–894.\\n\\n[18] Y. Jin and B. Sendhoff, “Reducing fitness evaluations using clustering\\ntechniques and neural networks ensembles,” in Proc. Genet. Evol. Com-\\nput. Conf., vol. 3102. 2004, pp. 688–699.\\n\\n[19] J. Branke and C. Schmidt, “Faster convergence by means of fitness\\nestimation,” Soft Comput., vol. 9, no. 1, pp. 13–20, Jan. 2005.\\n\\n[20] L. Gräning, Y. Jin, and B. Sendhoff. “Individual-based management of\\nmeta-models for evolutionary optimization with applications to 3-D blade\\noptimization,” Evolutionary Computation in Dynamic and Uncertain\\nEnvironments, S. Yang, Y.-S. Ong, and Y. Jin, Eds. Berlin, Germany:\\nSpringer-Verlag, 2007, pp. 225–250.\\n\\n[21] P. K. S. Nain and K. Deb, “Computationally effective search and\\noptimization procedure using coarse-to-fine approximation,” in Proc.\\nCongr. Evol. Comput., 2003, pp. 2081–2088.\\n\\n[22] Y. S. Ong, P. B. Nair, and K. Y. Lum, “Max–min surrogate-assisted\\nevolutionary algorithm for robust aerodynamic design,” IEEE Trans. Evol.\\nComput., vol. 10, no. 4, pp. 392–404, Aug. 2006.\\n\\n[23] Y. S. Ong, P. B. Nair, and K. Y. Lum, “Evolutionary algorithm with\\nhermite radial basis function interpolations for computationally expensive\\nadjoint solvers,” Comput. Optim. Applicat., vol. 39, no. 1, pp. 91–119,\\nJan. 2008.\\n\\n[24] K. C. Giannakoglou, D. I. Papadimitriou, and I. C. Kampolis, “Aero-\\ndynamic shape design using evolutionary algorithms and new gradient-\\nassisted metamodels,” Comput. Methods Appl. Mech. Eng., vol. 195,\\npp. 6312–6329, Sep. 2006.\\n\\n[25] M. D. Schmidt and H. Lipson, “Coevolution of fitness predictors,” IEEE\\nTrans. Evol. Comput., vol. 12, no. 6, pp. 736–749, Dec. 2008.\\n\\n[26] J. Knowles, “ParEGO: A hybrid algorithm with on-line landscape\\napproximation for expensive multiobjective optimization problems,” IEEE\\nTrans. Evol. Comput., vol. 10, no. 1, pp. 50–66, Feb. 2006.\\n\\n[27] D. Jones, M. Schonlau, and W. Welch, “Efficient global optimization\\nof expensive black-box functions,” J. Global Optim., vol. 13, no. 4, pp.\\n455–492, Dec. 1998.\\n\\n[28] K. Deb and P. K. S. Nain, “An evolutionary multiobjective meta-\\nmodeling procedure using artificial neural networks,” Evolutionary Com-\\nputation in Dynamic and Uncertain Environments, S. Yang, Y. S. Ong,\\nand Y. Jin, Eds. Berlin, Germany: Springer-Verlag, 2007, pp. 297–\\n322.\\n\\n[29] M. Emmerich, K. Giannakoglou, and B. Naujoks, “Single and multi-\\nobjective evolutionary optimization assisted by Gaussian random field\\nmetamodels,” IEEE Trans. Evol. Comput., vol. 10, no. 4, pp. 421–439,\\nAug. 2006.\\n\\n[30] D. Chafekar, L. Shi, K. Rasheed, and J. Xuan, “Multiobjective GA\\noptimization using reduced models,” IEEE Trans. Syst., Man, Cybern.\\nC, Appl. Rev., vol. 35, no. 2, pp. 261–265, May 2005.\\n\\n[31] I. Voutchkov and A. J. Keane. “Multiobjective optimization using\\nsurrogates,” in Proc. 7th Int. Conf. Adaptive Comput. Design Manufact.,\\nBaarn, The Netherlands: M.C. Escher Company, 2006, pp. 167–175.\\n\\n[32] Y. S. Ong, P. B. Nair, A. J. Keane, and K. W. Wong, “Surrogate-assisted\\nevolutionary optimization frameworks for high-fidelity engineering design\\nproblems,” Knowledge Incorporation in Evolutionary Computation, Y.\\nJin, Ed. Berlin, Germany: Springer-Verlag, 2004, pp. 307–331.\\n\\n[33] D. Lim, Y. S. Ong, Y. Jin, and B. Sendhoff, “A study on metamodeling\\ntechniques, ensembles, and multisurrogates in evolutionary computation,”\\nin Proc. Genet. Evol. Comput. Conf., London, U.K.: ACM, 2007, pp.\\n1288–1295.\\n\\n[34] A. Samad and K.-Y. Kim, “Multiple surrogate modeling for axial\\ncompressor blade shape optimization,” J. Propulsion Power, vol. 24, no.\\n2, pp. 302–310, Mar. 2008.\\n\\n[35] L. E. Zerpa, N. V. Queipo, S. Pintos, and J.-L. Salager, “An optimization\\nmethodology of alkaline-surfactant-polymer flooding processes using field\\nscale numerical simulation and multiple surrogates,” J. Petroleum Sci.\\nEng., vol. 47, no. 3–4, pp. 197–208, Jun. 2005.\\n\\n[36] D. Marjavaara, S. Lundström, and W. Shyy, “Hydraulic turbine diffuser\\nshape optimization by multiple surrogate model approximations of pareto\\nfronts,” ASME J. Fluids Eng., vol. 129, no. 9, pp. 1228–1240, 2007.\\n\\n[37] N. V. Queipo, R. T. Haftka, W. Shyy, T. Goel, R. Vaidyanathan, and P.\\nK. Tucker, “Surrogate-based analysis and optimization,” Progr. Aerospace\\nSci., vol. 37, pp. 59–118, 2001.\\n\\n[38] E. Sanchez, S. Pintos, and N. V. Queipo, “Toward an optimal ensemble\\nof kernel-based approximations with engineering applications,” in Proc.\\nInt. Joint Conf. Neural Netw. (IJCNN), Jul. 2006, pp. 2152–2158.\\n\\n[39] A. Samad, K.-D. Lee, K.-Y. Kim, and R. T. Haftka, “Application of\\nmultiple-surrogate model to optimization of a dimpled channel,” in Proc.\\n7th World Congr. Struct. Multidisc. Optim., 2007, pp. 2276–2282.\\n\\n[40] T. Goel, R. T. Haftka, W. Shyy, and N. V. Queipo, “Ensemble of\\nsurrogates,” Struct. Multidisc. Optim., vol. 33, no. 3, pp. 199–216, Mar.\\n2007.\\n\\n[41] E. Acar and M. Rais-Rohani, “Ensemble of metamodels with optimized\\nweight factors,” Struct. Multidisc. Optim., vol. 37, no. 3, pp. 279–294,\\n2009.\\n\\n[42] K.-H. Liang, X. Yao, and C. Newton, “Combining landscape approx-\\nimation and local search in global optimization,” in Proc. 1999 Congr.\\nEvol. Comput., Piscataway, NJ, vol. 2. 1999, pp. 1514–1520.\\n\\n[43] K.-H. Liang, X. Yao, and C. Newton, “Evolutionary search of approxi-\\nmated n-dimensional landscape,” Int. J. Knowl.-Based Intell. Eng. Syst.,\\nvol. 4, no. 3, pp. 172–183, 2000.\\n\\n[44] Y. S. Ong, Z. Zhou, and D. Lim, “Curse and blessing of uncertainty\\nin evolutionary algorithm using approximation,” in Proc. Congr. Evol.\\nComput., Vancouver, BC, 2006, pp. 2928–2935.\\n\\n[45] Y. Jin and J. Branke, “Evolutionary optimization in uncertain environ-\\nments: A survey,” IEEE Trans. Evol. Comput., vol. 9, no. 3, pp. 303–317,\\nJun. 2005.\\n\\n[46] R. Meuth, M. H. Lim, Y. S. Ong, and D. C. Wunsch, II, “A proposition\\non memes and meta-memes in computing for higher-order learning,”\\nMemetic Comput., vol. 1, no. 2, pp. 85–100, Jun. 2009.\\n\\n[47] C. K. Goh, E. J. Teoh, and K. C. Tan, “Hybrid multiobjective evolu-\\ntionary design for artificial neural networks,” IEEE Trans. Neural Netw.,\\nvol. 19, no. 9, pp. 1531–1548, Sep. 2008.\\n\\n[48] N. Noman and H. Iba, “Accelerating differential evolution using an\\nadaptive local search,” IEEE Trans. Evol. Comput., vol. 12, no. 1, pp.\\n107–125, Feb. 2008.\\n\\n\\n\\n354 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 14, NO. 3, JUNE 2010\\n\\n[49] D. S. Liu, K.C. Tan, C. K. Goh, and W. K. Ho, “A multiobjec-\\ntive memetic algorithm based on particle swarm optimization,” IEEE\\nTrans. Syst., Man, Cybern. B, Cybern., vol. 37, no. 1, pp. 42–50, Feb.\\n2007.\\n\\n[50] Y. S. Ong, M.H. Lim, N. Zhu, and K. W. Wong, “Classification of\\nadaptive memetic algorithms: A comparative study,” IEEE Trans. Syst.,\\nMan, Cybern. B, Cybern., vol. 36, no. 1, pp. 141–152, Feb. 2006.\\n\\n[51] Y. Liu, X. Yao, and T. Higuchi, “Evolutionary ensembles with negative\\ncorrelation learning,” IEEE Trans. Evol. Comput., vol. 4, no. 4, pp. 380–\\n387, Nov. 2000.\\n\\n[52] L. S. Kwan, “Diffusion equation and global optimization,” PhD dis-\\nsertation, Dept. Automat. Comput.-Aided Eng., Chin. Univ. Hong Kong,\\nChina, 2004.\\n\\n[53] J. F. Rodriguez, J. E. Renaud, and L. T. Watson, “Convergence of Trust-\\nregion augmented lagrangian methods using variable fidelity approxima-\\ntion data,” Virginia Polytech. Inst. State Univ. Blacksburg, Tech. Rep.\\nTR-97-14, 1997.\\n\\n[54] C. T. Lawrence and A. L. Tits, “A computationally efficient feasible\\nsequential quadratic programming algorithm,” Soc. Ind. Appl. Math.,\\nvol. 11, no. 4, pp. 1092–1118, 2001.\\n\\n[55] D. J. C. Mackay, “Introduction to Gaussian processes,” Neural Netw.\\nMach. Learn., vol. 168, pp. 133–165, 1998.\\n\\n[56] F. H. Lesh, “Multidimensional least-square polynomial curve fitting,”\\nCommun. ACM, vol. 2, no. 9, pp. 29–30, 1959.\\n\\n[57] C. Bishop, Neural Networks for Pattern Recognition. London, U.K.:\\nOxford Univ. Press, 1995.\\n\\n[58] H. Ishibuchi and T. Murata, “Multiobjective genetic local search algo-\\nrithm,” in Proc. IEEE Int. Conf. Evol. Comput., 1996, pp. 119–124.\\n\\n[59] A. Jaszkiewicz, “Genetic local search for multiple objective combinato-\\nrial optimization,” Inst. Comput. Sci., Poznan Univ. of Tech., Tech. Rep.\\nRA-014/98, 1998.\\n\\n[60] J. Knowles and D. Corne, “Memetic algorithms for multiobjective op-\\ntimization: Issues, methods and prospects,” Recent Advances in Memetic\\nAlgorithms, W. E. Hart, N. Krasnogor, and J. E. Smith, Ed. Berlin,\\nGermany: Springer-Verlag, 2005, pp. 313–352.\\n\\n[61] N. Alexandrov, J. E. Dennis, R. M. Lewis, and V. Torczon, “A trust-\\nregion framework for managing the use of approximation models in\\noptimization,” J. Struct. Optim., vol. 15, no. 1, pp. 16–23, 1998.\\n\\n[62] J. G. Digalakis and K. G. Margaritis, “On benchmarking functions for\\ngenetic algorithms,” Int. J. Comput. Math., vol. 77, no. 4, pp. 481–506,\\n2001.\\n\\n[63] P. N. Suganthan, N. Hansen, J. J. Liang, K. Deb, Y. P. Chen, A.\\nAuger, and S. Tiwari, “Problem definitions and evaluation criteria for\\nthe CEC 2005 special session on real-parameter optimization,” Nanyang\\nTech. Univ., Singapore, Tech. Rep. and IIT Kanpur, India, KanGAL Rep.\\n2005005, May 2005.\\n\\n[64] E. Zitzler, K. Deb, and L. Thiele, “Comparison of multiobjective\\nevolutionary algorithms: Empirical results,” Evol. Comput., vol. 8, no.\\n2, pp. 173–195, 2000.\\n\\n[65] K. Deb, S. Agrawal, A. Pratab, and T. Meyarivan, “A fast elitist\\nnondominated sorting genetic algorithm for multiobjective optimization:\\nNSGA-II,” in Proc. Parallel Problem Solving from Nature VI Conf., vol.\\n1917. 2000, pp. 849–858.\\n\\n[66] E. Zitzler, L. Thiele, M. Laumanns, C. M. Foneseca, and V. Grunert\\nda Fonseca, “Performance assessment of multiobjective optimizers: An\\nanalysis and review,” IEEE Trans. Evol. Comput., vol. 7, no. 2, pp. 117–\\n132, Apr. 2003.\\n\\n[67] C. A. Coello Coello, D. A. Van Veldhuizen, and G. B. Lamont,\\nEvolutionary Algorithms for Solving Multiobjective Problems. New York:\\nKluwer Academic, 2002, pp. 243–272.\\n\\n[68] D. A. Van Veldhuizen and G. B. Lamont, “Evolutionary computation\\nand convergence to a Pareto front,” Late Breaking Papers at the Genetic\\nProgramming, 1998, pp. 221–228.\\n\\n[69] D. A. Van Veldhuizen, “Multiobjective evolutionary algorithms: Clas-\\nsifications, analysis, and new innovations,” Ph.D. dissertation, Graduate\\nSchool Eng., Air Force Inst. Technol., Dayton, OH, 1999.\\n\\n[70] E. Zitzler, “Evolutionary algorithms for multiobjective optimization:\\nMethods and applications,” Ph.D. dissertation, Swiss Federal Inst. Tech-\\nnol. (ETH), Zurich, Switzerland, TIK-Schriftenreihe Nr. 30, dissertation\\nETH No. 13398. Aachen, Germany: Shaker Verlag, 1999.\\n\\n[71] E. Zitzler and L. Thiele, “Multiobjective optimization using evolutionary\\nalgorithms: A comparative case study,” in Proc. Fifth Int. Conf. Parallel\\nProblem Solving from Nature (PPSN-V). Berlin, Germany: Springer-\\nVerlag, 1998, pp. 292–301.\\n\\n[72] K. Deb, “Salient issues of multi-objective evolutionary algorithms,”\\nin Multiobjective Optimization Using Evolutionary Algorithms, 1st ed.\\nChichester, U.K.: Wiley, 2001, pp. 315–446.\\n\\n[73] R. G. Regis and C. A. Shoemaker, “Local function approximation in\\nevolutionary algorithms for the optimization of costly functions,” IEEE\\nTrans. Evol. Comput., vol. 8, no. 5, pp. 490–505, Oct. 2004.\\n\\n[74] H.-M. Gutmann, “On the semi-norm of radial basis function inter-\\npolants,” Dept. Applied Math. Theor. Phy., Univ. Cambridge, U.K., Tech.\\nRep. DAMTP 2000/NA04, 2000.\\n\\nDudy Lim received the B.Eng. (Hons.) degree\\nfrom the School of Computer Engineering, Nanyang\\nTechnological University, Singapore, in 2004. He is\\ncurrently pursuing the Ph.D. degree on the topic\\nof evolutionary optimization for computationally ex-\\npensive problems.\\n\\nPreviously, he worked as a Grid Engineer in\\nthe Grid Operation and Training Center, Parallel\\nand Distributed Computing Center, School of Com-\\nputer Engineering, Nanyang Technological Univer-\\nsity. Currently, he is with Center for Computa-\\n\\ntional Intelligence, School of Computer Engineering, Nanyang Technological\\nUniversity. His research interests include evolutionary computation, neural\\nnetworks, and grid computing.\\n\\nYaochu Jin (M’98–SM’02) received the B.Sc.,\\nM.Sc., and Ph.D. degrees, all in automatic con-\\ntrol, from Zhejiang University, Hangzhou, China, in\\n1988, 1991, and 1996, respectively, and the second\\nPh.D. degree in electrical and computer engineering\\nfrom Ruhr-Universität Bochum, Bochum, Germany,\\nin 2001.\\n\\nCurrently, he is the Principal Scientist with Honda\\nResearch Institute Europe, Offenbach, Germany, and\\nalso is the Scientific Coordinator at CoR-Lab Gradu-\\nate School, Bielefeld University, Bielefeld, Germany.\\n\\nPreviously, he was an Associate Professor in Electrical Engineering Depart-\\nment, Zhejiang University, a Visiting Researcher and was with the scientific\\nstaff at Institut für Neuroinformatik, Ruhr-Universität Bochum, a Postdoctoral\\nAssociate in the Industrial Engineering Department, Rutgers, Piscataway, NJ,\\nand a Scientist at Honda Research Institute Europe. His research interests\\ninclude computational approaches to understanding evolution, learning and\\ndevelopment in biology, and evolutionary developmental and learning ap-\\nproaches to complex systems design.\\n\\nDr. Jin is the co-editor of Evolutionary Computation in Dynamic and\\nUncertain Environments (Berlin, Germany: Springer, 2007), and the editor\\nof Multiobjective Machine Learning (Berlin, Germany: Springer, 2006) and\\nKnowledge Incorporation in Evolutionary Computation (Berlin, Germany:\\nSpringer, 2005). He is the author of Advanced Fuzzy Systems Design and\\nApplications (Heidelberg, Germany: Springer, 2003) and published about 100\\njournal and conference papers. Currently, he is an Associate Editor of the IEEE\\n\\nTransactions on Neural Networks, the IEEE Transactions on Con-\\n\\ntrol Systems Technology, the IEEE Transactions on Systems, Man,\\n\\nand Cybernetics, Part C: Applications and Reviews, and the IEEE\\n\\nComputational Intelligence Magazine. He is a member of Evolutionary\\nComputation Technical Committee and a Member of Emergent Technologies\\nTechnical Committee of the IEEE Computational Intelligence Society. He\\nhas been an Invited Keynote Speaker on three international conferences and\\nworkshops.\\n\\nYew-Soon Ong received the B.Eng. and M.Eng. de-\\ngrees, both in electrical and electronics engineering,\\nfrom Nanyang Technology University, Singapore,\\nin 1998 and 1999, respectively, and the Ph.D. de-\\ngree from University of Southampton, Southampton,\\nU.K., in 2002.\\n\\nCurrently, he is an Associate Professor in the\\nDivision of Information Systems, School of Com-\\nputer Engineering, Nanyang Technological Univer-\\nsity, Singapore, as well as the Director of the Cen-\\nter for Computational Intelligence (C2I), Nanyang\\n\\nTechnological University, Singapore. His research interests include in compu-\\ntational intelligence spanning: memetic algorithms, evolutionary design, and\\ngrid computing.\\n\\nDr. Ong is the co-technical editor-in-chief of Memetic Computing jour-\\nnal, an Associate Editor of IEEE Transactions on Systems, Man and\\n\\nCybernetics - Part B, International Journal of System Science\\n\\nand Soft Computing Journal. Also, he is the Chair of the Task Force\\n\\n\\n\\nLIM et al.: GENERALIZING SURROGATE-ASSISTED EVOLUTIONARY COMPUTATION 355\\n\\non Memetic Algorithms in the IEEE Computational Intelligence Society\\nEmergent Technology Technical Committee.\\n\\nBernhard Sendhoff (M’99–SM’05) studied physics\\nat the Ruhr-Universität Bochum, Germany, and\\nthe University of Sussex, U.K. He received the\\nDoctorate degree in physics from Ruhr-Universität\\nBochum, in 1998.\\n\\nFrom 1998 to 1999, he was a Research Assistant\\nat Institute for Neuroinformatics, Zurich, Switzer-\\nland. From 1999 to 2002, he was with Honda\\nResearch Institute Europe GmbH, Offenbach, Ger-\\nmany in several positions, lastly as Deputy Division\\nManager. Currently, he is the Chief Technology\\n\\nOfficer at Honda Research Institute Europe GmbH in addition to being\\nthe Head of the Evolutionary and Learning Technology Group. Also, he\\nis a Honorary Professor at the School of Computer Science, University\\nof Birmingham, U.K., and a Professor at the Technical University Darm-\\nstadt, Darmstadt, Germany. He is the author and co-author of over 120\\nresearch papers in journals and refereed conferences. His research interests\\ninclude topics from systems biology and computational intelligence such\\nas evolutionary system design and structure optimization of adaptive sys-\\ntems.\\n\\nDr. Sendhoff is a member of the Association for Computing Machin-\\nery, the European Neural Network Society and the Deutsche Physikalische\\nGesellschaft. He is a member of several advisory boards and scientific\\ncommittees.\\n\\n\\n'"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["df.loc[0,'Text']"]},{"cell_type":"code","execution_count":null,"id":"17e4b472","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"error","timestamp":1713587200956,"user":{"displayName":"Shivam","userId":"00171756857051516066"},"user_tz":-330},"id":"17e4b472","outputId":"87fe8c16-d8dc-491e-f1bb-d6e531955763"},"outputs":[{"ename":"OSError","evalue":"Cannot save file into a non-existent directory: '/content/drive/MyDrive/IK'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-4546d9e11cff>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/IK/IK_rr_DataFrame.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3770\u001b[0m         )\n\u001b[1;32m   3771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3772\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3773\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3774\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         )\n\u001b[0;32m-> 1186\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \"\"\"\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0;31m# Only for write methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0mcheck_parent_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf\"Cannot save file into a non-existent directory: '{parent}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/content/drive/MyDrive/IK'"]}],"source":["df.to_csv('/content/drive/MyDrive/IK/IK_rr_DataFrame.csv', index = False)"]},{"cell_type":"code","execution_count":null,"id":"e93d448f","metadata":{"id":"e93d448f"},"outputs":[],"source":["!ls /content/drive/MyDrive/IK/IK_rr_DataFrame.csv"]},{"cell_type":"code","execution_count":null,"id":"-cQDLolnGyEq","metadata":{"id":"-cQDLolnGyEq"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1ftmHH0aArC3EHYU6Ywu3JBILADtzZloB","timestamp":1741788353153}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"nbformat":4,"nbformat_minor":5}